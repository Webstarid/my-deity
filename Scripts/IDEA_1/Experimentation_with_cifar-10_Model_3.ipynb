{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 100 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 5 # all kernels are 5x5\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 128 # we look at 5000 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbca32b7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAEVCAYAAADZzOErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIUQW5HyEEFmQ8xFCZOH/A+quZhIR4EW2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbcecb8d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEVCAYAAAAVVdvAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmgOGmVVzwDCzag4YZlbNAcPMqv0/bkMRlOpm5V0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbca6d73dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x9NuOLur691UAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbca6b64e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypefloat32\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[ 0.89411765  0.89411765  0.89803922]\n",
      " [ 0.90196079  0.90196079  0.90980393]\n",
      " [ 0.90196079  0.90980393  0.91764706]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8BMSKQx+QiLlfwFSjkenMLrW0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbca6ac6250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2, tf.shape(a1), tf.shape(a2), tf.shape(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_, sha1, sha2, sha3 = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(inp, sha1, sha2, sha3):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, sha3, \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, sha2,\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, sha1,\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.shape(tf_input),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_, sha1, sha2, sha3)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector, sha1, sha2, sha3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 297050.21875\n",
      "range:(4480, 4608) loss= 20998.2285156\n",
      "range:(8960, 9088) loss= 11556.6767578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 11612.7949219\n",
      "range:(4480, 4608) loss= 7605.16796875\n",
      "range:(8960, 9088) loss= 6210.43847656\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 5558.04443359\n",
      "range:(4480, 4608) loss= 4467.40771484\n",
      "range:(8960, 9088) loss= 3363.94458008\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3538.38012695\n",
      "range:(4480, 4608) loss= 2988.40625\n",
      "range:(8960, 9088) loss= 2460.4699707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2698.07788086\n",
      "range:(4480, 4608) loss= 1917.81958008\n",
      "range:(8960, 9088) loss= 1257.5871582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 2\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1638.51293945\n",
      "range:(4480, 4608) loss= 1645.82617188\n",
      "range:(8960, 9088) loss= 1607.98193359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1473.32324219\n",
      "range:(4480, 4608) loss= 1375.42138672\n",
      "range:(8960, 9088) loss= 1049.7947998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1076.8503418\n",
      "range:(4480, 4608) loss= 1975.78320312\n",
      "range:(8960, 9088) loss= 1210.22497559\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 855.512939453\n",
      "range:(4480, 4608) loss= 872.605224609\n",
      "range:(8960, 9088) loss= 1632.22705078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 740.757629395\n",
      "range:(4480, 4608) loss= 933.51574707\n",
      "range:(8960, 9088) loss= 934.421386719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 3\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 722.39465332\n",
      "range:(4480, 4608) loss= 798.445251465\n",
      "range:(8960, 9088) loss= 633.384887695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 595.47442627\n",
      "range:(4480, 4608) loss= 1547.68725586\n",
      "range:(8960, 9088) loss= 583.316467285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 552.637573242\n",
      "range:(4480, 4608) loss= 521.790161133\n",
      "range:(8960, 9088) loss= 969.729492188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 981.278747559\n",
      "range:(4480, 4608) loss= 557.158325195\n",
      "range:(8960, 9088) loss= 862.431335449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 622.343261719\n",
      "range:(4480, 4608) loss= 297.980407715\n",
      "range:(8960, 9088) loss= 611.465454102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 4\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 557.590148926\n",
      "range:(4480, 4608) loss= 374.974212646\n",
      "range:(8960, 9088) loss= 525.040710449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 958.918457031\n",
      "range:(4480, 4608) loss= 738.009887695\n",
      "range:(8960, 9088) loss= 354.671356201\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 436.727661133\n",
      "range:(4480, 4608) loss= 410.654052734\n",
      "range:(8960, 9088) loss= 550.895202637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 561.95098877\n",
      "range:(4480, 4608) loss= 360.619689941\n",
      "range:(8960, 9088) loss= 313.877807617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 292.719421387\n",
      "range:(4480, 4608) loss= 410.495117188\n",
      "range:(8960, 9088) loss= 211.387771606\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 5\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 208.180618286\n",
      "range:(4480, 4608) loss= 227.301681519\n",
      "range:(8960, 9088) loss= 769.486755371\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 584.672363281\n",
      "range:(4480, 4608) loss= 297.792510986\n",
      "range:(8960, 9088) loss= 284.122375488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 225.626434326\n",
      "range:(4480, 4608) loss= 174.551330566\n",
      "range:(8960, 9088) loss= 403.783447266\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 299.796356201\n",
      "range:(4480, 4608) loss= 322.750396729\n",
      "range:(8960, 9088) loss= 238.165359497\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 194.771942139\n",
      "range:(4480, 4608) loss= 488.646606445\n",
      "range:(8960, 9088) loss= 161.956542969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 6\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 144.270874023\n",
      "range:(4480, 4608) loss= 274.427154541\n",
      "range:(8960, 9088) loss= 184.271636963\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 202.751312256\n",
      "range:(4480, 4608) loss= 147.771774292\n",
      "range:(8960, 9088) loss= 331.738677979\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 162.130172729\n",
      "range:(4480, 4608) loss= 152.751159668\n",
      "range:(8960, 9088) loss= 180.44909668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 136.609100342\n",
      "range:(4480, 4608) loss= 101.173599243\n",
      "range:(8960, 9088) loss= 55.0275650024\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 66.716506958\n",
      "range:(4480, 4608) loss= 235.26322937\n",
      "range:(8960, 9088) loss= 239.440231323\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 7\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 191.561569214\n",
      "range:(4480, 4608) loss= 67.2780990601\n",
      "range:(8960, 9088) loss= 442.402679443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 410.23324585\n",
      "range:(4480, 4608) loss= 99.5494384766\n",
      "range:(8960, 9088) loss= 262.140228271\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 466.99609375\n",
      "range:(4480, 4608) loss= 356.345062256\n",
      "range:(8960, 9088) loss= 252.277603149\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 229.610824585\n",
      "range:(4480, 4608) loss= 98.8083572388\n",
      "range:(8960, 9088) loss= 61.7694702148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 93.0490341187\n",
      "range:(4480, 4608) loss= 34.2044029236\n",
      "range:(8960, 9088) loss= 23.7714443207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 8\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 26.4605560303\n",
      "range:(4480, 4608) loss= 24.0156955719\n",
      "range:(8960, 9088) loss= 23.6341018677\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 23.5470466614\n",
      "range:(4480, 4608) loss= 22.6018619537\n",
      "range:(8960, 9088) loss= 33.3246994019\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 36.9929504395\n",
      "range:(4480, 4608) loss= 836.348388672\n",
      "range:(8960, 9088) loss= 430.937438965\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 390.885742188\n",
      "range:(4480, 4608) loss= 169.912399292\n",
      "range:(8960, 9088) loss= 53.9566612244\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 69.4172134399\n",
      "range:(4480, 4608) loss= 79.4230957031\n",
      "range:(8960, 9088) loss= 188.023773193\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 9\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 236.500732422\n",
      "range:(4480, 4608) loss= 50.2555351257\n",
      "range:(8960, 9088) loss= 84.8670043945\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 187.497940063\n",
      "range:(4480, 4608) loss= 284.809234619\n",
      "range:(8960, 9088) loss= 99.0557479858\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 65.433013916\n",
      "range:(4480, 4608) loss= 72.1763458252\n",
      "range:(8960, 9088) loss= 32.6697845459\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 30.7496910095\n",
      "range:(4480, 4608) loss= 21.1856269836\n",
      "range:(8960, 9088) loss= 18.0024414062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 20.229139328\n",
      "range:(4480, 4608) loss= 19.5280723572\n",
      "range:(8960, 9088) loss= 16.6303024292\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 10\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 22.8958740234\n",
      "range:(4480, 4608) loss= 17.9685153961\n",
      "range:(8960, 9088) loss= 18.1381587982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 18.8288860321\n",
      "range:(4480, 4608) loss= 272.023468018\n",
      "range:(8960, 9088) loss= 339.508392334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 386.617736816\n",
      "range:(4480, 4608) loss= 285.183868408\n",
      "range:(8960, 9088) loss= 91.4869842529\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 75.9832687378\n",
      "range:(4480, 4608) loss= 38.2630767822\n",
      "range:(8960, 9088) loss= 22.7118434906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 29.4086608887\n",
      "range:(4480, 4608) loss= 17.1026763916\n",
      "range:(8960, 9088) loss= 14.7366657257\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 11\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 16.7010574341\n",
      "range:(4480, 4608) loss= 14.8587799072\n",
      "range:(8960, 9088) loss= 13.6138963699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 14.3098678589\n",
      "range:(4480, 4608) loss= 14.121752739\n",
      "range:(8960, 9088) loss= 16.1297569275\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 15.4197416306\n",
      "range:(4480, 4608) loss= 19.8648986816\n",
      "range:(8960, 9088) loss= 15.8439235687\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 22.3742542267\n",
      "range:(4480, 4608) loss= 1087.68188477\n",
      "range:(8960, 9088) loss= 596.579711914\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 277.482849121\n",
      "range:(4480, 4608) loss= 107.548110962\n",
      "range:(8960, 9088) loss= 52.969783783\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 12\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 71.2673110962\n",
      "range:(4480, 4608) loss= 32.8594779968\n",
      "range:(8960, 9088) loss= 24.9407100677\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 31.6541080475\n",
      "range:(4480, 4608) loss= 21.6024227142\n",
      "range:(8960, 9088) loss= 18.2143344879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 18.5679721832\n",
      "range:(4480, 4608) loss= 16.5318641663\n",
      "range:(8960, 9088) loss= 13.0055150986\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 12.6721410751\n",
      "range:(4480, 4608) loss= 10.4914646149\n",
      "range:(8960, 9088) loss= 10.1616106033\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 11.3851537704\n",
      "range:(4480, 4608) loss= 11.0669698715\n",
      "range:(8960, 9088) loss= 8.73877334595\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 13\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 10.3837060928\n",
      "range:(4480, 4608) loss= 7.65848827362\n",
      "range:(8960, 9088) loss= 8.77919483185\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 10.712978363\n",
      "range:(4480, 4608) loss= 12.8863782883\n",
      "range:(8960, 9088) loss= 8.47089481354\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 10.4242954254\n",
      "range:(4480, 4608) loss= 15.5097932816\n",
      "range:(8960, 9088) loss= 260.013122559\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 661.54083252\n",
      "range:(4480, 4608) loss= 199.297866821\n",
      "range:(8960, 9088) loss= 159.451904297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 99.1088104248\n",
      "range:(4480, 4608) loss= 40.835483551\n",
      "range:(8960, 9088) loss= 27.5996818542\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 14\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 37.8202476501\n",
      "range:(4480, 4608) loss= 34.9654998779\n",
      "range:(8960, 9088) loss= 33.5559501648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 21.8922691345\n",
      "range:(4480, 4608) loss= 18.6302165985\n",
      "range:(8960, 9088) loss= 25.5535850525\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 23.1000461578\n",
      "range:(4480, 4608) loss= 16.6943969727\n",
      "range:(8960, 9088) loss= 20.2369365692\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 37.1319732666\n",
      "range:(4480, 4608) loss= 21.2810878754\n",
      "range:(8960, 9088) loss= 14.2280807495\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 21.4923057556\n",
      "range:(4480, 4608) loss= 70.2152481079\n",
      "range:(8960, 9088) loss= 74.4739456177\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 15\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 101.966697693\n",
      "range:(4480, 4608) loss= 35.2913551331\n",
      "range:(8960, 9088) loss= 34.3508033752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 48.6287841797\n",
      "range:(4480, 4608) loss= 19.2343959808\n",
      "range:(8960, 9088) loss= 14.3658981323\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 12.4675102234\n",
      "range:(4480, 4608) loss= 10.5315513611\n",
      "range:(8960, 9088) loss= 8.02081203461\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 9.83212184906\n",
      "range:(4480, 4608) loss= 6.08766746521\n",
      "range:(8960, 9088) loss= 8.21638202667\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 8.94033718109\n",
      "range:(4480, 4608) loss= 8.52646827698\n",
      "range:(8960, 9088) loss= 76.2728424072\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 16\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 49.2451477051\n",
      "range:(4480, 4608) loss= 81.1081161499\n",
      "range:(8960, 9088) loss= 83.1846389771\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 65.9144973755\n",
      "range:(4480, 4608) loss= 27.4616832733\n",
      "range:(8960, 9088) loss= 19.4029903412\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 15.8191843033\n",
      "range:(4480, 4608) loss= 11.5604629517\n",
      "range:(8960, 9088) loss= 8.77074718475\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 8.25550460815\n",
      "range:(4480, 4608) loss= 6.16995716095\n",
      "range:(8960, 9088) loss= 6.67243862152\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 9.4807138443\n",
      "range:(4480, 4608) loss= 7.82569074631\n",
      "range:(8960, 9088) loss= 5.6889333725\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 17\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 6.76198101044\n",
      "range:(4480, 4608) loss= 5.31564283371\n",
      "range:(8960, 9088) loss= 6.45236778259\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 8.07190895081\n",
      "range:(4480, 4608) loss= 141.437393188\n",
      "range:(8960, 9088) loss= 183.606109619\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 136.511245728\n",
      "range:(4480, 4608) loss= 42.7296218872\n",
      "range:(8960, 9088) loss= 29.6822929382\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 17.2381057739\n",
      "range:(4480, 4608) loss= 24.6591835022\n",
      "range:(8960, 9088) loss= 73.7641830444\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 61.6243171692\n",
      "range:(4480, 4608) loss= 30.0574951172\n",
      "range:(8960, 9088) loss= 21.0214099884\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 18\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 28.7705802917\n",
      "range:(4480, 4608) loss= 10.1701192856\n",
      "range:(8960, 9088) loss= 9.56822872162\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 11.6136751175\n",
      "range:(4480, 4608) loss= 7.22771835327\n",
      "range:(8960, 9088) loss= 5.72630977631\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 5.33997917175\n",
      "range:(4480, 4608) loss= 5.61895751953\n",
      "range:(8960, 9088) loss= 4.81891202927\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 5.457218647\n",
      "range:(4480, 4608) loss= 4.52852249146\n",
      "range:(8960, 9088) loss= 5.5972328186\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 6.63562488556\n",
      "range:(4480, 4608) loss= 5.50826215744\n",
      "range:(8960, 9088) loss= 5.72483444214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 19\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 7.54218578339\n",
      "range:(4480, 4608) loss= 124.186271667\n",
      "range:(8960, 9088) loss= 588.440063477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 291.646484375\n",
      "range:(4480, 4608) loss= 86.6350631714\n",
      "range:(8960, 9088) loss= 25.6941623688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 28.4319725037\n",
      "range:(4480, 4608) loss= 22.3929958344\n",
      "range:(8960, 9088) loss= 21.4701538086\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 19.7782497406\n",
      "range:(4480, 4608) loss= 13.384062767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 14.9635801315\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 15.7625255585\n",
      "range:(4480, 4608) loss= 10.6250705719\n",
      "range:(8960, 9088) loss= 8.22618484497\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 20\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 8.8668794632\n",
      "range:(4480, 4608) loss= 7.51250696182\n",
      "range:(8960, 9088) loss= 7.45520925522\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 8.29418373108\n",
      "range:(4480, 4608) loss= 6.91253519058\n",
      "range:(8960, 9088) loss= 6.78741693497\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 6.21555948257\n",
      "range:(4480, 4608) loss= 5.28486585617\n",
      "range:(8960, 9088) loss= 5.11699104309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 5.56136798859\n",
      "range:(4480, 4608) loss= 4.35606193542\n",
      "range:(8960, 9088) loss= 4.40172576904\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4.66072320938\n",
      "range:(4480, 4608) loss= 6.61671257019\n",
      "range:(8960, 9088) loss= 4.78697729111\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 21\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 5.523624897\n",
      "range:(4480, 4608) loss= 5.34062719345\n",
      "range:(8960, 9088) loss= 4.1466999054\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3.98771762848\n",
      "range:(4480, 4608) loss= 3.72679305077\n",
      "range:(8960, 9088) loss= 3.62159442902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3.50979971886\n",
      "range:(4480, 4608) loss= 4.24637365341\n",
      "range:(8960, 9088) loss= 7.52509117126\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 21.0143699646\n",
      "range:(4480, 4608) loss= 103.386894226\n",
      "range:(8960, 9088) loss= 44.6190299988\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 32.9472465515\n",
      "range:(4480, 4608) loss= 30.2338008881\n",
      "range:(8960, 9088) loss= 9.93437480927\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 22\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 12.5311317444\n",
      "range:(4480, 4608) loss= 9.50419998169\n",
      "range:(8960, 9088) loss= 6.27233123779\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 6.61977148056\n",
      "range:(4480, 4608) loss= 5.79524898529\n",
      "range:(8960, 9088) loss= 5.2496881485\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 5.95913648605\n",
      "range:(4480, 4608) loss= 4.83423376083\n",
      "range:(8960, 9088) loss= 4.28389120102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4.09662485123\n",
      "range:(4480, 4608) loss= 3.6116399765\n",
      "range:(8960, 9088) loss= 3.66169190407\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 3.72164440155\n",
      "range:(4480, 4608) loss= 3.44246411324\n",
      "range:(8960, 9088) loss= 3.35785055161\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 23\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3.50736498833\n",
      "range:(4480, 4608) loss= 3.46339297295\n",
      "range:(8960, 9088) loss= 3.49817562103\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3.46516609192\n",
      "range:(4480, 4608) loss= 3.66687655449\n",
      "range:(8960, 9088) loss= 3.59066295624\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3.68159842491\n",
      "range:(4480, 4608) loss= 5.51646995544\n",
      "range:(8960, 9088) loss= 273.573699951\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 376.467163086\n",
      "range:(4480, 4608) loss= 79.7311172485\n",
      "range:(8960, 9088) loss= 19.6094665527\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 21.3379497528\n",
      "range:(4480, 4608) loss= 12.9197034836\n",
      "range:(8960, 9088) loss= 9.78584480286\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 24\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 11.0342388153\n",
      "range:(4480, 4608) loss= 9.53039550781\n",
      "range:(8960, 9088) loss= 6.68842744827\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 6.92874145508\n",
      "range:(4480, 4608) loss= 5.65522480011\n",
      "range:(8960, 9088) loss= 5.97705173492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 6.85321426392\n",
      "range:(4480, 4608) loss= 5.3249206543\n",
      "range:(8960, 9088) loss= 5.21092033386\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4.85146522522\n",
      "range:(4480, 4608) loss= 4.01569080353\n",
      "range:(8960, 9088) loss= 3.84343934059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4.1491355896\n",
      "range:(4480, 4608) loss= 3.65315175056\n",
      "range:(8960, 9088) loss= 3.55546569824\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 25\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3.58805656433\n",
      "range:(4480, 4608) loss= 3.39210271835\n",
      "range:(8960, 9088) loss= 3.25249052048\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3.27927494049\n",
      "range:(4480, 4608) loss= 3.2301223278\n",
      "range:(8960, 9088) loss= 3.0651512146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3.18565368652\n",
      "range:(4480, 4608) loss= 3.05045318604\n",
      "range:(8960, 9088) loss= 2.97867536545\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3.07860279083\n",
      "range:(4480, 4608) loss= 3.15401697159\n",
      "range:(8960, 9088) loss= 2.8046503067\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 3.0433549881\n",
      "range:(4480, 4608) loss= 2.95679092407\n",
      "range:(8960, 9088) loss= 2.73065805435\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 26\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 2.92603826523\n",
      "range:(4480, 4608) loss= 2.67879772186\n",
      "range:(8960, 9088) loss= 2.93407559395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3.32041025162\n",
      "range:(4480, 4608) loss= 2.66943740845\n",
      "range:(8960, 9088) loss= 2.65419220924\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3.05289053917\n",
      "range:(4480, 4608) loss= 2.81184983253\n",
      "range:(8960, 9088) loss= 2.7178235054\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2.68087363243\n",
      "range:(4480, 4608) loss= 2.48922348022\n",
      "range:(8960, 9088) loss= 3.63354182243\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 8.37744235992\n",
      "range:(4480, 4608) loss= 106.132766724\n",
      "range:(8960, 9088) loss= 29.7537631989\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 27\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 49.6660575867\n",
      "range:(4480, 4608) loss= 18.4087276459\n",
      "range:(8960, 9088) loss= 7.79830741882\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 13.1754026413\n",
      "range:(4480, 4608) loss= 9.10216140747\n",
      "range:(8960, 9088) loss= 6.01282691956\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 4.77459764481\n",
      "range:(4480, 4608) loss= 4.87480068207\n",
      "range:(8960, 9088) loss= 4.00744438171\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4.62148952484\n",
      "range:(4480, 4608) loss= 3.35300946236\n",
      "range:(8960, 9088) loss= 3.14456653595\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4.32701730728\n",
      "range:(4480, 4608) loss= 3.26278185844\n",
      "range:(8960, 9088) loss= 2.81477499008\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 28\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3.20671701431\n",
      "range:(4480, 4608) loss= 2.88720917702\n",
      "range:(8960, 9088) loss= 2.80834555626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 2.71261000633\n",
      "range:(4480, 4608) loss= 2.69363498688\n",
      "range:(8960, 9088) loss= 2.59836506844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.80008602142\n",
      "range:(4480, 4608) loss= 2.43574643135\n",
      "range:(8960, 9088) loss= 2.41517162323\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2.50346231461\n",
      "range:(4480, 4608) loss= 2.25886750221\n",
      "range:(8960, 9088) loss= 2.3448266983\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2.32379674911\n",
      "range:(4480, 4608) loss= 2.18622684479\n",
      "range:(8960, 9088) loss= 2.15054392815\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 29\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2.71601700783\n",
      "range:(4480, 4608) loss= 45.9599380493\n",
      "range:(8960, 9088) loss= 82.834197998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 55.6997489929\n",
      "range:(4480, 4608) loss= 14.9444503784\n",
      "range:(8960, 9088) loss= 7.69486522675\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 7.61568975449\n",
      "range:(4480, 4608) loss= 5.45273208618\n",
      "range:(8960, 9088) loss= 5.23422384262\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4.76820850372\n",
      "range:(4480, 4608) loss= 4.41373157501\n",
      "range:(8960, 9088) loss= 3.8493719101\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4.40402030945\n",
      "range:(4480, 4608) loss= 3.57589697838\n",
      "range:(8960, 9088) loss= 3.0471997261\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 30\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3.35476636887\n",
      "range:(4480, 4608) loss= 2.68715405464\n",
      "range:(8960, 9088) loss= 2.53097820282\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 2.63457369804\n",
      "range:(4480, 4608) loss= 2.49787306786\n",
      "range:(8960, 9088) loss= 2.44059491158\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.90675520897\n",
      "range:(4480, 4608) loss= 2.71906995773\n",
      "range:(8960, 9088) loss= 2.31564283371\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2.19216704369\n",
      "range:(4480, 4608) loss= 2.13032341003\n",
      "range:(8960, 9088) loss= 2.25955247879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2.3073143959\n",
      "range:(4480, 4608) loss= 2.1578836441\n",
      "range:(8960, 9088) loss= 2.06542420387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 31\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2.03011012077\n",
      "range:(4480, 4608) loss= 1.87773287296\n",
      "range:(8960, 9088) loss= 1.912951231\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.94630980492\n",
      "range:(4480, 4608) loss= 1.87092137337\n",
      "range:(8960, 9088) loss= 1.94176125526\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.84093546867\n",
      "range:(4480, 4608) loss= 1.75410735607\n",
      "range:(8960, 9088) loss= 1.73580348492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.74266600609\n",
      "range:(4480, 4608) loss= 1.7065294981\n",
      "range:(8960, 9088) loss= 1.68816828728\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.79949831963\n",
      "range:(4480, 4608) loss= 1.60747337341\n",
      "range:(8960, 9088) loss= 1.55645823479\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 32\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.61421263218\n",
      "range:(4480, 4608) loss= 1.50566649437\n",
      "range:(8960, 9088) loss= 1.57053351402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.65122675896\n",
      "range:(4480, 4608) loss= 1.54172372818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 1.43491435051\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.47316002846\n",
      "range:(4480, 4608) loss= 1.56347000599\n",
      "range:(8960, 9088) loss= 1.42218637466\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.37972867489\n",
      "range:(4480, 4608) loss= 1.46257972717\n",
      "range:(8960, 9088) loss= 1.4612506628\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.48942923546\n",
      "range:(4480, 4608) loss= 1.30035746098\n",
      "range:(8960, 9088) loss= 1.28616476059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 33\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.35325682163\n",
      "range:(4480, 4608) loss= 1.31376755238\n",
      "range:(8960, 9088) loss= 1.27520859241\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.44685959816\n",
      "range:(4480, 4608) loss= 1.28124499321\n",
      "range:(8960, 9088) loss= 1.37771511078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.46697366238\n",
      "range:(4480, 4608) loss= 1.24144887924\n",
      "range:(8960, 9088) loss= 1.27818346024\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.24476182461\n",
      "range:(4480, 4608) loss= 1.26195120811\n",
      "range:(8960, 9088) loss= 1.18881678581\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.21395373344\n",
      "range:(4480, 4608) loss= 1.21973335743\n",
      "range:(8960, 9088) loss= 1.23601603508\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 34\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.31912755966\n",
      "range:(4480, 4608) loss= 252.633682251\n",
      "range:(8960, 9088) loss= 43.067237854\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 31.2819213867\n",
      "range:(4480, 4608) loss= 15.606297493\n",
      "range:(8960, 9088) loss= 9.66498756409\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 10.3436098099\n",
      "range:(4480, 4608) loss= 7.86187744141\n",
      "range:(8960, 9088) loss= 6.35717821121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 5.64420080185\n",
      "range:(4480, 4608) loss= 5.29769229889\n",
      "range:(8960, 9088) loss= 4.42345809937\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4.91748952866\n",
      "range:(4480, 4608) loss= 3.7439801693\n",
      "range:(8960, 9088) loss= 3.10753059387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 35\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3.68593430519\n",
      "range:(4480, 4608) loss= 2.67429041862\n",
      "range:(8960, 9088) loss= 2.56754732132\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 2.65150594711\n",
      "range:(4480, 4608) loss= 2.55698013306\n",
      "range:(8960, 9088) loss= 2.22711420059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.6674759388\n",
      "range:(4480, 4608) loss= 2.24361157417\n",
      "range:(8960, 9088) loss= 2.18626523018\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2.21791601181\n",
      "range:(4480, 4608) loss= 1.86198842525\n",
      "range:(8960, 9088) loss= 1.92889642715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2.27948236465\n",
      "range:(4480, 4608) loss= 1.71745646\n",
      "range:(8960, 9088) loss= 1.57471466064\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 36\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.97622466087\n",
      "range:(4480, 4608) loss= 1.50927722454\n",
      "range:(8960, 9088) loss= 1.66582226753\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.62137877941\n",
      "range:(4480, 4608) loss= 1.59377169609\n",
      "range:(8960, 9088) loss= 1.45394396782\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.48293042183\n",
      "range:(4480, 4608) loss= 1.43504142761\n",
      "range:(8960, 9088) loss= 1.35502231121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.48816657066\n",
      "range:(4480, 4608) loss= 1.37534379959\n",
      "range:(8960, 9088) loss= 1.2581114769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.34623718262\n",
      "range:(4480, 4608) loss= 1.53061747551\n",
      "range:(8960, 9088) loss= 1.22711539268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 37\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.33682715893\n",
      "range:(4480, 4608) loss= 1.17948126793\n",
      "range:(8960, 9088) loss= 1.20342648029\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.24256014824\n",
      "range:(4480, 4608) loss= 1.12305283546\n",
      "range:(8960, 9088) loss= 1.1635529995\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.14412522316\n",
      "range:(4480, 4608) loss= 1.08029019833\n",
      "range:(8960, 9088) loss= 1.07945692539\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.04885554314\n",
      "range:(4480, 4608) loss= 1.05583810806\n",
      "range:(8960, 9088) loss= 0.987416088581\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.08265972137\n",
      "range:(4480, 4608) loss= 0.977374792099\n",
      "range:(8960, 9088) loss= 0.954430222511\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 38\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.18284654617\n",
      "range:(4480, 4608) loss= 0.982226133347\n",
      "range:(8960, 9088) loss= 0.943593621254\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.940838932991\n",
      "range:(4480, 4608) loss= 0.980838477612\n",
      "range:(8960, 9088) loss= 0.987712204456\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.02817547321\n",
      "range:(4480, 4608) loss= 0.893305420876\n",
      "range:(8960, 9088) loss= 1.01185822487\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 7.37496900558\n",
      "range:(4480, 4608) loss= 11.9735336304\n",
      "range:(8960, 9088) loss= 3.05859041214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 3.32922458649\n",
      "range:(4480, 4608) loss= 1.43934190273\n",
      "range:(8960, 9088) loss= 1.2346020937\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 39\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.22672116756\n",
      "range:(4480, 4608) loss= 1.02287709713\n",
      "range:(8960, 9088) loss= 1.04655373096\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.06202876568\n",
      "range:(4480, 4608) loss= 0.911342024803\n",
      "range:(8960, 9088) loss= 0.929930090904\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.913509964943\n",
      "range:(4480, 4608) loss= 0.930419564247\n",
      "range:(8960, 9088) loss= 0.85726583004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.834306955338\n",
      "range:(4480, 4608) loss= 0.829797625542\n",
      "range:(8960, 9088) loss= 0.830575704575\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.830650925636\n",
      "range:(4480, 4608) loss= 0.788909435272\n",
      "range:(8960, 9088) loss= 0.757615804672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 40\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.829596281052\n",
      "range:(4480, 4608) loss= 0.757301747799\n",
      "range:(8960, 9088) loss= 0.753922104836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.767721772194\n",
      "range:(4480, 4608) loss= 0.762159705162\n",
      "range:(8960, 9088) loss= 0.748378872871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.740169405937\n",
      "range:(4480, 4608) loss= 0.735469937325\n",
      "range:(8960, 9088) loss= 0.719865441322\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.727936267853\n",
      "range:(4480, 4608) loss= 0.712579548359\n",
      "range:(8960, 9088) loss= 0.704863667488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.710743188858\n",
      "range:(4480, 4608) loss= 0.694001317024\n",
      "range:(8960, 9088) loss= 0.68016564846\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 41\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.723049640656\n",
      "range:(4480, 4608) loss= 0.704798281193\n",
      "range:(8960, 9088) loss= 0.678089976311\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.697468817234\n",
      "range:(4480, 4608) loss= 0.676900684834\n",
      "range:(8960, 9088) loss= 1.10262465477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.22159647942\n",
      "range:(4480, 4608) loss= 1.06112957001\n",
      "range:(8960, 9088) loss= 1.05839061737\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.13125288486\n",
      "range:(4480, 4608) loss= 1.18979048729\n",
      "range:(8960, 9088) loss= 8.97183322906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4.82961273193\n",
      "range:(4480, 4608) loss= 1.92619049549\n",
      "range:(8960, 9088) loss= 0.936143517494\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 42\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.966758310795\n",
      "range:(4480, 4608) loss= 0.736076235771\n",
      "range:(8960, 9088) loss= 0.723326265812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.717195928097\n",
      "range:(4480, 4608) loss= 0.70353782177\n",
      "range:(8960, 9088) loss= 0.682310402393\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.67898952961\n",
      "range:(4480, 4608) loss= 0.711574435234\n",
      "range:(8960, 9088) loss= 0.658287525177\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.669223070145\n",
      "range:(4480, 4608) loss= 0.637410283089\n",
      "range:(8960, 9088) loss= 0.647918283939\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.656075239182\n",
      "range:(4480, 4608) loss= 0.643350958824\n",
      "range:(8960, 9088) loss= 0.624263048172\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 43\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.679783463478\n",
      "range:(4480, 4608) loss= 1.28344082832\n",
      "range:(8960, 9088) loss= 1.05653476715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.935437858105\n",
      "range:(4480, 4608) loss= 1.01582288742\n",
      "range:(8960, 9088) loss= 0.994120359421\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.950739979744\n",
      "range:(4480, 4608) loss= 1.11164045334\n",
      "range:(8960, 9088) loss= 0.987815976143\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.05811882019\n",
      "range:(4480, 4608) loss= 0.828015565872\n",
      "range:(8960, 9088) loss= 1.14855992794\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.14516556263\n",
      "range:(4480, 4608) loss= 0.99633461237\n",
      "range:(8960, 9088) loss= 0.88530421257\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 44\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.898184359074\n",
      "range:(4480, 4608) loss= 0.741670072079\n",
      "range:(8960, 9088) loss= 0.937008142471\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.11457157135\n",
      "range:(4480, 4608) loss= 1.04219579697\n",
      "range:(8960, 9088) loss= 0.793897330761\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.86071228981\n",
      "range:(4480, 4608) loss= 0.954961419106\n",
      "range:(8960, 9088) loss= 0.859490990639\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.797985732555\n",
      "range:(4480, 4608) loss= 0.685069561005\n",
      "range:(8960, 9088) loss= 1.00542092323\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.12004268169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(4480, 4608) loss= 1.10221338272\n",
      "range:(8960, 9088) loss= 7.6058716774\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 45\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 4.36077833176\n",
      "range:(4480, 4608) loss= 1.19528031349\n",
      "range:(8960, 9088) loss= 0.805088162422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.7680798769\n",
      "range:(4480, 4608) loss= 0.724374949932\n",
      "range:(8960, 9088) loss= 0.671897113323\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.662201285362\n",
      "range:(4480, 4608) loss= 0.664011001587\n",
      "range:(8960, 9088) loss= 0.643644690514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.624852001667\n",
      "range:(4480, 4608) loss= 0.623302102089\n",
      "range:(8960, 9088) loss= 0.606690526009\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.624025344849\n",
      "range:(4480, 4608) loss= 0.593287944794\n",
      "range:(8960, 9088) loss= 0.581001639366\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 46\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.597892642021\n",
      "range:(4480, 4608) loss= 0.60151052475\n",
      "range:(8960, 9088) loss= 0.877948045731\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.830530405045\n",
      "range:(4480, 4608) loss= 0.741430461407\n",
      "range:(8960, 9088) loss= 0.781186282635\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.807328760624\n",
      "range:(4480, 4608) loss= 0.800012111664\n",
      "range:(8960, 9088) loss= 0.920355677605\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.18892657757\n",
      "range:(4480, 4608) loss= 0.76803201437\n",
      "range:(8960, 9088) loss= 0.647621691227\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.650827348232\n",
      "range:(4480, 4608) loss= 0.804744541645\n",
      "range:(8960, 9088) loss= 0.928473234177\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 47\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.864346921444\n",
      "range:(4480, 4608) loss= 0.775535345078\n",
      "range:(8960, 9088) loss= 0.839317381382\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.892521619797\n",
      "range:(4480, 4608) loss= 0.712475657463\n",
      "range:(8960, 9088) loss= 0.617760837078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.61046731472\n",
      "range:(4480, 4608) loss= 0.883861541748\n",
      "range:(8960, 9088) loss= 1.10143017769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.08084654808\n",
      "range:(4480, 4608) loss= 0.675020158291\n",
      "range:(8960, 9088) loss= 0.615058720112\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.627618193626\n",
      "range:(4480, 4608) loss= 0.609523713589\n",
      "range:(8960, 9088) loss= 0.598561406136\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 48\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.660326182842\n",
      "range:(4480, 4608) loss= 0.864049732685\n",
      "range:(8960, 9088) loss= 0.822871506214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.804770350456\n",
      "range:(4480, 4608) loss= 0.684178948402\n",
      "range:(8960, 9088) loss= 0.683010458946\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.766177296638\n",
      "range:(4480, 4608) loss= 1.12661671638\n",
      "range:(8960, 9088) loss= 6.54917955399\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4.4472990036\n",
      "range:(4480, 4608) loss= 1.06967329979\n",
      "range:(8960, 9088) loss= 0.699224889278\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.69375616312\n",
      "range:(4480, 4608) loss= 0.663112044334\n",
      "range:(8960, 9088) loss= 0.630980849266\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 49\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.635860681534\n",
      "range:(4480, 4608) loss= 0.576074004173\n",
      "range:(8960, 9088) loss= 0.577400326729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.612499952316\n",
      "range:(4480, 4608) loss= 0.571426689625\n",
      "range:(8960, 9088) loss= 0.536959171295\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.539316117764\n",
      "range:(4480, 4608) loss= 0.55216217041\n",
      "range:(8960, 9088) loss= 0.6213285923\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.791152000427\n",
      "range:(4480, 4608) loss= 0.596734106541\n",
      "range:(8960, 9088) loss= 0.574475169182\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.578825592995\n",
      "range:(4480, 4608) loss= 0.544256806374\n",
      "range:(8960, 9088) loss= 0.537273406982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 50\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.561991333961\n",
      "range:(4480, 4608) loss= 0.54215991497\n",
      "range:(8960, 9088) loss= 0.537387013435\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.528100132942\n",
      "range:(4480, 4608) loss= 1.23600149155\n",
      "range:(8960, 9088) loss= 0.752216041088\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.690757870674\n",
      "range:(4480, 4608) loss= 0.58605414629\n",
      "range:(8960, 9088) loss= 0.546389341354\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.572839379311\n",
      "range:(4480, 4608) loss= 0.538237810135\n",
      "range:(8960, 9088) loss= 0.538509130478\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.572775900364\n",
      "range:(4480, 4608) loss= 0.552625179291\n",
      "range:(8960, 9088) loss= 0.524993121624\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 51\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.542522311211\n",
      "range:(4480, 4608) loss= 0.617735624313\n",
      "range:(8960, 9088) loss= 1.1561923027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.17980527878\n",
      "range:(4480, 4608) loss= 0.729610621929\n",
      "range:(8960, 9088) loss= 0.526245176792\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.527086615562\n",
      "range:(4480, 4608) loss= 0.533553600311\n",
      "range:(8960, 9088) loss= 0.5191193223\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.560496449471\n",
      "range:(4480, 4608) loss= 0.858185172081\n",
      "range:(8960, 9088) loss= 0.822158813477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.826376080513\n",
      "range:(4480, 4608) loss= 0.579652190208\n",
      "range:(8960, 9088) loss= 0.521152377129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 52\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.576189160347\n",
      "range:(4480, 4608) loss= 0.514416754246\n",
      "range:(8960, 9088) loss= 0.56492638588\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.67197650671\n",
      "range:(4480, 4608) loss= 1.04045081139\n",
      "range:(8960, 9088) loss= 0.592113435268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.598350465298\n",
      "range:(4480, 4608) loss= 0.544771134853\n",
      "range:(8960, 9088) loss= 0.514714121819\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.534645020962\n",
      "range:(4480, 4608) loss= 0.503896415234\n",
      "range:(8960, 9088) loss= 0.508796811104\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.519059181213\n",
      "range:(4480, 4608) loss= 0.528767287731\n",
      "range:(8960, 9088) loss= 0.741499483585\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 53\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.747923851013\n",
      "range:(4480, 4608) loss= 0.67853474617\n",
      "range:(8960, 9088) loss= 0.581570863724\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.566762685776\n",
      "range:(4480, 4608) loss= 0.51319861412\n",
      "range:(8960, 9088) loss= 0.503165483475\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.496844679117\n",
      "range:(4480, 4608) loss= 0.516559362411\n",
      "range:(8960, 9088) loss= 0.896565616131\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.944568157196\n",
      "range:(4480, 4608) loss= 0.627558350563\n",
      "range:(8960, 9088) loss= 0.501975953579\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.5203769207\n",
      "range:(4480, 4608) loss= 0.492372304201\n",
      "range:(8960, 9088) loss= 0.483051300049\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 54\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.487364828587\n",
      "range:(4480, 4608) loss= 0.485359638929\n",
      "range:(8960, 9088) loss= 0.492454230785\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.517611086369\n",
      "range:(4480, 4608) loss= 0.535754084587\n",
      "range:(8960, 9088) loss= 0.750393092632\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.688923478127\n",
      "range:(4480, 4608) loss= 0.663697600365\n",
      "range:(8960, 9088) loss= 0.495914518833\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.495993882418\n",
      "range:(4480, 4608) loss= 0.471393376589\n",
      "range:(8960, 9088) loss= 0.480179190636\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.484736859798\n",
      "range:(4480, 4608) loss= 0.476711660624\n",
      "range:(8960, 9088) loss= 0.483262628317\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 55\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.505812704563\n",
      "range:(4480, 4608) loss= 0.566936433315\n",
      "range:(8960, 9088) loss= 0.520603656769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.542560577393\n",
      "range:(4480, 4608) loss= 0.501175582409\n",
      "range:(8960, 9088) loss= 0.501654446125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.497582435608\n",
      "range:(4480, 4608) loss= 0.576828837395\n",
      "range:(8960, 9088) loss= 0.501437544823\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.488531023264\n",
      "range:(4480, 4608) loss= 0.470034092665\n",
      "range:(8960, 9088) loss= 0.483580946922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.504846930504\n",
      "range:(4480, 4608) loss= 0.482037365437\n",
      "range:(8960, 9088) loss= 0.486306399107\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 56\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.49390360713\n",
      "range:(4480, 4608) loss= 0.490279734135\n",
      "range:(8960, 9088) loss= 0.507258057594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.531534016132\n",
      "range:(4480, 4608) loss= 0.488569289446\n",
      "range:(8960, 9088) loss= 0.48670977354\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.476499021053\n",
      "range:(4480, 4608) loss= 0.501000761986\n",
      "range:(8960, 9088) loss= 0.482325255871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.481389641762\n",
      "range:(4480, 4608) loss= 0.47745770216\n",
      "range:(8960, 9088) loss= 0.47030633688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.493413865566\n",
      "range:(4480, 4608) loss= 0.474198639393\n",
      "range:(8960, 9088) loss= 0.467438578606\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 57\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.487147510052\n",
      "range:(4480, 4608) loss= 0.47586363554\n",
      "range:(8960, 9088) loss= 0.46451318264\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.492028415203\n",
      "range:(4480, 4608) loss= 0.497067421675\n",
      "range:(8960, 9088) loss= 0.507441759109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.488400906324\n",
      "range:(4480, 4608) loss= 0.49641802907\n",
      "range:(8960, 9088) loss= 0.468145787716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.487434327602\n",
      "range:(4480, 4608) loss= 0.465622663498\n",
      "range:(8960, 9088) loss= 0.472542673349\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.483825713396\n",
      "range:(4480, 4608) loss= 0.47651797533\n",
      "range:(8960, 9088) loss= 0.500248789787\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 58\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.490538060665\n",
      "range:(4480, 4608) loss= 0.463297247887\n",
      "range:(8960, 9088) loss= 0.472294062376\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.517045199871\n",
      "range:(4480, 4608) loss= 0.480107247829\n",
      "range:(8960, 9088) loss= 0.467770397663\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.474353134632\n",
      "range:(4480, 4608) loss= 0.472161948681\n",
      "range:(8960, 9088) loss= 0.489251255989\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.469896644354\n",
      "range:(4480, 4608) loss= 0.45779171586\n",
      "range:(8960, 9088) loss= 0.466836124659\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.486827135086\n",
      "range:(4480, 4608) loss= 0.454714238644\n",
      "range:(8960, 9088) loss= 0.473023355007\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 59\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.470639407635\n",
      "range:(4480, 4608) loss= 0.464017987251\n",
      "range:(8960, 9088) loss= 0.459803521633\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.465769559145\n",
      "range:(4480, 4608) loss= 0.465204060078\n",
      "range:(8960, 9088) loss= 0.454694271088\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.473388671875\n",
      "range:(4480, 4608) loss= 0.468242824078\n",
      "range:(8960, 9088) loss= 0.456338763237\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.462646663189\n",
      "range:(4480, 4608) loss= 0.450621068478\n",
      "range:(8960, 9088) loss= 0.457171082497\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.474968522787\n",
      "range:(4480, 4608) loss= 0.461290240288\n",
      "range:(8960, 9088) loss= 0.451835930347\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 60\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.44659948349\n",
      "range:(4480, 4608) loss= 0.458889067173\n",
      "range:(8960, 9088) loss= 0.457998991013\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.467588961124\n",
      "range:(4480, 4608) loss= 0.471193313599\n",
      "range:(8960, 9088) loss= 0.445680081844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.459551274776\n",
      "range:(4480, 4608) loss= 0.457020908594\n",
      "range:(8960, 9088) loss= 0.441454052925\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.448469042778\n",
      "range:(4480, 4608) loss= 0.448959380388\n",
      "range:(8960, 9088) loss= 0.430655360222\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.446888923645\n",
      "range:(4480, 4608) loss= 0.43926692009\n",
      "range:(8960, 9088) loss= 0.43545883894\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 61\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.431496560574\n",
      "range:(4480, 4608) loss= 0.451935470104\n",
      "range:(8960, 9088) loss= 0.446726799011\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.449054926634\n",
      "range:(4480, 4608) loss= 0.437488406897\n",
      "range:(8960, 9088) loss= 0.432870805264\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.437989920378\n",
      "range:(4480, 4608) loss= 0.439701318741\n",
      "range:(8960, 9088) loss= 0.442440062761\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.4400151968\n",
      "range:(4480, 4608) loss= 0.419303148985\n",
      "range:(8960, 9088) loss= 0.427770674229\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.436785161495\n",
      "range:(4480, 4608) loss= 0.439951837063\n",
      "range:(8960, 9088) loss= 0.413716137409\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 62\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.418231248856\n",
      "range:(4480, 4608) loss= 0.424266934395\n",
      "range:(8960, 9088) loss= 0.426691353321\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.439458370209\n",
      "range:(4480, 4608) loss= 0.425711840391\n",
      "range:(8960, 9088) loss= 0.425331145525\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.431961834431\n",
      "range:(4480, 4608) loss= 0.429495155811\n",
      "range:(8960, 9088) loss= 0.420217335224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.421338528395\n",
      "range:(4480, 4608) loss= 0.416047394276\n",
      "range:(8960, 9088) loss= 0.412074297667\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.427898466587\n",
      "range:(4480, 4608) loss= 0.417801082134\n",
      "range:(8960, 9088) loss= 0.404601633549\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 63\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.405470490456\n",
      "range:(4480, 4608) loss= 0.411410391331\n",
      "range:(8960, 9088) loss= 0.407222777605\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.424066603184\n",
      "range:(4480, 4608) loss= 0.423950254917\n",
      "range:(8960, 9088) loss= 0.412634015083\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.404535651207\n",
      "range:(4480, 4608) loss= 0.420144319534\n",
      "range:(8960, 9088) loss= 0.4089217484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.407040834427\n",
      "range:(4480, 4608) loss= 0.408520340919\n",
      "range:(8960, 9088) loss= 0.405183970928\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.416690707207\n",
      "range:(4480, 4608) loss= 0.416184365749\n",
      "range:(8960, 9088) loss= 0.403614193201\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 64\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.396458655596\n",
      "range:(4480, 4608) loss= 0.405972957611\n",
      "range:(8960, 9088) loss= 0.408661842346\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.413818359375\n",
      "range:(4480, 4608) loss= 0.412737578154\n",
      "range:(8960, 9088) loss= 0.409462511539\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.402886301279\n",
      "range:(4480, 4608) loss= 0.414634108543\n",
      "range:(8960, 9088) loss= 0.404457092285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.402274042368\n",
      "range:(4480, 4608) loss= 0.399091601372\n",
      "range:(8960, 9088) loss= 0.398717343807\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.404425323009\n",
      "range:(4480, 4608) loss= 0.40190577507\n",
      "range:(8960, 9088) loss= 0.394836932421\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 65\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.390269964933\n",
      "range:(4480, 4608) loss= 0.406288564205\n",
      "range:(8960, 9088) loss= 0.40272051096\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.411119133234\n",
      "range:(4480, 4608) loss= 0.413233608007\n",
      "range:(8960, 9088) loss= 0.409244835377\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.397184044123\n",
      "range:(4480, 4608) loss= 0.410300672054\n",
      "range:(8960, 9088) loss= 0.401470154524\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.401613950729\n",
      "range:(4480, 4608) loss= 0.403604924679\n",
      "range:(8960, 9088) loss= 0.394044458866\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.400112628937\n",
      "range:(4480, 4608) loss= 0.399183928967\n",
      "range:(8960, 9088) loss= 0.392041623592\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 66\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.386852681637\n",
      "range:(4480, 4608) loss= 0.404092311859\n",
      "range:(8960, 9088) loss= 0.400297701359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.411868214607\n",
      "range:(4480, 4608) loss= 0.408656299114\n",
      "range:(8960, 9088) loss= 0.401751577854\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.393358051777\n",
      "range:(4480, 4608) loss= 0.410706400871\n",
      "range:(8960, 9088) loss= 0.403624236584\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.398652017117\n",
      "range:(4480, 4608) loss= 0.397245943546\n",
      "range:(8960, 9088) loss= 0.390793770552\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.399983048439\n",
      "range:(4480, 4608) loss= 0.397292852402\n",
      "range:(8960, 9088) loss= 0.394153594971\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 67\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.385590791702\n",
      "range:(4480, 4608) loss= 0.402827858925\n",
      "range:(8960, 9088) loss= 0.397768348455\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.404391139746\n",
      "range:(4480, 4608) loss= 0.404373049736\n",
      "range:(8960, 9088) loss= 0.400799363852\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.391216188669\n",
      "range:(4480, 4608) loss= 0.40702933073\n",
      "range:(8960, 9088) loss= 0.401507288218\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.398809790611\n",
      "range:(4480, 4608) loss= 0.39381814003\n",
      "range:(8960, 9088) loss= 0.392237484455\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.39683291316\n",
      "range:(4480, 4608) loss= 0.393102139235\n",
      "range:(8960, 9088) loss= 0.387901902199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 68\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.382332146168\n",
      "range:(4480, 4608) loss= 0.398722678423\n",
      "range:(8960, 9088) loss= 0.3968950212\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.403821706772\n",
      "range:(4480, 4608) loss= 0.4011952281\n",
      "range:(8960, 9088) loss= 0.39990311861\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.391554445028\n",
      "range:(4480, 4608) loss= 0.404222607613\n",
      "range:(8960, 9088) loss= 0.397339046001\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.393523007631\n",
      "range:(4480, 4608) loss= 0.395816385746\n",
      "range:(8960, 9088) loss= 0.38874733448\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.393609702587\n",
      "range:(4480, 4608) loss= 0.393277078867\n",
      "range:(8960, 9088) loss= 0.385979801416\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 69\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.38085258007\n",
      "range:(4480, 4608) loss= 0.397565633059\n",
      "range:(8960, 9088) loss= 0.395089745522\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.402768850327\n",
      "range:(4480, 4608) loss= 0.401465922594\n",
      "range:(8960, 9088) loss= 0.397531747818\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.390936613083\n",
      "range:(4480, 4608) loss= 0.403003156185\n",
      "range:(8960, 9088) loss= 0.395870566368\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.392055988312\n",
      "range:(4480, 4608) loss= 0.391485124826\n",
      "range:(8960, 9088) loss= 0.387776374817\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.392530322075\n",
      "range:(4480, 4608) loss= 0.392115950584\n",
      "range:(8960, 9088) loss= 0.386073470116\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 70\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.38003924489\n",
      "range:(4480, 4608) loss= 0.397947132587\n",
      "range:(8960, 9088) loss= 0.39412355423\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.399798035622\n",
      "range:(4480, 4608) loss= 0.399213731289\n",
      "range:(8960, 9088) loss= 0.396336585283\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.387536764145\n",
      "range:(4480, 4608) loss= 0.402597427368\n",
      "range:(8960, 9088) loss= 0.393745064735\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.392425268888\n",
      "range:(4480, 4608) loss= 0.390292644501\n",
      "range:(8960, 9088) loss= 0.387356370687\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.391326963902\n",
      "range:(4480, 4608) loss= 0.390517413616\n",
      "range:(8960, 9088) loss= 0.385247498751\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 71\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.377290695906\n",
      "range:(4480, 4608) loss= 0.395492970943\n",
      "range:(8960, 9088) loss= 0.393694043159\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.399851411581\n",
      "range:(4480, 4608) loss= 0.398116767406\n",
      "range:(8960, 9088) loss= 0.395316332579\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.386812269688\n",
      "range:(4480, 4608) loss= 0.401523858309\n",
      "range:(8960, 9088) loss= 0.394163012505\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.390407681465\n",
      "range:(4480, 4608) loss= 0.389877557755\n",
      "range:(8960, 9088) loss= 0.385219156742\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.391085505486\n",
      "range:(4480, 4608) loss= 0.388892501593\n",
      "range:(8960, 9088) loss= 0.384009540081\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 72\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.376252353191\n",
      "range:(4480, 4608) loss= 0.394531875849\n",
      "range:(8960, 9088) loss= 0.39249175787\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.398600697517\n",
      "range:(4480, 4608) loss= 0.39702630043\n",
      "range:(8960, 9088) loss= 0.395221620798\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.386881440878\n",
      "range:(4480, 4608) loss= 0.40090161562\n",
      "range:(8960, 9088) loss= 0.39285749197\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.390179097652\n",
      "range:(4480, 4608) loss= 0.389511227608\n",
      "range:(8960, 9088) loss= 0.385337769985\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.389773190022\n",
      "range:(4480, 4608) loss= 0.38846385479\n",
      "range:(8960, 9088) loss= 0.382757425308\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 73\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.375479996204\n",
      "range:(4480, 4608) loss= 0.394171476364\n",
      "range:(8960, 9088) loss= 0.392122775316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.398017346859\n",
      "range:(4480, 4608) loss= 0.397671073675\n",
      "range:(8960, 9088) loss= 0.3946352005\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.385368585587\n",
      "range:(4480, 4608) loss= 0.401484698057\n",
      "range:(8960, 9088) loss= 0.392791867256\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.389750123024\n",
      "range:(4480, 4608) loss= 0.388157218695\n",
      "range:(8960, 9088) loss= 0.384899616241\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.389227479696\n",
      "range:(4480, 4608) loss= 0.388411462307\n",
      "range:(8960, 9088) loss= 0.382084727287\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 74\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.375201076269\n",
      "range:(4480, 4608) loss= 0.394083380699\n",
      "range:(8960, 9088) loss= 0.391469240189\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.39743578434\n",
      "range:(4480, 4608) loss= 0.396468698978\n",
      "range:(8960, 9088) loss= 0.393806517124\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.385152220726\n",
      "range:(4480, 4608) loss= 0.399784386158\n",
      "range:(8960, 9088) loss= 0.392144799232\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.389494001865\n",
      "range:(4480, 4608) loss= 0.388099700212\n",
      "range:(8960, 9088) loss= 0.384069174528\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.388725310564\n",
      "range:(4480, 4608) loss= 0.387645900249\n",
      "range:(8960, 9088) loss= 0.381877779961\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 75\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.374714583158\n",
      "range:(4480, 4608) loss= 0.392120689154\n",
      "range:(8960, 9088) loss= 0.386019676924\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.391242235899\n",
      "range:(4480, 4608) loss= 0.390688091516\n",
      "range:(8960, 9088) loss= 0.383018791676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.372331380844\n",
      "range:(4480, 4608) loss= 0.388514846563\n",
      "range:(8960, 9088) loss= 0.378657877445\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.378661215305\n",
      "range:(4480, 4608) loss= 0.374156117439\n",
      "range:(8960, 9088) loss= 0.366271674633\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.377181112766\n",
      "range:(4480, 4608) loss= 0.367169231176\n",
      "range:(8960, 9088) loss= 0.364394366741\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 76\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.36064183712\n",
      "range:(4480, 4608) loss= 0.369359970093\n",
      "range:(8960, 9088) loss= 0.368951737881\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.378700107336\n",
      "range:(4480, 4608) loss= 0.374031960964\n",
      "range:(8960, 9088) loss= 0.371026068926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.362809240818\n",
      "range:(4480, 4608) loss= 0.379308283329\n",
      "range:(8960, 9088) loss= 0.370144546032\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.365950882435\n",
      "range:(4480, 4608) loss= 0.36640393734\n",
      "range:(8960, 9088) loss= 0.360941827297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.37279266119\n",
      "range:(4480, 4608) loss= 0.361913740635\n",
      "range:(8960, 9088) loss= 0.361365020275\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 77\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.354074180126\n",
      "range:(4480, 4608) loss= 0.364283174276\n",
      "range:(8960, 9088) loss= 0.367216765881\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.374018549919\n",
      "range:(4480, 4608) loss= 0.370316058397\n",
      "range:(8960, 9088) loss= 0.366975724697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.358882904053\n",
      "range:(4480, 4608) loss= 0.376334965229\n",
      "range:(8960, 9088) loss= 0.367037087679\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363574415445\n",
      "range:(4480, 4608) loss= 0.36426538229\n",
      "range:(8960, 9088) loss= 0.359291642904\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.370514988899\n",
      "range:(4480, 4608) loss= 0.360251158476\n",
      "range:(8960, 9088) loss= 0.359923183918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 78\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.353440761566\n",
      "range:(4480, 4608) loss= 0.362297266722\n",
      "range:(8960, 9088) loss= 0.364897012711\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3729211092\n",
      "range:(4480, 4608) loss= 0.368810474873\n",
      "range:(8960, 9088) loss= 0.365867227316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.357557892799\n",
      "range:(4480, 4608) loss= 0.375995218754\n",
      "range:(8960, 9088) loss= 0.366303086281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362289905548\n",
      "range:(4480, 4608) loss= 0.363375127316\n",
      "range:(8960, 9088) loss= 0.357826650143\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.369946241379\n",
      "range:(4480, 4608) loss= 0.358437895775\n",
      "range:(8960, 9088) loss= 0.358624845743\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 79\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352266818285\n",
      "range:(4480, 4608) loss= 0.361709833145\n",
      "range:(8960, 9088) loss= 0.36392056942\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.369186639786\n",
      "range:(4480, 4608) loss= 0.367686659098\n",
      "range:(8960, 9088) loss= 0.365730315447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.358393251896\n",
      "range:(4480, 4608) loss= 0.374702751637\n",
      "range:(8960, 9088) loss= 0.366271734238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361951708794\n",
      "range:(4480, 4608) loss= 0.363334029913\n",
      "range:(8960, 9088) loss= 0.357375919819\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.369156837463\n",
      "range:(4480, 4608) loss= 0.358016133308\n",
      "range:(8960, 9088) loss= 0.357897698879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 80\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352889925241\n",
      "range:(4480, 4608) loss= 0.360435187817\n",
      "range:(8960, 9088) loss= 0.362300485373\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.369103252888\n",
      "range:(4480, 4608) loss= 0.367256253958\n",
      "range:(8960, 9088) loss= 0.364836752415\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.356947988272\n",
      "range:(4480, 4608) loss= 0.374224334955\n",
      "range:(8960, 9088) loss= 0.365757584572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360624551773\n",
      "range:(4480, 4608) loss= 0.362098813057\n",
      "range:(8960, 9088) loss= 0.357275754213\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.368736863136\n",
      "range:(4480, 4608) loss= 0.357670724392\n",
      "range:(8960, 9088) loss= 0.357665479183\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 81\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351397335529\n",
      "range:(4480, 4608) loss= 0.359597086906\n",
      "range:(8960, 9088) loss= 0.362078130245\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.369161188602\n",
      "range:(4480, 4608) loss= 0.367232859135\n",
      "range:(8960, 9088) loss= 0.365451693535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.357346892357\n",
      "range:(4480, 4608) loss= 0.373850554228\n",
      "range:(8960, 9088) loss= 0.365227639675\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361021101475\n",
      "range:(4480, 4608) loss= 0.361941874027\n",
      "range:(8960, 9088) loss= 0.3557035923\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.368235945702\n",
      "range:(4480, 4608) loss= 0.356874555349\n",
      "range:(8960, 9088) loss= 0.357531487942\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 82\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350113272667\n",
      "range:(4480, 4608) loss= 0.359335482121\n",
      "range:(8960, 9088) loss= 0.362311720848\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.369343936443\n",
      "range:(4480, 4608) loss= 0.366439938545\n",
      "range:(8960, 9088) loss= 0.364026844501\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355989605188\n",
      "range:(4480, 4608) loss= 0.373546183109\n",
      "range:(8960, 9088) loss= 0.363753437996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360596567392\n",
      "range:(4480, 4608) loss= 0.361469388008\n",
      "range:(8960, 9088) loss= 0.355493336916\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.368785560131\n",
      "range:(4480, 4608) loss= 0.356828331947\n",
      "range:(8960, 9088) loss= 0.356269478798\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 83\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350901663303\n",
      "range:(4480, 4608) loss= 0.35867279768\n",
      "range:(8960, 9088) loss= 0.361232161522\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.368214666843\n",
      "range:(4480, 4608) loss= 0.366028428078\n",
      "range:(8960, 9088) loss= 0.363717794418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.356677949429\n",
      "range:(4480, 4608) loss= 0.372680038214\n",
      "range:(8960, 9088) loss= 0.36346924305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359911233187\n",
      "range:(4480, 4608) loss= 0.360624730587\n",
      "range:(8960, 9088) loss= 0.355262875557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.367972910404\n",
      "range:(4480, 4608) loss= 0.355742275715\n",
      "range:(8960, 9088) loss= 0.356017947197\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 84\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350422084332\n",
      "range:(4480, 4608) loss= 0.357739120722\n",
      "range:(8960, 9088) loss= 0.361300826073\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.367693126202\n",
      "range:(4480, 4608) loss= 0.365849792957\n",
      "range:(8960, 9088) loss= 0.362775921822\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.356144726276\n",
      "range:(4480, 4608) loss= 0.3728505373\n",
      "range:(8960, 9088) loss= 0.363652348518\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359634697437\n",
      "range:(4480, 4608) loss= 0.360481441021\n",
      "range:(8960, 9088) loss= 0.355233728886\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.367565333843\n",
      "range:(4480, 4608) loss= 0.355268836021\n",
      "range:(8960, 9088) loss= 0.355809688568\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 85\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350130051374\n",
      "range:(4480, 4608) loss= 0.358186125755\n",
      "range:(8960, 9088) loss= 0.361295640469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.367038011551\n",
      "range:(4480, 4608) loss= 0.365799605846\n",
      "range:(8960, 9088) loss= 0.362720489502\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35466337204\n",
      "range:(4480, 4608) loss= 0.372485101223\n",
      "range:(8960, 9088) loss= 0.362992316484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358986139297\n",
      "range:(4480, 4608) loss= 0.360779345036\n",
      "range:(8960, 9088) loss= 0.354609340429\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.367078006268\n",
      "range:(4480, 4608) loss= 0.354880720377\n",
      "range:(8960, 9088) loss= 0.356203377247\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 86\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35010445118\n",
      "range:(4480, 4608) loss= 0.357843995094\n",
      "range:(8960, 9088) loss= 0.360656559467\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.364870578051\n",
      "range:(4480, 4608) loss= 0.365308016539\n",
      "range:(8960, 9088) loss= 0.362902343273\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354748010635\n",
      "range:(4480, 4608) loss= 0.372220695019\n",
      "range:(8960, 9088) loss= 0.363147228956\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358742237091\n",
      "range:(4480, 4608) loss= 0.360557198524\n",
      "range:(8960, 9088) loss= 0.354449033737\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.366278767586\n",
      "range:(4480, 4608) loss= 0.355368673801\n",
      "range:(8960, 9088) loss= 0.355680644512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 87\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350433170795\n",
      "range:(4480, 4608) loss= 0.35808044672\n",
      "range:(8960, 9088) loss= 0.360999166965\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.365096956491\n",
      "range:(4480, 4608) loss= 0.364851593971\n",
      "range:(8960, 9088) loss= 0.363022953272\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354726284742\n",
      "range:(4480, 4608) loss= 0.372388631105\n",
      "range:(8960, 9088) loss= 0.362349867821\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35830706358\n",
      "range:(4480, 4608) loss= 0.359970033169\n",
      "range:(8960, 9088) loss= 0.354022145271\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36706674099\n",
      "range:(4480, 4608) loss= 0.35465413332\n",
      "range:(8960, 9088) loss= 0.355594336987\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 88\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349346995354\n",
      "range:(4480, 4608) loss= 0.357542335987\n",
      "range:(8960, 9088) loss= 0.360210776329\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.364731013775\n",
      "range:(4480, 4608) loss= 0.364786297083\n",
      "range:(8960, 9088) loss= 0.36257481575\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355176448822\n",
      "range:(4480, 4608) loss= 0.371299624443\n",
      "range:(8960, 9088) loss= 0.362136691809\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358260929585\n",
      "range:(4480, 4608) loss= 0.359352111816\n",
      "range:(8960, 9088) loss= 0.35407859087\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36629062891\n",
      "range:(4480, 4608) loss= 0.354651808739\n",
      "range:(8960, 9088) loss= 0.355090022087\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 89\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349740326405\n",
      "range:(4480, 4608) loss= 0.356665313244\n",
      "range:(8960, 9088) loss= 0.360273778439\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.365050643682\n",
      "range:(4480, 4608) loss= 0.364505589008\n",
      "range:(8960, 9088) loss= 0.362019389868\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355832666159\n",
      "range:(4480, 4608) loss= 0.371113598347\n",
      "range:(8960, 9088) loss= 0.362154960632\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358280479908\n",
      "range:(4480, 4608) loss= 0.358827590942\n",
      "range:(8960, 9088) loss= 0.353848814964\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.366086244583\n",
      "range:(4480, 4608) loss= 0.35422539711\n",
      "range:(8960, 9088) loss= 0.35500267148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 90\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347762703896\n",
      "range:(4480, 4608) loss= 0.356303840876\n",
      "range:(8960, 9088) loss= 0.359740495682\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.363598436117\n",
      "range:(4480, 4608) loss= 0.36383587122\n",
      "range:(8960, 9088) loss= 0.360818713903\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354617863894\n",
      "range:(4480, 4608) loss= 0.370694875717\n",
      "range:(8960, 9088) loss= 0.361897051334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357372164726\n",
      "range:(4480, 4608) loss= 0.359175503254\n",
      "range:(8960, 9088) loss= 0.353884249926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.366995453835\n",
      "range:(4480, 4608) loss= 0.354908257723\n",
      "range:(8960, 9088) loss= 0.354815304279\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 91\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347248703241\n",
      "range:(4480, 4608) loss= 0.355754107237\n",
      "range:(8960, 9088) loss= 0.359002530575\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.363623768091\n",
      "range:(4480, 4608) loss= 0.364063411951\n",
      "range:(8960, 9088) loss= 0.360998630524\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354737490416\n",
      "range:(4480, 4608) loss= 0.371016293764\n",
      "range:(8960, 9088) loss= 0.361454755068\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358000844717\n",
      "range:(4480, 4608) loss= 0.358286559582\n",
      "range:(8960, 9088) loss= 0.353853583336\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.365615993738\n",
      "range:(4480, 4608) loss= 0.353874504566\n",
      "range:(8960, 9088) loss= 0.3547514081\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 92\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347747474909\n",
      "range:(4480, 4608) loss= 0.355890452862\n",
      "range:(8960, 9088) loss= 0.359295547009\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.363500416279\n",
      "range:(4480, 4608) loss= 0.364025145769\n",
      "range:(8960, 9088) loss= 0.360701233149\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355388641357\n",
      "range:(4480, 4608) loss= 0.371302306652\n",
      "range:(8960, 9088) loss= 0.361374914646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358513563871\n",
      "range:(4480, 4608) loss= 0.358702510595\n",
      "range:(8960, 9088) loss= 0.353700101376\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.365067005157\n",
      "range:(4480, 4608) loss= 0.353909909725\n",
      "range:(8960, 9088) loss= 0.355203330517\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 93\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.346846282482\n",
      "range:(4480, 4608) loss= 0.356323719025\n",
      "range:(8960, 9088) loss= 0.358956903219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.363638341427\n",
      "range:(4480, 4608) loss= 0.363914728165\n",
      "range:(8960, 9088) loss= 0.36048835516\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354181170464\n",
      "range:(4480, 4608) loss= 0.370881736279\n",
      "range:(8960, 9088) loss= 0.361820101738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359255701303\n",
      "range:(4480, 4608) loss= 0.358159929514\n",
      "range:(8960, 9088) loss= 0.353265345097\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.365496993065\n",
      "range:(4480, 4608) loss= 0.353053301573\n",
      "range:(8960, 9088) loss= 0.35486894846\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 94\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34592717886\n",
      "range:(4480, 4608) loss= 0.35564750433\n",
      "range:(8960, 9088) loss= 0.358723104\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.365524530411\n",
      "range:(4480, 4608) loss= 0.363697856665\n",
      "range:(8960, 9088) loss= 0.360478937626\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354759007692\n",
      "range:(4480, 4608) loss= 0.370839595795\n",
      "range:(8960, 9088) loss= 0.361482769251\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35776796937\n",
      "range:(4480, 4608) loss= 0.358216762543\n",
      "range:(8960, 9088) loss= 0.353283941746\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.364710628986\n",
      "range:(4480, 4608) loss= 0.352907121181\n",
      "range:(8960, 9088) loss= 0.354478538036\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 95\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.346193253994\n",
      "range:(4480, 4608) loss= 0.354989737272\n",
      "range:(8960, 9088) loss= 0.358688920736\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.365386128426\n",
      "range:(4480, 4608) loss= 0.362732201815\n",
      "range:(8960, 9088) loss= 0.360033035278\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354348659515\n",
      "range:(4480, 4608) loss= 0.370700478554\n",
      "range:(8960, 9088) loss= 0.361165970564\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357994019985\n",
      "range:(4480, 4608) loss= 0.357935547829\n",
      "range:(8960, 9088) loss= 0.353588432074\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.365372151136\n",
      "range:(4480, 4608) loss= 0.352961152792\n",
      "range:(8960, 9088) loss= 0.354415953159\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 96\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.346263468266\n",
      "range:(4480, 4608) loss= 0.356122821569\n",
      "range:(8960, 9088) loss= 0.358462810516\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.362833589315\n",
      "range:(4480, 4608) loss= 0.363030254841\n",
      "range:(8960, 9088) loss= 0.359674632549\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355335354805\n",
      "range:(4480, 4608) loss= 0.371304690838\n",
      "range:(8960, 9088) loss= 0.360737621784\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35749784112\n",
      "range:(4480, 4608) loss= 0.357713401318\n",
      "range:(8960, 9088) loss= 0.353775203228\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.364728391171\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 0\n",
    "    for ep in range(no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(np.ceil(float(len(batch_images)) / min_batch_size))):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                if(index % 35 ==0):\n",
    "                    print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_3\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
