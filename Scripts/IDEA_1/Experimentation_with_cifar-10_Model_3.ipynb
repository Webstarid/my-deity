{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 100 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 5 # all kernels are 5x5\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 128 # we look at 5000 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4ySKG7/jJIobv+Mkyv8Hi9ZxodGWVz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f598b342650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAEVCAYAAADZzOErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfBzHSYIHH8dxkuDBx3GcJHjwcRwnCf8fVNyNDJKQrUUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59d4c191d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEVCAYAAAAPaTtOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx8kw7gAcJ8O4A3CcDPP/ASJUfUIkPu9PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f598ee01490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7zgZxZ3fcTKKO7/jZBR3fsfJKP8f10uBJBimGW4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f598ebdc490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypefloat32\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[ 0.22745098  0.25490198  0.25098041]\n",
      " [ 0.22745098  0.21568628  0.22745098]\n",
      " [ 0.27058825  0.25490198  0.27058825]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59c86a8bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2, tf.shape(a1), tf.shape(a2), tf.shape(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_, sha1, sha2, sha3 = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(inp, sha1, sha2, sha3):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, sha3, \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, sha2,\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, sha1,\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.shape(tf_input),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_, sha1, sha2, sha3)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector, sha1, sha2, sha3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../Models/IDEA_1/Model_cifar_3/model_cifar_3-190\n",
      "epoch: 195\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343792796135\n",
      "range:(4480, 4608) loss= 0.355322062969\n",
      "range:(8960, 9088) loss= 0.359214276075\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.362949430943\n",
      "range:(4480, 4608) loss= 0.363498151302\n",
      "range:(8960, 9088) loss= 0.356970518827\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355767458677\n",
      "range:(4480, 4608) loss= 0.366467535496\n",
      "range:(8960, 9088) loss= 0.358683049679\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.355846583843\n",
      "range:(4480, 4608) loss= 0.357203900814\n",
      "range:(8960, 9088) loss= 0.350586235523\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361387252808\n",
      "range:(4480, 4608) loss= 0.355189442635\n",
      "range:(8960, 9088) loss= 0.353049337864\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 196\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349212110043\n",
      "range:(4480, 4608) loss= 0.354460060596\n",
      "range:(8960, 9088) loss= 0.352017939091\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360662996769\n",
      "range:(4480, 4608) loss= 0.362693548203\n",
      "range:(8960, 9088) loss= 0.355278849602\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355680882931\n",
      "range:(4480, 4608) loss= 0.370356500149\n",
      "range:(8960, 9088) loss= 0.36154294014\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358357429504\n",
      "range:(4480, 4608) loss= 0.360339343548\n",
      "range:(8960, 9088) loss= 0.351861536503\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362102419138\n",
      "range:(4480, 4608) loss= 0.351233869791\n",
      "range:(8960, 9088) loss= 0.353929340839\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 197\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.353813737631\n",
      "range:(4480, 4608) loss= 0.361824601889\n",
      "range:(8960, 9088) loss= 0.357644915581\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.366382032633\n",
      "range:(4480, 4608) loss= 0.362650632858\n",
      "range:(8960, 9088) loss= 0.357234030962\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351236879826\n",
      "range:(4480, 4608) loss= 0.366897940636\n",
      "range:(8960, 9088) loss= 0.360592126846\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352470815182\n",
      "range:(4480, 4608) loss= 0.357339173555\n",
      "range:(8960, 9088) loss= 0.349592208862\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361452460289\n",
      "range:(4480, 4608) loss= 0.352213382721\n",
      "range:(8960, 9088) loss= 0.353877931833\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 198\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348050832748\n",
      "range:(4480, 4608) loss= 0.362502336502\n",
      "range:(8960, 9088) loss= 0.358854323626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.366861343384\n",
      "range:(4480, 4608) loss= 0.359529793262\n",
      "range:(8960, 9088) loss= 0.357899487019\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354742914438\n",
      "range:(4480, 4608) loss= 0.375125229359\n",
      "range:(8960, 9088) loss= 0.37021318078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.356495022774\n",
      "range:(4480, 4608) loss= 0.358674824238\n",
      "range:(8960, 9088) loss= 0.356578528881\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.363622426987\n",
      "range:(4480, 4608) loss= 0.354823827744\n",
      "range:(8960, 9088) loss= 0.355190038681\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 199\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352926462889\n",
      "range:(4480, 4608) loss= 0.362635165453\n",
      "range:(8960, 9088) loss= 0.361586391926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.368200600147\n",
      "range:(4480, 4608) loss= 0.361766517162\n",
      "range:(8960, 9088) loss= 0.360655277967\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.353734523058\n",
      "range:(4480, 4608) loss= 0.366050124168\n",
      "range:(8960, 9088) loss= 0.361530780792\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.356179684401\n",
      "range:(4480, 4608) loss= 0.353948891163\n",
      "range:(8960, 9088) loss= 0.352318376303\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362329125404\n",
      "range:(4480, 4608) loss= 0.350285589695\n",
      "range:(8960, 9088) loss= 0.354207724333\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 200\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342292666435\n",
      "range:(4480, 4608) loss= 0.358231633902\n",
      "range:(8960, 9088) loss= 0.359045982361\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.361354082823\n",
      "range:(4480, 4608) loss= 0.361948490143\n",
      "range:(8960, 9088) loss= 0.362275063992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.362329959869\n",
      "range:(4480, 4608) loss= 0.369969576597\n",
      "range:(8960, 9088) loss= 0.364399492741\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35565662384\n",
      "range:(4480, 4608) loss= 0.360201001167\n",
      "range:(8960, 9088) loss= 0.352165102959\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.364021062851\n",
      "range:(4480, 4608) loss= 0.355116993189\n",
      "range:(8960, 9088) loss= 0.357063651085\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 201\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.345670640469\n",
      "range:(4480, 4608) loss= 0.355989277363\n",
      "range:(8960, 9088) loss= 0.355217754841\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360912084579\n",
      "range:(4480, 4608) loss= 0.362090289593\n",
      "range:(8960, 9088) loss= 0.354876935482\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351836502552\n",
      "range:(4480, 4608) loss= 0.365742325783\n",
      "range:(8960, 9088) loss= 0.364597022533\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.353614389896\n",
      "range:(4480, 4608) loss= 0.354496717453\n",
      "range:(8960, 9088) loss= 0.349349290133\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359209775925\n",
      "range:(4480, 4608) loss= 0.353247463703\n",
      "range:(8960, 9088) loss= 0.35198777914\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 202\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34419387579\n",
      "range:(4480, 4608) loss= 0.353589057922\n",
      "range:(8960, 9088) loss= 0.355845153332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.363814860582\n",
      "range:(4480, 4608) loss= 0.365972161293\n",
      "range:(8960, 9088) loss= 0.355942755938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35205578804\n",
      "range:(4480, 4608) loss= 0.367399156094\n",
      "range:(8960, 9088) loss= 0.360113799572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354879438877\n",
      "range:(4480, 4608) loss= 0.361076295376\n",
      "range:(8960, 9088) loss= 0.355143070221\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.364354372025\n",
      "range:(4480, 4608) loss= 0.360606700182\n",
      "range:(8960, 9088) loss= 0.356100380421\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 203\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347147405148\n",
      "range:(4480, 4608) loss= 0.356706023216\n",
      "range:(8960, 9088) loss= 0.359030067921\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.36477380991\n",
      "range:(4480, 4608) loss= 0.364591836929\n",
      "range:(8960, 9088) loss= 0.360270321369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.357089489698\n",
      "range:(4480, 4608) loss= 0.367680817842\n",
      "range:(8960, 9088) loss= 0.359036177397\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354875296354\n",
      "range:(4480, 4608) loss= 0.355917096138\n",
      "range:(8960, 9088) loss= 0.350942850113\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361432939768\n",
      "range:(4480, 4608) loss= 0.352137804031\n",
      "range:(8960, 9088) loss= 0.351078689098\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 204\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343844354153\n",
      "range:(4480, 4608) loss= 0.353548705578\n",
      "range:(8960, 9088) loss= 0.354680776596\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359901785851\n",
      "range:(4480, 4608) loss= 0.362400531769\n",
      "range:(8960, 9088) loss= 0.354629814625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354499071836\n",
      "range:(4480, 4608) loss= 0.368004381657\n",
      "range:(8960, 9088) loss= 0.361182749271\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.353376597166\n",
      "range:(4480, 4608) loss= 0.352721512318\n",
      "range:(8960, 9088) loss= 0.358307123184\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.363531917334\n",
      "range:(4480, 4608) loss= 0.352657645941\n",
      "range:(8960, 9088) loss= 0.35220426321\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 205\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344354867935\n",
      "range:(4480, 4608) loss= 0.356698870659\n",
      "range:(8960, 9088) loss= 0.353286445141\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359448969364\n",
      "range:(4480, 4608) loss= 0.361463725567\n",
      "range:(8960, 9088) loss= 0.359352231026\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350876569748\n",
      "range:(4480, 4608) loss= 0.365925580263\n",
      "range:(8960, 9088) loss= 0.359460115433\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.353225111961\n",
      "range:(4480, 4608) loss= 0.354525625706\n",
      "range:(8960, 9088) loss= 0.349729835987\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360766112804\n",
      "range:(4480, 4608) loss= 0.35318171978\n",
      "range:(8960, 9088) loss= 0.350293755531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 206\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34108042717\n",
      "range:(4480, 4608) loss= 0.353311896324\n",
      "range:(8960, 9088) loss= 0.354780256748\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359603613615\n",
      "range:(4480, 4608) loss= 0.359600841999\n",
      "range:(8960, 9088) loss= 0.354755669832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354867756367\n",
      "range:(4480, 4608) loss= 0.368427991867\n",
      "range:(8960, 9088) loss= 0.369912445545\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.368355572224\n",
      "range:(4480, 4608) loss= 0.360729694366\n",
      "range:(8960, 9088) loss= 0.355015963316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361756980419\n",
      "range:(4480, 4608) loss= 0.353022128344\n",
      "range:(8960, 9088) loss= 0.352343857288\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 207\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.345370292664\n",
      "range:(4480, 4608) loss= 0.352411448956\n",
      "range:(8960, 9088) loss= 0.352737993002\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.357796490192\n",
      "range:(4480, 4608) loss= 0.361011147499\n",
      "range:(8960, 9088) loss= 0.358576416969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354733109474\n",
      "range:(4480, 4608) loss= 0.369698941708\n",
      "range:(8960, 9088) loss= 0.358656197786\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354722946882\n",
      "range:(4480, 4608) loss= 0.352879136801\n",
      "range:(8960, 9088) loss= 0.356660068035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362539947033\n",
      "range:(4480, 4608) loss= 0.351125955582\n",
      "range:(8960, 9088) loss= 0.353588283062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 208\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343350827694\n",
      "range:(4480, 4608) loss= 0.357262432575\n",
      "range:(8960, 9088) loss= 0.353544235229\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359337329865\n",
      "range:(4480, 4608) loss= 0.360765874386\n",
      "range:(8960, 9088) loss= 0.354488968849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34981238842\n",
      "range:(4480, 4608) loss= 0.366414785385\n",
      "range:(8960, 9088) loss= 0.358115553856\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352077960968\n",
      "range:(4480, 4608) loss= 0.35361123085\n",
      "range:(8960, 9088) loss= 0.348526656628\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360218405724\n",
      "range:(4480, 4608) loss= 0.348976582289\n",
      "range:(8960, 9088) loss= 0.351387679577\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 209\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342674434185\n",
      "range:(4480, 4608) loss= 0.350676208735\n",
      "range:(8960, 9088) loss= 0.353442698717\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359019517899\n",
      "range:(4480, 4608) loss= 0.357733696699\n",
      "range:(8960, 9088) loss= 0.355278372765\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349789738655\n",
      "range:(4480, 4608) loss= 0.365672886372\n",
      "range:(8960, 9088) loss= 0.358613550663\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350784957409\n",
      "range:(4480, 4608) loss= 0.354111433029\n",
      "range:(8960, 9088) loss= 0.351521402597\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360622227192\n",
      "range:(4480, 4608) loss= 0.348713219166\n",
      "range:(8960, 9088) loss= 0.353619515896\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 210\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.345365226269\n",
      "range:(4480, 4608) loss= 0.351601362228\n",
      "range:(8960, 9088) loss= 0.354484468699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360328674316\n",
      "range:(4480, 4608) loss= 0.362122833729\n",
      "range:(8960, 9088) loss= 0.356096327305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350603580475\n",
      "range:(4480, 4608) loss= 0.364586353302\n",
      "range:(8960, 9088) loss= 0.360815942287\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35080987215\n",
      "range:(4480, 4608) loss= 0.356496036053\n",
      "range:(8960, 9088) loss= 0.348937749863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359029173851\n",
      "range:(4480, 4608) loss= 0.349863111973\n",
      "range:(8960, 9088) loss= 0.352535068989\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 211\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342768073082\n",
      "range:(4480, 4608) loss= 0.355190396309\n",
      "range:(8960, 9088) loss= 0.353648543358\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359944760799\n",
      "range:(4480, 4608) loss= 0.359304308891\n",
      "range:(8960, 9088) loss= 0.354712843895\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.352431863546\n",
      "range:(4480, 4608) loss= 0.364131450653\n",
      "range:(8960, 9088) loss= 0.357422292233\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350621700287\n",
      "range:(4480, 4608) loss= 0.354990929365\n",
      "range:(8960, 9088) loss= 0.350566327572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360892593861\n",
      "range:(4480, 4608) loss= 0.360887378454\n",
      "range:(8960, 9088) loss= 0.353045463562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 212\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.345276653767\n",
      "range:(4480, 4608) loss= 0.355156779289\n",
      "range:(8960, 9088) loss= 0.35496789217\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360392689705\n",
      "range:(4480, 4608) loss= 0.358295857906\n",
      "range:(8960, 9088) loss= 0.354389965534\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348309636116\n",
      "range:(4480, 4608) loss= 0.364804118872\n",
      "range:(8960, 9088) loss= 0.358324706554\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351049244404\n",
      "range:(4480, 4608) loss= 0.352013885975\n",
      "range:(8960, 9088) loss= 0.348033666611\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358402311802\n",
      "range:(4480, 4608) loss= 0.34927880764\n",
      "range:(8960, 9088) loss= 0.350406736135\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 213\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342820346355\n",
      "range:(4480, 4608) loss= 0.351772218943\n",
      "range:(8960, 9088) loss= 0.352477550507\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.362411856651\n",
      "range:(4480, 4608) loss= 0.360377430916\n",
      "range:(8960, 9088) loss= 0.356607377529\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351015120745\n",
      "range:(4480, 4608) loss= 0.365227133036\n",
      "range:(8960, 9088) loss= 0.357100307941\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.348815619946\n",
      "range:(4480, 4608) loss= 0.351892411709\n",
      "range:(8960, 9088) loss= 0.347262084484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357725083828\n",
      "range:(4480, 4608) loss= 0.347971916199\n",
      "range:(8960, 9088) loss= 0.349680900574\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 214\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344001442194\n",
      "range:(4480, 4608) loss= 0.352448284626\n",
      "range:(8960, 9088) loss= 0.352188766003\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.362756788731\n",
      "range:(4480, 4608) loss= 0.359403610229\n",
      "range:(8960, 9088) loss= 0.355579286814\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349402606487\n",
      "range:(4480, 4608) loss= 0.366291105747\n",
      "range:(8960, 9088) loss= 0.357749730349\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.34894412756\n",
      "range:(4480, 4608) loss= 0.351159274578\n",
      "range:(8960, 9088) loss= 0.350392580032\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358532577753\n",
      "range:(4480, 4608) loss= 0.347777605057\n",
      "range:(8960, 9088) loss= 0.350385636091\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 215\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34334653616\n",
      "range:(4480, 4608) loss= 0.353727638721\n",
      "range:(8960, 9088) loss= 0.352400064468\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.361011654139\n",
      "range:(4480, 4608) loss= 0.360732734203\n",
      "range:(8960, 9088) loss= 0.354736447334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347876429558\n",
      "range:(4480, 4608) loss= 0.364928305149\n",
      "range:(8960, 9088) loss= 0.359925925732\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357233285904\n",
      "range:(4480, 4608) loss= 0.361743986607\n",
      "range:(8960, 9088) loss= 0.357785046101\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.369191169739\n",
      "range:(4480, 4608) loss= 0.35767531395\n",
      "range:(8960, 9088) loss= 0.357359051704\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 216\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.345885962248\n",
      "range:(4480, 4608) loss= 0.354022681713\n",
      "range:(8960, 9088) loss= 0.353254139423\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.361394017935\n",
      "range:(4480, 4608) loss= 0.358728826046\n",
      "range:(8960, 9088) loss= 0.354203015566\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348401755095\n",
      "range:(4480, 4608) loss= 0.364398300648\n",
      "range:(8960, 9088) loss= 0.357880294323\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350721359253\n",
      "range:(4480, 4608) loss= 0.353178977966\n",
      "range:(8960, 9088) loss= 0.347285509109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357958555222\n",
      "range:(4480, 4608) loss= 0.347977817059\n",
      "range:(8960, 9088) loss= 0.350077271461\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 217\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.341863334179\n",
      "range:(4480, 4608) loss= 0.352230399847\n",
      "range:(8960, 9088) loss= 0.352423727512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.36136418581\n",
      "range:(4480, 4608) loss= 0.362303316593\n",
      "range:(8960, 9088) loss= 0.358609557152\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.353746950626\n",
      "range:(4480, 4608) loss= 0.371443510056\n",
      "range:(8960, 9088) loss= 0.360714733601\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.355296462774\n",
      "range:(4480, 4608) loss= 0.353503286839\n",
      "range:(8960, 9088) loss= 0.348237007856\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357226610184\n",
      "range:(4480, 4608) loss= 0.350907951593\n",
      "range:(8960, 9088) loss= 0.350058853626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 218\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344891726971\n",
      "range:(4480, 4608) loss= 0.352090954781\n",
      "range:(8960, 9088) loss= 0.354112923145\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359006822109\n",
      "range:(4480, 4608) loss= 0.365489661694\n",
      "range:(8960, 9088) loss= 0.361896008253\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35606533289\n",
      "range:(4480, 4608) loss= 0.367106616497\n",
      "range:(8960, 9088) loss= 0.358654379845\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352706521749\n",
      "range:(4480, 4608) loss= 0.357100665569\n",
      "range:(8960, 9088) loss= 0.352297395468\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358608782291\n",
      "range:(4480, 4608) loss= 0.349309742451\n",
      "range:(8960, 9088) loss= 0.351734876633\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 219\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342814087868\n",
      "range:(4480, 4608) loss= 0.354568183422\n",
      "range:(8960, 9088) loss= 0.353770524263\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360933840275\n",
      "range:(4480, 4608) loss= 0.357161939144\n",
      "range:(8960, 9088) loss= 0.355906367302\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349580466747\n",
      "range:(4480, 4608) loss= 0.363986670971\n",
      "range:(8960, 9088) loss= 0.35747808218\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349963605404\n",
      "range:(4480, 4608) loss= 0.353509038687\n",
      "range:(8960, 9088) loss= 0.3467412889\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35816141963\n",
      "range:(4480, 4608) loss= 0.350136488676\n",
      "range:(8960, 9088) loss= 0.351942181587\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 220\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342668503523\n",
      "range:(4480, 4608) loss= 0.354640364647\n",
      "range:(8960, 9088) loss= 0.351492226124\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359374254942\n",
      "range:(4480, 4608) loss= 0.360870063305\n",
      "range:(8960, 9088) loss= 0.354860305786\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349309623241\n",
      "range:(4480, 4608) loss= 0.365001887083\n",
      "range:(8960, 9088) loss= 0.35667270422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350559830666\n",
      "range:(4480, 4608) loss= 0.352251946926\n",
      "range:(8960, 9088) loss= 0.350450992584\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358762830496\n",
      "range:(4480, 4608) loss= 0.348216235638\n",
      "range:(8960, 9088) loss= 0.349794507027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 221\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342903614044\n",
      "range:(4480, 4608) loss= 0.350937783718\n",
      "range:(8960, 9088) loss= 0.353244334459\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358850359917\n",
      "range:(4480, 4608) loss= 0.360512405634\n",
      "range:(8960, 9088) loss= 0.359117925167\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.353532373905\n",
      "range:(4480, 4608) loss= 0.366098284721\n",
      "range:(8960, 9088) loss= 0.363666832447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36726629734\n",
      "range:(4480, 4608) loss= 0.367346763611\n",
      "range:(8960, 9088) loss= 0.354436397552\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36710357666\n",
      "range:(4480, 4608) loss= 0.352358341217\n",
      "range:(8960, 9088) loss= 0.352981388569\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 222\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344039976597\n",
      "range:(4480, 4608) loss= 0.354290872812\n",
      "range:(8960, 9088) loss= 0.352553963661\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.361574411392\n",
      "range:(4480, 4608) loss= 0.359609127045\n",
      "range:(8960, 9088) loss= 0.354716420174\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348632961512\n",
      "range:(4480, 4608) loss= 0.364371806383\n",
      "range:(8960, 9088) loss= 0.356843292713\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350036382675\n",
      "range:(4480, 4608) loss= 0.351822733879\n",
      "range:(8960, 9088) loss= 0.349024772644\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360898882151\n",
      "range:(4480, 4608) loss= 0.348771870136\n",
      "range:(8960, 9088) loss= 0.350346207619\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 223\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.345952242613\n",
      "range:(4480, 4608) loss= 0.354416787624\n",
      "range:(8960, 9088) loss= 0.351580172777\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360048085451\n",
      "range:(4480, 4608) loss= 0.359684020281\n",
      "range:(8960, 9088) loss= 0.358562141657\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354156345129\n",
      "range:(4480, 4608) loss= 0.36611866951\n",
      "range:(8960, 9088) loss= 0.357650756836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352917820215\n",
      "range:(4480, 4608) loss= 0.355696678162\n",
      "range:(8960, 9088) loss= 0.348157554865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360198915005\n",
      "range:(4480, 4608) loss= 0.357406020164\n",
      "range:(8960, 9088) loss= 0.352716386318\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 224\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.346868097782\n",
      "range:(4480, 4608) loss= 0.352766156197\n",
      "range:(8960, 9088) loss= 0.354315876961\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.364666342735\n",
      "range:(4480, 4608) loss= 0.361553370953\n",
      "range:(8960, 9088) loss= 0.355037719011\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35129365325\n",
      "range:(4480, 4608) loss= 0.379319608212\n",
      "range:(8960, 9088) loss= 0.368951052427\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.365595281124\n",
      "range:(4480, 4608) loss= 0.368461310863\n",
      "range:(8960, 9088) loss= 0.352361232042\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360159546137\n",
      "range:(4480, 4608) loss= 0.351361393929\n",
      "range:(8960, 9088) loss= 0.356496989727\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 225\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344227313995\n",
      "range:(4480, 4608) loss= 0.35543975234\n",
      "range:(8960, 9088) loss= 0.352564632893\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.363471716642\n",
      "range:(4480, 4608) loss= 0.361623674631\n",
      "range:(8960, 9088) loss= 0.359351038933\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.355883836746\n",
      "range:(4480, 4608) loss= 0.36830997467\n",
      "range:(8960, 9088) loss= 0.363836824894\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352307736874\n",
      "range:(4480, 4608) loss= 0.358501434326\n",
      "range:(8960, 9088) loss= 0.352846413851\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361002624035\n",
      "range:(4480, 4608) loss= 0.34982830286\n",
      "range:(8960, 9088) loss= 0.3544511199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 226\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342121481895\n",
      "range:(4480, 4608) loss= 0.357165694237\n",
      "range:(8960, 9088) loss= 0.354697465897\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360650360584\n",
      "range:(4480, 4608) loss= 0.356796354055\n",
      "range:(8960, 9088) loss= 0.357861816883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351826786995\n",
      "range:(4480, 4608) loss= 0.365545272827\n",
      "range:(8960, 9088) loss= 0.358679652214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352225601673\n",
      "range:(4480, 4608) loss= 0.357706815004\n",
      "range:(8960, 9088) loss= 0.350311070681\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362654238939\n",
      "range:(4480, 4608) loss= 0.353882789612\n",
      "range:(8960, 9088) loss= 0.352992266417\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 227\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.341299444437\n",
      "range:(4480, 4608) loss= 0.352667272091\n",
      "range:(8960, 9088) loss= 0.352605462074\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360001444817\n",
      "range:(4480, 4608) loss= 0.359970748425\n",
      "range:(8960, 9088) loss= 0.354526877403\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349981606007\n",
      "range:(4480, 4608) loss= 0.364142149687\n",
      "range:(8960, 9088) loss= 0.356976091862\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349870204926\n",
      "range:(4480, 4608) loss= 0.353024303913\n",
      "range:(8960, 9088) loss= 0.350702792406\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358815938234\n",
      "range:(4480, 4608) loss= 0.350982904434\n",
      "range:(8960, 9088) loss= 0.351812422276\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 228\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343109607697\n",
      "range:(4480, 4608) loss= 0.358643233776\n",
      "range:(8960, 9088) loss= 0.356754541397\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.36059731245\n",
      "range:(4480, 4608) loss= 0.361137986183\n",
      "range:(8960, 9088) loss= 0.356869071722\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351361453533\n",
      "range:(4480, 4608) loss= 0.365728616714\n",
      "range:(8960, 9088) loss= 0.359131932259\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35056489706\n",
      "range:(4480, 4608) loss= 0.353277921677\n",
      "range:(8960, 9088) loss= 0.348970592022\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360617160797\n",
      "range:(4480, 4608) loss= 0.349145591259\n",
      "range:(8960, 9088) loss= 0.354326277971\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 229\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343047857285\n",
      "range:(4480, 4608) loss= 0.35241189599\n",
      "range:(8960, 9088) loss= 0.351605087519\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358512282372\n",
      "range:(4480, 4608) loss= 0.357610344887\n",
      "range:(8960, 9088) loss= 0.354607284069\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349190860987\n",
      "range:(4480, 4608) loss= 0.364263832569\n",
      "range:(8960, 9088) loss= 0.35970556736\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350958943367\n",
      "range:(4480, 4608) loss= 0.354834675789\n",
      "range:(8960, 9088) loss= 0.351156949997\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.364339828491\n",
      "range:(4480, 4608) loss= 0.349669843912\n",
      "range:(8960, 9088) loss= 0.351390600204\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 230\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344586908817\n",
      "range:(4480, 4608) loss= 0.354571402073\n",
      "range:(8960, 9088) loss= 0.352304458618\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359831959009\n",
      "range:(4480, 4608) loss= 0.36050760746\n",
      "range:(8960, 9088) loss= 0.355677783489\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35190808773\n",
      "range:(4480, 4608) loss= 0.363964855671\n",
      "range:(8960, 9088) loss= 0.357863903046\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349302411079\n",
      "range:(4480, 4608) loss= 0.352947354317\n",
      "range:(8960, 9088) loss= 0.349179506302\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360052108765\n",
      "range:(4480, 4608) loss= 0.352579414845\n",
      "range:(8960, 9088) loss= 0.352315545082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 231\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343509852886\n",
      "range:(4480, 4608) loss= 0.35245603323\n",
      "range:(8960, 9088) loss= 0.35308688879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359681487083\n",
      "range:(4480, 4608) loss= 0.359219551086\n",
      "range:(8960, 9088) loss= 0.356655299664\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349043607712\n",
      "range:(4480, 4608) loss= 0.363303542137\n",
      "range:(8960, 9088) loss= 0.3590015769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352986842394\n",
      "range:(4480, 4608) loss= 0.3565325737\n",
      "range:(8960, 9088) loss= 0.351654171944\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360171675682\n",
      "range:(4480, 4608) loss= 0.376570403576\n",
      "range:(8960, 9088) loss= 0.358657777309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 232\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.355810910463\n",
      "range:(4480, 4608) loss= 0.358585178852\n",
      "range:(8960, 9088) loss= 0.35957556963\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.363592803478\n",
      "range:(4480, 4608) loss= 0.363314300776\n",
      "range:(8960, 9088) loss= 0.359867215157\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354213476181\n",
      "range:(4480, 4608) loss= 0.368841826916\n",
      "range:(8960, 9088) loss= 0.359967559576\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.355843484402\n",
      "range:(4480, 4608) loss= 0.358022421598\n",
      "range:(8960, 9088) loss= 0.351959943771\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.363258332014\n",
      "range:(4480, 4608) loss= 0.351958394051\n",
      "range:(8960, 9088) loss= 0.353131383657\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 233\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344615697861\n",
      "range:(4480, 4608) loss= 0.35400980711\n",
      "range:(8960, 9088) loss= 0.357561022043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.361421823502\n",
      "range:(4480, 4608) loss= 0.362178742886\n",
      "range:(8960, 9088) loss= 0.358515679836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.352243423462\n",
      "range:(4480, 4608) loss= 0.36848077178\n",
      "range:(8960, 9088) loss= 0.359270602465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354959279299\n",
      "range:(4480, 4608) loss= 0.356409698725\n",
      "range:(8960, 9088) loss= 0.35142827034\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362179994583\n",
      "range:(4480, 4608) loss= 0.351085007191\n",
      "range:(8960, 9088) loss= 0.352910041809\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 234\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343881964684\n",
      "range:(4480, 4608) loss= 0.353644669056\n",
      "range:(8960, 9088) loss= 0.356549918652\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359699964523\n",
      "range:(4480, 4608) loss= 0.36098831892\n",
      "range:(8960, 9088) loss= 0.358432412148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.352860987186\n",
      "range:(4480, 4608) loss= 0.369204074144\n",
      "range:(8960, 9088) loss= 0.360555529594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357570290565\n",
      "range:(4480, 4608) loss= 0.356543600559\n",
      "range:(8960, 9088) loss= 0.351783663034\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361895084381\n",
      "range:(4480, 4608) loss= 0.350669384003\n",
      "range:(8960, 9088) loss= 0.352341175079\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 235\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343804389238\n",
      "range:(4480, 4608) loss= 0.35454761982\n",
      "range:(8960, 9088) loss= 0.356256395578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359187841415\n",
      "range:(4480, 4608) loss= 0.36023619771\n",
      "range:(8960, 9088) loss= 0.357652336359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351867467165\n",
      "range:(4480, 4608) loss= 0.367759048939\n",
      "range:(8960, 9088) loss= 0.358291745186\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.356364488602\n",
      "range:(4480, 4608) loss= 0.355231046677\n",
      "range:(8960, 9088) loss= 0.351182162762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36227697134\n",
      "range:(4480, 4608) loss= 0.350192427635\n",
      "range:(8960, 9088) loss= 0.351719021797\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 236\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344982802868\n",
      "range:(4480, 4608) loss= 0.354031443596\n",
      "range:(8960, 9088) loss= 0.355573952198\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358568817377\n",
      "range:(4480, 4608) loss= 0.360382497311\n",
      "range:(8960, 9088) loss= 0.357015192509\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351162910461\n",
      "range:(4480, 4608) loss= 0.368224143982\n",
      "range:(8960, 9088) loss= 0.358871936798\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357851624489\n",
      "range:(4480, 4608) loss= 0.355501532555\n",
      "range:(8960, 9088) loss= 0.351229399443\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360656678677\n",
      "range:(4480, 4608) loss= 0.350235998631\n",
      "range:(8960, 9088) loss= 0.351726621389\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 237\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344216495752\n",
      "range:(4480, 4608) loss= 0.353781163692\n",
      "range:(8960, 9088) loss= 0.355637490749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360037326813\n",
      "range:(4480, 4608) loss= 0.360290467739\n",
      "range:(8960, 9088) loss= 0.356792390347\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351505935192\n",
      "range:(4480, 4608) loss= 0.367592394352\n",
      "range:(8960, 9088) loss= 0.359069049358\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.355372488499\n",
      "range:(4480, 4608) loss= 0.35640078783\n",
      "range:(8960, 9088) loss= 0.350803136826\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361010193825\n",
      "range:(4480, 4608) loss= 0.349385410547\n",
      "range:(8960, 9088) loss= 0.351143360138\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 238\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344100356102\n",
      "range:(4480, 4608) loss= 0.353567242622\n",
      "range:(8960, 9088) loss= 0.354350805283\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358846843243\n",
      "range:(4480, 4608) loss= 0.36134570837\n",
      "range:(8960, 9088) loss= 0.356649398804\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35214817524\n",
      "range:(4480, 4608) loss= 0.367001593113\n",
      "range:(8960, 9088) loss= 0.357809662819\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.356870889664\n",
      "range:(4480, 4608) loss= 0.354370713234\n",
      "range:(8960, 9088) loss= 0.350435763597\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360536038876\n",
      "range:(4480, 4608) loss= 0.349629878998\n",
      "range:(8960, 9088) loss= 0.351194083691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 239\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344683140516\n",
      "range:(4480, 4608) loss= 0.353279054165\n",
      "range:(8960, 9088) loss= 0.355145454407\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359696924686\n",
      "range:(4480, 4608) loss= 0.359728693962\n",
      "range:(8960, 9088) loss= 0.357605040073\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.354301929474\n",
      "range:(4480, 4608) loss= 0.366636633873\n",
      "range:(8960, 9088) loss= 0.358746826649\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.3539737463\n",
      "range:(4480, 4608) loss= 0.354043751955\n",
      "range:(8960, 9088) loss= 0.35022392869\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36294990778\n",
      "range:(4480, 4608) loss= 0.349719554186\n",
      "range:(8960, 9088) loss= 0.350375533104\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 240\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343636661768\n",
      "range:(4480, 4608) loss= 0.353749275208\n",
      "range:(8960, 9088) loss= 0.355121642351\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.36172658205\n",
      "range:(4480, 4608) loss= 0.359081298113\n",
      "range:(8960, 9088) loss= 0.356005042791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349730074406\n",
      "range:(4480, 4608) loss= 0.366106003523\n",
      "range:(8960, 9088) loss= 0.358157157898\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354348927736\n",
      "range:(4480, 4608) loss= 0.355535626411\n",
      "range:(8960, 9088) loss= 0.349390923977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360370099545\n",
      "range:(4480, 4608) loss= 0.350209414959\n",
      "range:(8960, 9088) loss= 0.35068488121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 241\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344442337751\n",
      "range:(4480, 4608) loss= 0.353127419949\n",
      "range:(8960, 9088) loss= 0.354667782784\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359495460987\n",
      "range:(4480, 4608) loss= 0.359375655651\n",
      "range:(8960, 9088) loss= 0.355294287205\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349552333355\n",
      "range:(4480, 4608) loss= 0.364841222763\n",
      "range:(8960, 9088) loss= 0.357566177845\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.353972196579\n",
      "range:(4480, 4608) loss= 0.355041503906\n",
      "range:(8960, 9088) loss= 0.348303854465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362592577934\n",
      "range:(4480, 4608) loss= 0.351026296616\n",
      "range:(8960, 9088) loss= 0.351649999619\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 242\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342703044415\n",
      "range:(4480, 4608) loss= 0.353116810322\n",
      "range:(8960, 9088) loss= 0.354016065598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358626484871\n",
      "range:(4480, 4608) loss= 0.359767735004\n",
      "range:(8960, 9088) loss= 0.356605768204\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34982162714\n",
      "range:(4480, 4608) loss= 0.36689043045\n",
      "range:(8960, 9088) loss= 0.358105957508\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35424041748\n",
      "range:(4480, 4608) loss= 0.354755997658\n",
      "range:(8960, 9088) loss= 0.350425869226\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360150456429\n",
      "range:(4480, 4608) loss= 0.35000950098\n",
      "range:(8960, 9088) loss= 0.34995752573\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 243\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343902587891\n",
      "range:(4480, 4608) loss= 0.35229575634\n",
      "range:(8960, 9088) loss= 0.353958815336\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.362020045519\n",
      "range:(4480, 4608) loss= 0.359211564064\n",
      "range:(8960, 9088) loss= 0.355395108461\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348802983761\n",
      "range:(4480, 4608) loss= 0.366352975368\n",
      "range:(8960, 9088) loss= 0.359428584576\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354085087776\n",
      "range:(4480, 4608) loss= 0.353014707565\n",
      "range:(8960, 9088) loss= 0.348232358694\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359873354435\n",
      "range:(4480, 4608) loss= 0.349163144827\n",
      "range:(8960, 9088) loss= 0.351560592651\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 244\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344026744366\n",
      "range:(4480, 4608) loss= 0.353733837605\n",
      "range:(8960, 9088) loss= 0.353591263294\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358310222626\n",
      "range:(4480, 4608) loss= 0.359486699104\n",
      "range:(8960, 9088) loss= 0.357366621494\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.352849811316\n",
      "range:(4480, 4608) loss= 0.367481708527\n",
      "range:(8960, 9088) loss= 0.357408612967\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350774377584\n",
      "range:(4480, 4608) loss= 0.353926718235\n",
      "range:(8960, 9088) loss= 0.347777217627\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358188062906\n",
      "range:(4480, 4608) loss= 0.349663823843\n",
      "range:(8960, 9088) loss= 0.350691974163\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 245\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344169676304\n",
      "range:(4480, 4608) loss= 0.353660583496\n",
      "range:(8960, 9088) loss= 0.353704750538\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358304142952\n",
      "range:(4480, 4608) loss= 0.358987033367\n",
      "range:(8960, 9088) loss= 0.355353623629\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349679470062\n",
      "range:(4480, 4608) loss= 0.36500954628\n",
      "range:(8960, 9088) loss= 0.356941461563\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351286649704\n",
      "range:(4480, 4608) loss= 0.353258192539\n",
      "range:(8960, 9088) loss= 0.347412616014\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358917295933\n",
      "range:(4480, 4608) loss= 0.348818004131\n",
      "range:(8960, 9088) loss= 0.350166976452\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 246\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342668682337\n",
      "range:(4480, 4608) loss= 0.35265058279\n",
      "range:(8960, 9088) loss= 0.353963792324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359661906958\n",
      "range:(4480, 4608) loss= 0.357998430729\n",
      "range:(8960, 9088) loss= 0.354594707489\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348221868277\n",
      "range:(4480, 4608) loss= 0.368291914463\n",
      "range:(8960, 9088) loss= 0.357495367527\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.3540584445\n",
      "range:(4480, 4608) loss= 0.35474589467\n",
      "range:(8960, 9088) loss= 0.348564773798\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358625113964\n",
      "range:(4480, 4608) loss= 0.35046851635\n",
      "range:(8960, 9088) loss= 0.35020622611\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 247\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343265354633\n",
      "range:(4480, 4608) loss= 0.351953566074\n",
      "range:(8960, 9088) loss= 0.353684186935\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.357768088579\n",
      "range:(4480, 4608) loss= 0.35750412941\n",
      "range:(8960, 9088) loss= 0.354917824268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348557382822\n",
      "range:(4480, 4608) loss= 0.364697933197\n",
      "range:(8960, 9088) loss= 0.359893262386\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354198038578\n",
      "range:(4480, 4608) loss= 0.351988852024\n",
      "range:(8960, 9088) loss= 0.347875833511\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362098455429\n",
      "range:(4480, 4608) loss= 0.349980860949\n",
      "range:(8960, 9088) loss= 0.351804316044\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 248\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343631416559\n",
      "range:(4480, 4608) loss= 0.353254377842\n",
      "range:(8960, 9088) loss= 0.354743182659\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356742024422\n",
      "range:(4480, 4608) loss= 0.356872320175\n",
      "range:(8960, 9088) loss= 0.355499863625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351046562195\n",
      "range:(4480, 4608) loss= 0.364018172026\n",
      "range:(8960, 9088) loss= 0.355876982212\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354049324989\n",
      "range:(4480, 4608) loss= 0.3531280756\n",
      "range:(8960, 9088) loss= 0.347692430019\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36003267765\n",
      "range:(4480, 4608) loss= 0.351396143436\n",
      "range:(8960, 9088) loss= 0.350875347853\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 249\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343395531178\n",
      "range:(4480, 4608) loss= 0.351803958416\n",
      "range:(8960, 9088) loss= 0.352683961391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356376558542\n",
      "range:(4480, 4608) loss= 0.360139399767\n",
      "range:(8960, 9088) loss= 0.355555295944\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350807487965\n",
      "range:(4480, 4608) loss= 0.365484088659\n",
      "range:(8960, 9088) loss= 0.356937289238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35368013382\n",
      "range:(4480, 4608) loss= 0.35338896513\n",
      "range:(8960, 9088) loss= 0.346806228161\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358571499586\n",
      "range:(4480, 4608) loss= 0.349020332098\n",
      "range:(8960, 9088) loss= 0.35251390934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 250\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.345434129238\n",
      "range:(4480, 4608) loss= 0.353456199169\n",
      "range:(8960, 9088) loss= 0.35283601284\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.367030858994\n",
      "range:(4480, 4608) loss= 0.360848546028\n",
      "range:(8960, 9088) loss= 0.356044411659\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351336151361\n",
      "range:(4480, 4608) loss= 0.364147156477\n",
      "range:(8960, 9088) loss= 0.35608112812\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351629078388\n",
      "range:(4480, 4608) loss= 0.354212760925\n",
      "range:(8960, 9088) loss= 0.346577763557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359496772289\n",
      "range:(4480, 4608) loss= 0.349338769913\n",
      "range:(8960, 9088) loss= 0.35016477108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 251\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347472459078\n",
      "range:(4480, 4608) loss= 0.352536022663\n",
      "range:(8960, 9088) loss= 0.354494929314\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358296453953\n",
      "range:(4480, 4608) loss= 0.357999891043\n",
      "range:(8960, 9088) loss= 0.356441438198\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349087744951\n",
      "range:(4480, 4608) loss= 0.367027848959\n",
      "range:(8960, 9088) loss= 0.356506228447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35844117403\n",
      "range:(4480, 4608) loss= 0.354487329721\n",
      "range:(8960, 9088) loss= 0.347636729479\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358917653561\n",
      "range:(4480, 4608) loss= 0.348384529352\n",
      "range:(8960, 9088) loss= 0.351546257734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 252\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342094838619\n",
      "range:(4480, 4608) loss= 0.351704806089\n",
      "range:(8960, 9088) loss= 0.351381897926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.35845553875\n",
      "range:(4480, 4608) loss= 0.357722699642\n",
      "range:(8960, 9088) loss= 0.354805886745\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34809666872\n",
      "range:(4480, 4608) loss= 0.36342138052\n",
      "range:(8960, 9088) loss= 0.357223212719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352236747742\n",
      "range:(4480, 4608) loss= 0.352282464504\n",
      "range:(8960, 9088) loss= 0.348883509636\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359062373638\n",
      "range:(4480, 4608) loss= 0.348752498627\n",
      "range:(8960, 9088) loss= 0.350481569767\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 253\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343199193478\n",
      "range:(4480, 4608) loss= 0.352097451687\n",
      "range:(8960, 9088) loss= 0.351855516434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356948763132\n",
      "range:(4480, 4608) loss= 0.357374042273\n",
      "range:(8960, 9088) loss= 0.354653477669\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349200814962\n",
      "range:(4480, 4608) loss= 0.364707559347\n",
      "range:(8960, 9088) loss= 0.356219828129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350918650627\n",
      "range:(4480, 4608) loss= 0.352962613106\n",
      "range:(8960, 9088) loss= 0.350360423326\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356625497341\n",
      "range:(4480, 4608) loss= 0.348470211029\n",
      "range:(8960, 9088) loss= 0.350005418062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 254\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343403160572\n",
      "range:(4480, 4608) loss= 0.355126023293\n",
      "range:(8960, 9088) loss= 0.351993322372\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356013894081\n",
      "range:(4480, 4608) loss= 0.359769463539\n",
      "range:(8960, 9088) loss= 0.355386525393\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348896145821\n",
      "range:(4480, 4608) loss= 0.364240884781\n",
      "range:(8960, 9088) loss= 0.356740355492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352544665337\n",
      "range:(4480, 4608) loss= 0.353036880493\n",
      "range:(8960, 9088) loss= 0.346411108971\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361583888531\n",
      "range:(4480, 4608) loss= 0.349421679974\n",
      "range:(8960, 9088) loss= 0.350559294224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 255\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342946112156\n",
      "range:(4480, 4608) loss= 0.352573752403\n",
      "range:(8960, 9088) loss= 0.351776540279\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358487665653\n",
      "range:(4480, 4608) loss= 0.357890367508\n",
      "range:(8960, 9088) loss= 0.355283498764\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348290264606\n",
      "range:(4480, 4608) loss= 0.364870578051\n",
      "range:(8960, 9088) loss= 0.356396615505\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349967777729\n",
      "range:(4480, 4608) loss= 0.351663410664\n",
      "range:(8960, 9088) loss= 0.347500443459\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357896864414\n",
      "range:(4480, 4608) loss= 0.349545359612\n",
      "range:(8960, 9088) loss= 0.351014494896\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 256\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342000603676\n",
      "range:(4480, 4608) loss= 0.351660370827\n",
      "range:(8960, 9088) loss= 0.352992117405\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356503129005\n",
      "range:(4480, 4608) loss= 0.358199059963\n",
      "range:(8960, 9088) loss= 0.353965580463\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34794074297\n",
      "range:(4480, 4608) loss= 0.364579111338\n",
      "range:(8960, 9088) loss= 0.355486631393\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349570780993\n",
      "range:(4480, 4608) loss= 0.35111489892\n",
      "range:(8960, 9088) loss= 0.348916172981\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357061058283\n",
      "range:(4480, 4608) loss= 0.351045131683\n",
      "range:(8960, 9088) loss= 0.349410623312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 257\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342339247465\n",
      "range:(4480, 4608) loss= 0.35316157341\n",
      "range:(8960, 9088) loss= 0.35256332159\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.357231706381\n",
      "range:(4480, 4608) loss= 0.358406126499\n",
      "range:(8960, 9088) loss= 0.356176495552\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.3522310853\n",
      "range:(4480, 4608) loss= 0.365276247263\n",
      "range:(8960, 9088) loss= 0.355965137482\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35289528966\n",
      "range:(4480, 4608) loss= 0.353554874659\n",
      "range:(8960, 9088) loss= 0.348929584026\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359190195799\n",
      "range:(4480, 4608) loss= 0.349618554115\n",
      "range:(8960, 9088) loss= 0.34885212779\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 258\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342164933681\n",
      "range:(4480, 4608) loss= 0.353913724422\n",
      "range:(8960, 9088) loss= 0.35392343998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.35872527957\n",
      "range:(4480, 4608) loss= 0.359935283661\n",
      "range:(8960, 9088) loss= 0.355824679136\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.352717101574\n",
      "range:(4480, 4608) loss= 0.36394777894\n",
      "range:(8960, 9088) loss= 0.357299506664\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349152773619\n",
      "range:(4480, 4608) loss= 0.351834416389\n",
      "range:(8960, 9088) loss= 0.346103161573\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358192563057\n",
      "range:(4480, 4608) loss= 0.348751425743\n",
      "range:(8960, 9088) loss= 0.349532872438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 259\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34641829133\n",
      "range:(4480, 4608) loss= 0.351179689169\n",
      "range:(8960, 9088) loss= 0.350612044334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356323421001\n",
      "range:(4480, 4608) loss= 0.356506556273\n",
      "range:(8960, 9088) loss= 0.354508697987\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347937256098\n",
      "range:(4480, 4608) loss= 0.365965306759\n",
      "range:(8960, 9088) loss= 0.357032239437\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.355825483799\n",
      "range:(4480, 4608) loss= 0.351480066776\n",
      "range:(8960, 9088) loss= 0.346070170403\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356889218092\n",
      "range:(4480, 4608) loss= 0.349605381489\n",
      "range:(8960, 9088) loss= 0.351131379604\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 260\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342098653316\n",
      "range:(4480, 4608) loss= 0.352227985859\n",
      "range:(8960, 9088) loss= 0.353843212128\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356685012579\n",
      "range:(4480, 4608) loss= 0.358912825584\n",
      "range:(8960, 9088) loss= 0.356201738119\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347442924976\n",
      "range:(4480, 4608) loss= 0.3643425107\n",
      "range:(8960, 9088) loss= 0.356596708298\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351126551628\n",
      "range:(4480, 4608) loss= 0.35338357091\n",
      "range:(8960, 9088) loss= 0.346402496099\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357657670975\n",
      "range:(4480, 4608) loss= 0.349055498838\n",
      "range:(8960, 9088) loss= 0.349748849869\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 261\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342739224434\n",
      "range:(4480, 4608) loss= 0.350808560848\n",
      "range:(8960, 9088) loss= 0.352193593979\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.361666023731\n",
      "range:(4480, 4608) loss= 0.358693689108\n",
      "range:(8960, 9088) loss= 0.354813635349\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350078642368\n",
      "range:(4480, 4608) loss= 0.363455057144\n",
      "range:(8960, 9088) loss= 0.356062889099\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352706730366\n",
      "range:(4480, 4608) loss= 0.353222846985\n",
      "range:(8960, 9088) loss= 0.347975969315\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357618927956\n",
      "range:(4480, 4608) loss= 0.350297808647\n",
      "range:(8960, 9088) loss= 0.350168347359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 262\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343644946814\n",
      "range:(4480, 4608) loss= 0.352756977081\n",
      "range:(8960, 9088) loss= 0.350603103638\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.357789754868\n",
      "range:(4480, 4608) loss= 0.356215834618\n",
      "range:(8960, 9088) loss= 0.356946825981\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349074840546\n",
      "range:(4480, 4608) loss= 0.36528801918\n",
      "range:(8960, 9088) loss= 0.356288582087\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351596534252\n",
      "range:(4480, 4608) loss= 0.35266071558\n",
      "range:(8960, 9088) loss= 0.348557710648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359359890223\n",
      "range:(4480, 4608) loss= 0.348691374063\n",
      "range:(8960, 9088) loss= 0.352001845837\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 263\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343073785305\n",
      "range:(4480, 4608) loss= 0.351264506578\n",
      "range:(8960, 9088) loss= 0.353057682514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356689363718\n",
      "range:(4480, 4608) loss= 0.358448296785\n",
      "range:(8960, 9088) loss= 0.354214251041\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34961310029\n",
      "range:(4480, 4608) loss= 0.363172769547\n",
      "range:(8960, 9088) loss= 0.356594145298\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.353296130896\n",
      "range:(4480, 4608) loss= 0.352433025837\n",
      "range:(8960, 9088) loss= 0.345889031887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35688328743\n",
      "range:(4480, 4608) loss= 0.349091053009\n",
      "range:(8960, 9088) loss= 0.351318985224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 264\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34500181675\n",
      "range:(4480, 4608) loss= 0.351173102856\n",
      "range:(8960, 9088) loss= 0.351439714432\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356288254261\n",
      "range:(4480, 4608) loss= 0.356635391712\n",
      "range:(8960, 9088) loss= 0.354223787785\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348104804754\n",
      "range:(4480, 4608) loss= 0.364921987057\n",
      "range:(8960, 9088) loss= 0.356897413731\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350637733936\n",
      "range:(4480, 4608) loss= 0.351859509945\n",
      "range:(8960, 9088) loss= 0.348001241684\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359700292349\n",
      "range:(4480, 4608) loss= 0.350444555283\n",
      "range:(8960, 9088) loss= 0.348817706108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 265\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342638850212\n",
      "range:(4480, 4608) loss= 0.351018846035\n",
      "range:(8960, 9088) loss= 0.352904647589\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.35914003849\n",
      "range:(4480, 4608) loss= 0.35727584362\n",
      "range:(8960, 9088) loss= 0.353780388832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348457932472\n",
      "range:(4480, 4608) loss= 0.362594038248\n",
      "range:(8960, 9088) loss= 0.35676458478\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352872490883\n",
      "range:(4480, 4608) loss= 0.352843552828\n",
      "range:(8960, 9088) loss= 0.346290230751\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356555521488\n",
      "range:(4480, 4608) loss= 0.346840709448\n",
      "range:(8960, 9088) loss= 0.348358094692\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 266\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343984901905\n",
      "range:(4480, 4608) loss= 0.351064532995\n",
      "range:(8960, 9088) loss= 0.355149567127\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.357978641987\n",
      "range:(4480, 4608) loss= 0.357227057219\n",
      "range:(8960, 9088) loss= 0.354978203773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349265217781\n",
      "range:(4480, 4608) loss= 0.364130496979\n",
      "range:(8960, 9088) loss= 0.356534719467\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349370896816\n",
      "range:(4480, 4608) loss= 0.351840555668\n",
      "range:(8960, 9088) loss= 0.348460137844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35870295763\n",
      "range:(4480, 4608) loss= 0.347564935684\n",
      "range:(8960, 9088) loss= 0.349964261055\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 267\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342173993587\n",
      "range:(4480, 4608) loss= 0.350509762764\n",
      "range:(8960, 9088) loss= 0.352638810873\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356978595257\n",
      "range:(4480, 4608) loss= 0.358125567436\n",
      "range:(8960, 9088) loss= 0.354641377926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347980231047\n",
      "range:(4480, 4608) loss= 0.362764060497\n",
      "range:(8960, 9088) loss= 0.356143593788\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352397054434\n",
      "range:(4480, 4608) loss= 0.35114389658\n",
      "range:(8960, 9088) loss= 0.348187297583\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358577668667\n",
      "range:(4480, 4608) loss= 0.346950411797\n",
      "range:(8960, 9088) loss= 0.350062668324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 268\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344524204731\n",
      "range:(4480, 4608) loss= 0.351492762566\n",
      "range:(8960, 9088) loss= 0.351884365082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.357800424099\n",
      "range:(4480, 4608) loss= 0.358833134174\n",
      "range:(8960, 9088) loss= 0.355388581753\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348353773355\n",
      "range:(4480, 4608) loss= 0.364126861095\n",
      "range:(8960, 9088) loss= 0.356739014387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350658386946\n",
      "range:(4480, 4608) loss= 0.351056873798\n",
      "range:(8960, 9088) loss= 0.346180319786\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357303500175\n",
      "range:(4480, 4608) loss= 0.348968625069\n",
      "range:(8960, 9088) loss= 0.350780636072\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 269\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342467904091\n",
      "range:(4480, 4608) loss= 0.353843092918\n",
      "range:(8960, 9088) loss= 0.354234814644\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358700186014\n",
      "range:(4480, 4608) loss= 0.35741776228\n",
      "range:(8960, 9088) loss= 0.354152202606\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.361512392759\n",
      "range:(4480, 4608) loss= 0.370619237423\n",
      "range:(8960, 9088) loss= 0.359307944775\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35425555706\n",
      "range:(4480, 4608) loss= 0.351838290691\n",
      "range:(8960, 9088) loss= 0.346260309219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358354210854\n",
      "range:(4480, 4608) loss= 0.347106635571\n",
      "range:(8960, 9088) loss= 0.350430846214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 270\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342238664627\n",
      "range:(4480, 4608) loss= 0.349671989679\n",
      "range:(8960, 9088) loss= 0.352179169655\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356624752283\n",
      "range:(4480, 4608) loss= 0.35676163435\n",
      "range:(8960, 9088) loss= 0.356145620346\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349374353886\n",
      "range:(4480, 4608) loss= 0.363234043121\n",
      "range:(8960, 9088) loss= 0.355013728142\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349767923355\n",
      "range:(4480, 4608) loss= 0.355426371098\n",
      "range:(8960, 9088) loss= 0.345973014832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361982226372\n",
      "range:(4480, 4608) loss= 0.348068863153\n",
      "range:(8960, 9088) loss= 0.352962821722\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 271\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342856228352\n",
      "range:(4480, 4608) loss= 0.351500838995\n",
      "range:(8960, 9088) loss= 0.352277636528\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356636404991\n",
      "range:(4480, 4608) loss= 0.356914520264\n",
      "range:(8960, 9088) loss= 0.354992568493\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349207222462\n",
      "range:(4480, 4608) loss= 0.365196883678\n",
      "range:(8960, 9088) loss= 0.355793088675\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351312637329\n",
      "range:(4480, 4608) loss= 0.357957929373\n",
      "range:(8960, 9088) loss= 0.345551818609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358978360891\n",
      "range:(4480, 4608) loss= 0.347653448582\n",
      "range:(8960, 9088) loss= 0.349683433771\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 272\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343550622463\n",
      "range:(4480, 4608) loss= 0.349706023932\n",
      "range:(8960, 9088) loss= 0.352865040302\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356185317039\n",
      "range:(4480, 4608) loss= 0.361603975296\n",
      "range:(8960, 9088) loss= 0.355016827583\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348879814148\n",
      "range:(4480, 4608) loss= 0.363750100136\n",
      "range:(8960, 9088) loss= 0.355995953083\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349439740181\n",
      "range:(4480, 4608) loss= 0.35476064682\n",
      "range:(8960, 9088) loss= 0.344613730907\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359387338161\n",
      "range:(4480, 4608) loss= 0.348568737507\n",
      "range:(8960, 9088) loss= 0.35124155879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 273\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347351461649\n",
      "range:(4480, 4608) loss= 0.352383196354\n",
      "range:(8960, 9088) loss= 0.352333635092\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360128641129\n",
      "range:(4480, 4608) loss= 0.35794711113\n",
      "range:(8960, 9088) loss= 0.357472807169\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350897014141\n",
      "range:(4480, 4608) loss= 0.36292809248\n",
      "range:(8960, 9088) loss= 0.355464100838\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.348896771669\n",
      "range:(4480, 4608) loss= 0.352130651474\n",
      "range:(8960, 9088) loss= 0.346756517887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358103692532\n",
      "range:(4480, 4608) loss= 0.350264310837\n",
      "range:(8960, 9088) loss= 0.349260687828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 274\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34197217226\n",
      "range:(4480, 4608) loss= 0.350071251392\n",
      "range:(8960, 9088) loss= 0.353896737099\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359186947346\n",
      "range:(4480, 4608) loss= 0.3562707901\n",
      "range:(8960, 9088) loss= 0.355311244726\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350224912167\n",
      "range:(4480, 4608) loss= 0.363157480955\n",
      "range:(8960, 9088) loss= 0.359261006117\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.350612431765\n",
      "range:(4480, 4608) loss= 0.35155236721\n",
      "range:(8960, 9088) loss= 0.345498800278\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358608603477\n",
      "range:(4480, 4608) loss= 0.348307579756\n",
      "range:(8960, 9088) loss= 0.348713755608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 275\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344398081303\n",
      "range:(4480, 4608) loss= 0.35053601861\n",
      "range:(8960, 9088) loss= 0.352324754\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.359179377556\n",
      "range:(4480, 4608) loss= 0.358642488718\n",
      "range:(8960, 9088) loss= 0.353702306747\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348578929901\n",
      "range:(4480, 4608) loss= 0.367012619972\n",
      "range:(8960, 9088) loss= 0.35681822896\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.3501085639\n",
      "range:(4480, 4608) loss= 0.351426780224\n",
      "range:(8960, 9088) loss= 0.345852077007\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358325183392\n",
      "range:(4480, 4608) loss= 0.34806188941\n",
      "range:(8960, 9088) loss= 0.350149482489\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 276\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344145059586\n",
      "range:(4480, 4608) loss= 0.350765347481\n",
      "range:(8960, 9088) loss= 0.349884927273\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.355951488018\n",
      "range:(4480, 4608) loss= 0.35792568326\n",
      "range:(8960, 9088) loss= 0.354764521122\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34840041399\n",
      "range:(4480, 4608) loss= 0.364155977964\n",
      "range:(8960, 9088) loss= 0.356630116701\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.354562431574\n",
      "range:(4480, 4608) loss= 0.357233405113\n",
      "range:(8960, 9088) loss= 0.34678208828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.354803979397\n",
      "range:(4480, 4608) loss= 0.348990261555\n",
      "range:(8960, 9088) loss= 0.349747300148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 277\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.341888785362\n",
      "range:(4480, 4608) loss= 0.349441856146\n",
      "range:(8960, 9088) loss= 0.352521300316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.355570018291\n",
      "range:(4480, 4608) loss= 0.358292579651\n",
      "range:(8960, 9088) loss= 0.354861974716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346523344517\n",
      "range:(4480, 4608) loss= 0.362539589405\n",
      "range:(8960, 9088) loss= 0.354935795069\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.349829614162\n",
      "range:(4480, 4608) loss= 0.351664364338\n",
      "range:(8960, 9088) loss= 0.346316635609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35975959897\n",
      "range:(4480, 4608) loss= 0.35028475523\n",
      "range:(8960, 9088) loss= 0.357034683228\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 278\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344643890858\n",
      "range:(4480, 4608) loss= 0.351554989815\n",
      "range:(8960, 9088) loss= 0.352638274431\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356976509094\n",
      "range:(4480, 4608) loss= 0.357915818691\n",
      "range:(8960, 9088) loss= 0.35715919733\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35178732872\n",
      "range:(4480, 4608) loss= 0.365006625652\n",
      "range:(8960, 9088) loss= 0.356066137552\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.35000538826\n",
      "range:(4480, 4608) loss= 0.35353833437\n",
      "range:(8960, 9088) loss= 0.345775932074\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356178760529\n",
      "range:(4480, 4608) loss= 0.347459077835\n",
      "range:(8960, 9088) loss= 0.351357638836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 279\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343977272511\n",
      "range:(4480, 4608) loss= 0.351724922657\n",
      "range:(8960, 9088) loss= 0.350157380104\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.360380083323\n",
      "range:(4480, 4608) loss= 0.35888415575\n",
      "range:(8960, 9088) loss= 0.353723108768\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347202658653\n",
      "range:(4480, 4608) loss= 0.362900614738\n",
      "range:(8960, 9088) loss= 0.356168866158\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351594090462\n",
      "range:(4480, 4608) loss= 0.350955665112\n",
      "range:(8960, 9088) loss= 0.346349537373\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357381284237\n",
      "range:(4480, 4608) loss= 0.348895788193\n",
      "range:(8960, 9088) loss= 0.35082668066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 280\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.342004239559\n",
      "range:(4480, 4608) loss= 0.350276857615\n",
      "range:(8960, 9088) loss= 0.352595865726\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356792271137\n",
      "range:(4480, 4608) loss= 0.357724547386\n",
      "range:(8960, 9088) loss= 0.354883611202\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349547922611\n",
      "range:(4480, 4608) loss= 0.36164778471\n",
      "range:(8960, 9088) loss= 0.357527881861\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352598726749\n",
      "range:(4480, 4608) loss= 0.351988613605\n",
      "range:(8960, 9088) loss= 0.345833808184\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.355718374252\n",
      "range:(4480, 4608) loss= 0.348718374968\n",
      "range:(8960, 9088) loss= 0.349505931139\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 281\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344257354736\n",
      "range:(4480, 4608) loss= 0.350320577621\n",
      "range:(8960, 9088) loss= 0.350789189339\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.355833828449\n",
      "range:(4480, 4608) loss= 0.357822328806\n",
      "range:(8960, 9088) loss= 0.35520619154\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351117640734\n",
      "range:(4480, 4608) loss= 0.362414449453\n",
      "range:(8960, 9088) loss= 0.356087744236\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.348919332027\n",
      "range:(4480, 4608) loss= 0.351926505566\n",
      "range:(8960, 9088) loss= 0.345149546862\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.355995118618\n",
      "range:(4480, 4608) loss= 0.348767220974\n",
      "range:(8960, 9088) loss= 0.351948082447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 282\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343048155308\n",
      "range:(4480, 4608) loss= 0.351315379143\n",
      "range:(8960, 9088) loss= 0.350850254297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356431812048\n",
      "range:(4480, 4608) loss= 0.358065485954\n",
      "range:(8960, 9088) loss= 0.354139119387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346391260624\n",
      "range:(4480, 4608) loss= 0.364677131176\n",
      "range:(8960, 9088) loss= 0.358293414116\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352756679058\n",
      "range:(4480, 4608) loss= 0.352597057819\n",
      "range:(8960, 9088) loss= 0.346387147903\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358218759298\n",
      "range:(4480, 4608) loss= 0.350833505392\n",
      "range:(8960, 9088) loss= 0.352296292782\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 283\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.344893872738\n",
      "range:(4480, 4608) loss= 0.352721869946\n",
      "range:(8960, 9088) loss= 0.351412892342\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.358957320452\n",
      "range:(4480, 4608) loss= 0.3573602736\n",
      "range:(8960, 9088) loss= 0.353858113289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348413646221\n",
      "range:(4480, 4608) loss= 0.363870918751\n",
      "range:(8960, 9088) loss= 0.357025504112\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.34852617979\n",
      "range:(4480, 4608) loss= 0.350999563932\n",
      "range:(8960, 9088) loss= 0.345321625471\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357841968536\n",
      "range:(4480, 4608) loss= 0.35090097785\n",
      "range:(8960, 9088) loss= 0.350868344307\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 284\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.343881517649\n",
      "range:(4480, 4608) loss= 0.351526945829\n",
      "range:(8960, 9088) loss= 0.35076791048\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3567879498\n",
      "range:(4480, 4608) loss= 0.356760978699\n",
      "range:(8960, 9088) loss= 0.353553771973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.345987677574\n",
      "range:(4480, 4608) loss= 0.366724491119\n",
      "range:(8960, 9088) loss= 0.357739448547\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.351278424263\n",
      "range:(4480, 4608) loss= 0.350645393133\n",
      "range:(8960, 9088) loss= 0.34689861536\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.355688273907\n",
      "range:(4480, 4608) loss= 0.348594129086\n",
      "range:(8960, 9088) loss= 0.350383102894\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 285\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.341773688793\n",
      "range:(4480, 4608) loss= 0.351278275251\n",
      "range:(8960, 9088) loss= 0.35547208786\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.356721669436\n",
      "range:(4480, 4608) loss= 0.357138365507\n",
      "range:(8960, 9088) loss= 0.354149878025\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348546147346\n",
      "range:(4480, 4608) loss= 0.36145311594\n",
      "range:(8960, 9088) loss= 0.356780588627\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.352329671383\n",
      "range:(4480, 4608) loss= 0.35223197937\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5fd3001f5887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mminX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mminY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m35\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 76321\n",
    "    for ep in range(194, 300):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(np.ceil(float(len(batch_images)) / min_batch_size))):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                if(index % 35 ==0):\n",
    "                    print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_3\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
