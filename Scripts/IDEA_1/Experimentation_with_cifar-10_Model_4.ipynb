{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 100 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 5 # all kernels are 5x5\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 128 # we look at 5000 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9aefc24d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEVCAYAAAAPaTtOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHCfDuAFwnAzjBsBxMsz/B94cFtFqzaAyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9f88dd1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXmQZXd13z/f916v063ZZzQzEhptLAJbAsYyLoOjsIod\nXC4MibHiIog4JjYVCBaiArKLCpAAgqQCWBgZYRYhdpkABhQZTDASI4E2JMFIGkkzmn3tmd7ecvLH\nvQ1vmnt+/Xp7PcM9n6qpeX3P+9177u/ec5ff953zk5kRBEH5qCy1A0EQLA0R/EFQUiL4g6CkRPAH\nQUmJ4A+CkhLBHwQl5ZQKfkmXSNqx1H6cTEh6l6T9knYvtS8Akq6S9KlFWve/k/T9xVj3ycBMfSfp\nHkmXLNT25h38krZLGpN0TNJuSZ+QNLQQzi0lkkzSeUvtRwpJjwPeDFxgZqcvwfZP2ovxYl6ElmI7\nAGb2ZDP7p4Va30Ld+V9qZkPARcBTgbct0HqDNI8DDpjZ3iKjpFqX/QlOIRb0sd/MdgP/SHYRAEDS\niyX9WNJRSY9KuqrNtjm/w14m6ZH88fXtbfaB/EnikKSfAr/Vvj1JT5L0T5IO549EL2uzfULShyV9\nI38q+X+STpf0wXx990l6aif7lV/dPy/pU5JGJN0l6fGS3iZpb75fz2/7/p9Iujf/7oOS3jBtfW+V\ntEvSY5L+fftThqQ+Se/L+2OPpI9KGijw6bnAt4GN+f59oq0/XyfpEeD/5t99Wd4/h/P+elLberZL\n+i+S7pR0XNLHJa3P+21E0nckrSzY/jLgG23bPyZpY27ulfTJvP09kra0tdso6YuS9kl6SNKfJ/p9\ntaQb83PnVuDcafYP5X1/VNJtkp6VL78UuBL4w9yvO2Y6LpLWSPpa3kcHJf2zpErKZ287MyHpLyXt\nzP24X9Jz2sypvtueH/epc/ILkj6Xf/d2SRd2sv1fYGbz+gdsB56bfz4DuAv4UJv9EuA3yC40vwns\nAV6R2zYDBnwMGAAuBCaAJ+X29wD/DKwCzgTuBnbkth5gG1nn9wLPBkaAJ+T2TwD7gacD/WSB8BDw\nx0AVeBdwc2K/DDgv/3wVMA68AKgBn8zX9fbcj9cDD7W1fTHZiSrgXwGjwNNy26XAbuDJwCDwqWnb\nuhq4Md/nYeAfgHc7Pl4y1R/T+vOTwLK8Tx8PHAeel/v61rzfetuO3w+B9cAmYC9wO9kT3FS/vbOT\n7U/rqxfl/fxu4Ie5rQLcBrwjP2bnAA8CL3DWfz1wQ74vTwF2At9vs/8RsDo/Jm/O+7W/zY9PTVtf\n6ri8G/ho3kc9wLPy7yV9drZzBfA1Z5+eADwKbGw7ZufO1HcFsXYVUAf+IPf3LWTnZE/HsbtAwX+M\nLPAMuAlYkfj+B4Grp52sZ7TZbwVenX9+ELi0zXY5vwz+Z+UHu9Jm/yxwVVvwf6zN9p+Ae9v+/g3g\n8CyC/9tttpfm+1zN/x7Ov1+438BXgL/IP19LWzAD501tKz/Zjk+dDLn9d2i7sHQY/Oe0LfuvwA1t\nf1fIguiStuP3b9vsXwQ+Mq3fvjLL4P9O298XAGP5598GHpn2/bcBf1ew7mp+cj+xbdl/oy34C9oc\nAi70gnKG4/LXwFenjnnbd5I+d7KdaW3PI7vAPpdpgZrqu7Zj1R787ReGCrALeFanvizUY/8rzGyY\n7GR4IrBmyiDptyXdnD8yHQH+Q7s9p32kehSYGjDcSHaVnOLhts8bgUfNrDXNvqnt7z1tn8cK/p7N\nwOT0tvvNrNn2N1Prk/RCST/MHx8Pk13Jp/Z5+j61f15L9jRwW/74eRj4Zr58NrSvcyNt/Zb316Ms\nXj/Brx7PfmXjD2eRvSYcbtu/K8meOqazluyO7h1/JL0lf4w/kq9rOb96brV/P3Vc/gfZE9G38leC\nK/Lls/F5RsxsG/AmsuDdK+n6ttcl8PuuiF/0TX5cd5Ad745Y6Hf+75Ldcd/XtvgzZI+xZ5rZcrJH\nK3W4yl1kj/tTPK7t82PAmVPvZW32nbN0e0GR1Ed293wfsN7MVgBf55f7vIvs9WiK9v3bTxZsTzaz\nFfm/5ZYNps6G9lTNx8hO4Cn/lG9zIfpptimhj5I9xaxo+zdsZi8q+O4+oIFz/PP3+7cCrwJW5v18\nhF/28wm+zXRczGzEzN5sZucALwP+c/4uPpPPs06LNbPPmNkzyY6LAe+d7TpyftE3eRycQXa8O2Ix\ndP4PAs9rG3wYBg6a2biki4F/M4t13QC8TdJKSWeQPYJOcQvZlfGtknqU6Z8vJXtPXEp6gT7yk1fS\nC4Hnt9lvAP5E2WDlINljOfCLq/fHgKslrQOQtEnSC+bhzw3AiyU9R1IP2bvxBPCDeaxzij3AaknL\nO/z+rcBIPuA1IKkq6SmSfmv6F/Onqi8BV0kalHQBcFnbV4bJLg77gJqkdwCnTfNtc9vNIXlcJL1E\n0nn5xfEI0ARaHfg8fTtJJD1B0rPzi9E42cW+NUMzj6dL+v38yeBNZMf1h502XvDgN7N9ZANO78gX\n/UfgryWN5MtumMXq/orsUe8h4FvA37dtZ5Is2F9Idsf8MPDHZnbffPdhPpjZCPDnZPt5iOxid2Ob\n/RvA/wRuJnvMnDpYE/n/fzm1XNJR4Dtkg0Rz9ed+soGx/0XWTy8lk2Yn57rOtnXfRzbO8mD+SJx8\n5MwD+iVkatBDuT9/S/a4XsQbyV45dpM9Uf5dm+0fyV6JfkZ2joxz4ivC5/P/D0i6fabjApxP1tfH\ngH8BPmxmN3fg8wnbAZB0paRvOPvURzaQvT/fr3XMXRr/KvCH+f68Fvh9M6t32lj5YEGwRCiT3e4G\n+syssdT+BKcGyiTz88zsj+a6jlPq572/Lkh6pTI9fyXZ+94/ROAH3SaCf2l4A5nc8wDZu+WfLq07\nQRmJx/4gKClx5w+CkhLBHwQlJYI/CEpKBH8QlJQI/iAoKRH8QVBSIviDoKRE8AdBSYngD4KSEsEf\nBCUlgj8ISkoEfxCUlAj+ICgpEfxBUFLmNaNLPmnBh8jKLP+tmb0n9f3lq1bZ6Zs2FRsTqcVutc9E\nG2v5tlbKZn45Nc+SyopO+ZFKp07ZWsl2s2/Tavn7nPK/OZds8FbTNyX6XhX/VLWE/5Pj45351b4t\n+fVlU7usSqIu7RxS51NVbr3zY2LsGI3J8Y4K5M45+CVVgf9NNhnEDuBHkm40s596bU7ftIm/+fJX\nitfX9AvZqOmcMIk2zXG/lNn4qN/u6PiYaxt3Ts5G0+/ryfEJ31b3/ajX/SAZnfTL79UbxT6OTST6\nIxEgk2N+u5FJf7/di834YbfNaKKv+oZXu7b66HHXtv3+nxUuV+LK1dff59rGE+dc/4DfrpVoZ84F\nsZa4mDScc+Def/k/bpvpzOex/2Jgm5k9mBeDvB54+TzWFwRBF5lP8G/ixGqpOzhxIoggCE5iFn3A\nT9LlkrZK2nrk4MHF3lwQBB0yn+DfyYmzqZxBwSwwZnaNmW0xsy3LV62ax+aCIFhI5hP8PwLOl3S2\npF7g1Zw4CUIQBCcxcx7tN7OGpDeSzZxSBa41s3uSbVotJryR5UaibL03UpoaQZ305Z/6ZGqU3R/d\nrruj/W4T6on9aiRG+1NKQDOxzqYzip1u4+9As5mYSSrRj/2V4naVhj8yP3Z8xLVVegdcW7Xuqx8t\nR70ZPT7qtpkc9Lc1mZhZq9Ea9P1ISb7OaH9P1b83N+vFykhKtp3OvHR+M/s62WSHQRCcYsQv/IKg\npETwB0FJieAPgpISwR8EJSWCPwhKyrxG+2eLAU1Pikhke1UciS2VupRIzEpmbSmx1oqKr5WVxCW0\nmjA2Uk4m88cSrZyEmuSErKn+6PGb1RKy6PJKsQzYHPN/5Xn3j293bb3rznJt6zb7tp5qsQy4f8cD\nbpu+Pj9BZ7Lh73N12P8R29DaDX47p/uTmZ2OpDubsybu/EFQUiL4g6CkRPAHQUmJ4A+CkhLBHwQl\npauj/Sm8kXSASsUZw0zVskuMslstMSbqDb2CW8TPlKhzl7i8tqoJ/xPrtJRI4JR+skQNvHrLT9Dp\n7fVHvtcM+4ksu356R+Hyww/6o+xje3e4toe3bXNtG1b7BaSeck5xfZn7f/A9t00zcdDqCSVgYHC5\na6tV/f73TtV6SlnwToJZDPfHnT8ISkoEfxCUlAj+ICgpEfxBUFIi+IOgpETwB0FJOXmkvkS9soQi\n5mIJzaPSSmwrUTuv4tTBM/OTkiw1hVPCppTkmCh36NUgTOX19CRuAeuqfq276og/+44dfqxw+Z5H\nH/L9MH/GnhU1f6fHDuxzbStXrihc3tvX77ZpjPnbqvb4mU4JNQ81/TqDVKpOo0Rij1M4MnXe/8pm\nO/5mEAS/VkTwB0FJieAPgpISwR8EJSWCPwhKSgR/EJSUeUl9krYDI0ATaJjZluT3gYpTLy6VqJZO\nY3OaJI1zq4/nkar7l6wX6Ek8QCVhqyW0zx5HImwlpuvqqfpTPJ25eplrG6r4clnN2d6D993vtjl6\nwK/vt2aVnzG3ef2Qa7NKsSTWkH/qN2uJvk/oon2JfpT5Up+sWD40p3YlwKQzRVmyVuM0FkLn/9dm\ntn8B1hMEQReJx/4gKCnzDX4DviXpNkmXL4RDQRB0h/k+9j/TzHZKWgd8W9J9ZnZCiZT8onA5wLoN\nfu3yIAi6y7zu/Ga2M/9/L/Bl4OKC71xjZlvMbMvylSvns7kgCBaQOQe/pGWShqc+A88H7l4ox4Ig\nWFzm89i/HvhyLmXVgM+Y2TfnurKUROFKHqk2rURGlDdlGNBKTBvWSkgvHikZMEUr4X9qlq9l/b2F\ny3tTdUkn/X3uXzbs2hqO3ARQ7S+W3zasW+u2GX3ElwGffsYa1/bCi053bdsmBgqXJ5LzaFpxHwJU\ne/yQmZz0MyBPwy/GWXHSAScS8mzTEbNnI2LPOfjN7EHgwrm2D4JgaQmpLwhKSgR/EJSUCP4gKCkR\n/EFQUiL4g6CkdL+Ap1O0UonrkDwBIyH1KZVhlbjkVRJz/FUd+a2VrNHpSzwpebOVcLKZmMONZrE8\nNDIy4jYZPe7bzlp3mms7Y7kvA/auK57Hb/86/5Rbc46fQfj0NQl5do8vEQ6veXLh8pXLiyVAgL37\njru21atWubbBVcXFQgF6Bvzt7TtyrHD5eGLOwGrVkyM7l5bjzh8EJSWCPwhKSgR/EJSUCP4gKCkR\n/EFQUro72i+hanF9tEpqei1vADM1sJkagU/sdTUxbViPOTalEoV8RyYT03VVE6qDan5WyoSTpNOU\nX5euUetzba1ef5R6xTI/AaZ3WXGfnPt4PwnnyIRvW9byR+ArE+Oureb040Cv3x8DCaXotCG/XuCa\nx53t2o6O++scP1Q8Tdlk4tzpd+JoNnlkcecPgpISwR8EJSWCPwhKSgR/EJSUCP4gKCkR/EFQUrqe\n2ONN15VKcvGSbeao9FFJyHlKyG+p2nnuthKSXc2ZWgugr+bbmg2/r8YaxbJXan2TvupFb0L2Gj9+\nyLVVKU4wOu9cXw772SP3+Y4c9OvjVep+rTsbHStcvnbITyJqrfD7anTCr3e47dG9rm1k0j9mDRVL\nt6okalQ6CVyzmYou7vxBUFIi+IOgpETwB0FJieAPgpISwR8EJSWCPwhKyoxSn6RrgZcAe83sKfmy\nVcDngM3AduBVZubrPieusHBxanott+7fXLQ3oJKoj5esq+eWEkz4njBVU/4nJMdaYuqnFdXiKbR6\n+3xHjsufdmt5rTjjDGD/7odd2zIrrgtYb/iS3ehYcS07AC+hElL17GBZb/EpvqLfX19lpZ812Vjl\nNxwd9G2NA/6+NZyMS29KLgAlpvLqlE7u/J8ALp227ArgJjM7H7gp/zsIglOIGYPfzL4HHJy2+OXA\ndfnn64BXLLBfQRAsMnN9519vZrvyz7vJZuwNguAUYt4Dfpa98LovJ5Iul7RV0tYjhzobFgiCYPGZ\na/DvkbQBIP/f/VGzmV1jZlvMbMvylSvnuLkgCBaauQb/jcBl+efLgK8ujDtBEHSLTqS+zwKXAGsk\n7QDeCbwHuEHS64CHgVd1vEVHFlNCEvNsSaEvJdm1ElN5JeQ3z5TK3MPfFJbYVqIZq1b4U2id94Sz\nCpdvWO0X4hwY8NP6Jpp+P37hb25ybSsHi7Pf+n03GDt2xLX1D/tTg61Y5w85DZ21oXD52pXF04kB\nbN68xrWNnuZPydW7epNre3D7bte2/bH9hcuPHvcl2DFnyrak7DyNGYPfzF7jmJ7T8VaCIDjpiF/4\nBUFJieAPgpISwR8EJSWCPwhKSgR/EJSUrhfw9FLjlJAoZE5WXyoRMCEEpiXCxDodaS4l9Sm1tcQ+\nNxPX5b1jvhB4+JEDhcuX7/L9GOzzbWOj09M6fsmuxw777VYV+79upe/7YMvPVhzo9TP3qn1+Ft6O\nY8X+7z3oZ9kdPebLgHv3+cesf6c/Z2DVm1sPGHVqgtYTk0pW+oozCGeT6Rp3/iAoKRH8QVBSIviD\noKRE8AdBSYngD4KSEsEfBCWlu1Kf+YU609lIXuXMVJPOs5vaSUklLWedlswS9LeVqNFJJbFzo42E\n1DdWXHBzx+hxt01jvLjYJsC6Zb78tn6jn013fGxn4fJ9jz7mtlmJL4e1HGkLYKLHP40nJ4v9H53w\n92v7qN8fR4Z8P5Ynjkut6hfcHKsXa33Nln+CuNJhSH1BEMxEBH8QlJQI/iAoKRH8QVBSIviDoKR0\nd7RfUPGm3kqMbHo191Kj7M2mb0spC6nRfrPidXoqAKQFidS4bMrHlM3r356af6hHjvtKQL3iZJ0A\n55zr16w74CTO7PvRPW6b4R5/JL017NfOq20617WttOIknYGh5W6b+mQiiWjAryVYld9OtcT0Ws7A\n/dhxX3UY7veTmTol7vxBUFIi+IOgpETwB0FJieAPgpISwR8EJSWCPwhKSifTdV0LvATYa2ZPyZdd\nBbwe2Jd/7Uoz+3onG3RlqtT0Wo7EJidJKNUms82tnef7XGW5llObEMASQqA1fdmo7iSlDPf461tz\nxlp/fUf8RJyx+qhrGxkprp2nUb9Nb48/l1draJVrO9LnTwD73ZtvLVw+MuZPhVUZGnJtPdVEvcaE\nXD2ZSFry6vE1zJf6Eqd+x3Ry5/8EcGnB8qvN7KL8X0eBHwTBycOMwW9m3wP8Eq5BEJySzOed/42S\n7pR0rST/uSsIgpOSuQb/R4BzgYuAXcD7vS9KulzSVklbjxyMB4ggOFmYU/Cb2R4za1r2Y/ePARcn\nvnuNmW0xsy3LV/mDNkEQdJc5Bb+kDW1/vhK4e2HcCYKgW3Qi9X0WuARYI2kH8E7gEkkXkSWtbQfe\n0MnGBFQdNSShemFObpyqCb0jtUI/UQ0jIfXJyepzlgM0W/71tdHw68jR9J1M1f7z1lkb9Keg2rB+\nnWt75MgO12YVXy6rHS32ozlaXGMQoL5+mWtbc/aFru3O7f7r5De/+Z1iw/KNbhvMlxwbTb/zj5l/\nzOqj/rEe7C+W+lKZmI2mJzu7TX6FGYPfzF5TsPjjnW8iCIKTkfiFXxCUlAj+ICgpEfxBUFIi+IOg\npETwB0FJ6W4BTxIFMjufZWjmdQGVRPZVelupYpxOIdGEvJLKIEzM7sREQlJqJOTDurPOSk+f22Zw\n6DTXNtzvnyKDVV+2OzZWnL3XlzguPQN+5lv/sP8L8r3btvt+OIVca4n+GDd/n4/XfcnOk7EBrO5n\nYvbUZj/1VtOVpDvX+uLOHwQlJYI/CEpKBH8QlJQI/iAoKRH8QVBSIviDoKR0XeqbSxFM7wqVkvrm\nSmqdninlezORnedlZoEv2QFMptbpSFHVSsLHCX+uvoGqn7lXnTiasBWvc6jfl9iqPf5cdw/t2uXa\nfr7DLzKqZcUSYaPPn3Nvsur7MTE25tr6qr5UWa35c+uNTRb3cb2VOM6tYukwJS1PJ+78QVBSIviD\noKRE8AdBSYngD4KSEsEfBCWlq6P9Zkar5Uy9lWroDLObs66sSSLpR4kplxIF8uS0U6KGX8qPamJb\nFWc0F6A57o/O91jxyHF18pjbplZPJNT0+KPH+3b6I/ATx4tHxZclkmYm5SsBt97yY9f2g1sfcG29\ny4qTlibk1+nD/JH5Sssf7a/43UgzlTDmTBHXSrRJ1XHslLjzB0FJieAPgpISwR8EJSWCPwhKSgR/\nEJSUCP4gKCmdTNd1JvBJYD1ZgbBrzOxDklYBnwM2k03Z9SozOzTDulwprWb+dciTNRJNaDm12wAq\nlZTUl6r95xXIS+guiZp1qTKDq3v9pI6zVvkNayuKpa01y31pa/LQdte2rObX6Ttw1E/6YcyRKvt8\nGW20f42/unE/2ebxZ/rtVm/YULh8tN+vCfjQUf/ceXCnL7/Veoqn3QIYm0xIt+4K/fBsTha3msVs\nXR3d+RvAm83sAuAZwJ9JugC4ArjJzM4Hbsr/DoLgFGHG4DezXWZ2e/55BLgX2AS8HLgu/9p1wCsW\ny8kgCBaeWb3zS9oMPBW4BVhvZlM/8dpN9loQBMEpQsfBL2kI+CLwJjM7oYqDZdUsCl83JF0uaauk\nrYcP+lMpB0HQXToKfkk9ZIH/aTP7Ur54j6QNuX0DsLeorZldY2ZbzGzLilWrFsLnIAgWgBmDX1lm\nyseBe83sA22mG4HL8s+XAV9dePeCIFgsOsnq+13gtcBdkn6SL7sSeA9wg6TXAQ8Dr+pkg54oJktl\nvxUvbyVkuUZC6kumECay8DwZJeE6lpABm/LTwGp9g67t9FW+TLVmZbHUN3nssNvmvgfucG1nbfTl\nq9Oqvnx4tFEsA7aW+1OD9a0/17VNHPH97x1a4dqGVjtDUVV/v/on/Mw9S7SrJ6ZRo5UQ4ZwTvNab\nqCU4Me6vr0NmDH4z+z5+uDxn3h4EQbAkxC/8gqCkRPAHQUmJ4A+CkhLBHwQlJYI/CEpK16fr8qQ0\nKSGFuNNkpbbTuUsL0GxOtCp+9x+c8KXKA4/4stfkA/sKl9fG/F9XNvb6026dNuxLc8cOF/6uC4C+\nWvG0YSxL/NBr6HTXtOPAbte2fbcvze2sjBYuP9zypbKRhn9cRlt+kVE5U6UB9CQLwxbbmgl50CuE\nOxvizh8EJSWCPwhKSgR/EJSUCP4gKCkR/EFQUiL4g6CkdF3q8+bJUyWl2xXbLKH1KSHapebPS9s8\ny2zKJv6SqvwineaXdWQ0kUZ44Hix3PS4ZX6WYN+gX1Tz4Yd3uLbW4QOubW2lOKvPTvOlvjsf3u/a\ntj3iy4qVletc20itOPPwSN0/Zo0ePywqPb6s2Jjwi5329Pp97J1z3hx+eauErTPizh8EJSWCPwhK\nSgR/EJSUCP4gKCkR/EFQUro62i/JnSqrkkhU8JJ+KokRT0uM2ldTI/qJuoBWKa6510qO9vu2asJm\nCSWgNVmcrAKg8WOFy5cN+/Xgqv3+Pj+6wx9lX+tMGQVQ6y1OgNmbSFj6/h13u7aDidH5tYN+Xb2J\npjNNlqM6AVSrieNZ8ZN3rObvW73l91UPxeeVnPMNgKoXup2rAHHnD4KSEsEfBCUlgj8ISkoEfxCU\nlAj+ICgpEfxBUFJmlPoknQl8kmwKbgOuMbMPSboKeD0wVTTuSjP7+kzrc6W+pA/Fy5PpNAnFo5Ww\neYlHAKoWJ2ckmlBRIjmjkkj26EnsXX2PaxqoF0t91Qm/9ty6Df7s6o9tu9O1VSYSyTFDw4XLdx8p\n9g/g6FG/rl6zb5lrO94sTiICqI0Xy6IV+dJnHX99wj+etX5/nc2ExOlLvr7UV6kVH89WQsaeTic6\nfwN4s5ndLmkYuE3St3Pb1Wb2vo63FgTBSUMnc/XtAnbln0ck3QtsWmzHgiBYXGb1zi9pM/BU4JZ8\n0Rsl3SnpWkn+1LFBEJx0dBz8koaALwJvMrOjwEeAc4GLyJ4M3u+0u1zSVklbDx/0a8cHQdBdOgp+\nST1kgf9pM/sSgJntMbOmZeVGPgZcXNTWzK4xsy1mtmXFqsSEDUEQdJUZg19ZjaGPA/ea2Qfalm9o\n+9orAT8rIwiCk45ORvt/F3gtcJekn+TLrgReI+kiMsVtO/CGTjYoZ9oiJa5DnnhRSYl9iey8ipsR\nBdVEIlWPk+3VSLRRxZd4GuZnetWbvuw1ejAh9Xmba/oSUL3hZ6oNJKStvkRnTVSKpai+VWvdNqct\n96cNqxzxMxknxkdcW93JwqtU/ZqGqviS3WBKIqz6fTVRdbILSdXw889vTzJvzSKrr5PR/u9THH8z\navpBEJy8xC/8gqCkRPAHQUmJ4A+CkhLBHwQlJYI/CEpK9wt4Oilw1eR0Xd76EkZLXNcSmXbVhHxV\nc2S7RP1Lmgn5p5Ho/RWDQ66tcqYvlzUPF8tevVW/s/Yf8H952Trmy2iDPX5W4vjgacXrG1zutlHN\nz/hbO+RnJVrN37cBZ5qy/mF/iq9jiQy84/VExl/Dl26bLV+6rVWKT4RW3ZcHPSl7Fkl9cecPgrIS\nwR8EJSWCPwhKSgR/EJSUCP4gKCkR/EFQUroq9QFUHcmpYgmNwstuSsxlllIOm1VfkqkkGvbUvOKj\nvuRVb/ldXKlPuLaBMV/2umiNL5cdqRTLQ6MtXzYaG/GlreFefx68oZUDrm1yuLh2wwOPPOq2Obzf\nz1Y895zzXRsDvo91J3tvvOYfs8aEf1wmEnNKtpyMVYBKQkL2MvRIrK9aK15ffRZaX9z5g6CkRPAH\nQUmJ4A+CkhLBHwQlJYI/CEpKBH8QlJTuZvVh7tx1KYXCVfoScp5avrE/UeRwUn47c2z1hORYr/mF\nIkcmDru2+7/3Tde2aeSAa9t8wRMKl687+1y3zUDzca5t30Zf9tqfOGjjgysKl6+cPOK22bDRvxet\nOM2fE+ZQw/fj4X3F2xvrSxTUTMwL2Ewk2lVrfjjJT0rEE56biWzFSmP2WbC/so55ryEIglOSCP4g\nKCkR/EFQUiL4g6CkRPAHQUmZcbRfUj/wPaAv//4XzOydks4GrgdWA7cBrzUzv8AZgPxZtJQYgfdK\n7iUG5pOTFiXr9CVGbCtOjTYlplVKTRvW6CseEQdY88wXubbJI3td23cPHipcPrTH9/H0Hr8mYON8\nf3LV/Ud8vyrYAAAE0klEQVT8+n6yYpVjT2O72+bgcv+4aO1m1zY56SfbVGy4cLkl6uMNDfn1EydG\nj7s2S0y/llICWlbsf63XTz6qOErL5AIn9kwAzzazC8mm475U0jOA9wJXm9l5wCHgdR1vNQiCJWfG\n4LeMqfzSnvyfAc8GvpAvvw54xaJ4GATBotDRO7+kaj5D717g28ADwGEzm3p22gFsWhwXgyBYDDoK\nfjNrmtlFwBnAxcATO92ApMslbZW09VCiPnwQBN1lVqP9ZnYYuBn4HWCFpKlRjDOAnU6ba8xsi5lt\nWbnaHzwKgqC7zBj8ktZKWpF/HgCeB9xLdhH4g/xrlwFfXSwngyBYeDpJ7NkAXCepSnaxuMHMvibp\np8D1kt4F/Bj4+MyrEp4Il5L6jAWeyivVLuGHJxGq5Us8lpABByt+AkljwLf9/LCvqB5fsaZw+aGm\n7+OxWt21bVyZmF6r4UtzRw4VT0+1+5Cf2ENPYkquXt824UhlAL1Dxf6bI4kCyZOn0ju3XDhLJJo1\nnWPT29vrtqk6yWRjszjxZ9wTM7sTeGrB8gfJ3v+DIDgFiV/4BUFJieAPgpISwR8EJSWCPwhKSgR/\nEJQUpaSoBd+YtA94OP9zDbC/axv3CT9OJPw4kVPNj7PMzE/TbKOrwX/ChqWtZrZlSTYefoQf4Uc8\n9gdBWYngD4KSspTBf80Sbrud8ONEwo8T+bX1Y8ne+YMgWFrisT8ISsqSBL+kSyXdL2mbpCuWwofc\nj+2S7pL0E0lbu7jdayXtlXR327JVkr4t6ef5//78VIvrx1WSduZ98hNJfiXRhfPjTEk3S/qppHsk\n/UW+vKt9kvCjq30iqV/SrZLuyP34q3z52ZJuyePmc5L8tL9OMLOu/gOqZGXAzgF6gTuAC7rtR+7L\ndmDNEmz394CnAXe3LfvvwBX55yuA9y6RH1cBb+lyf2wAnpZ/HgZ+BlzQ7T5J+NHVPiHLex/KP/cA\ntwDPAG4AXp0v/yjwp/PZzlLc+S8GtpnZg5aV+r4eePkS+LFkmNn3gOk1zV5OVggVulQQ1fGj65jZ\nLjO7Pf88QlYsZhNd7pOEH13FMha9aO5SBP8m4NG2v5ey+KcB35J0m6TLl8iHKdab2a78825g/RL6\n8kZJd+avBYv++tGOpM1k9SNuYQn7ZJof0OU+6UbR3LIP+D3TzJ4GvBD4M0m/t9QOQXblhzmUL1oY\nPgKcSzZHwy7g/d3asKQh4IvAm8zsaLutm31S4EfX+8TmUTS3U5Yi+HcCZ7b97Rb/XGzMbGf+/17g\nyyxtZaI9kjYA5P/70/IsIma2Jz/xWsDH6FKfSOohC7hPm9mX8sVd75MiP5aqT/Jtz7pobqcsRfD/\nCDg/H7nsBV4N3NhtJyQtkzQ89Rl4PnB3utWiciNZIVRYwoKoU8GW80q60CeSRFYD8l4z+0Cbqat9\n4vnR7T7pWtHcbo1gThvNfBHZSOoDwNuXyIdzyJSGO4B7uukH8Fmyx8c62bvb68jmPLwJ+DnwHWDV\nEvnx98BdwJ1kwbehC348k+yR/k7gJ/m/F3W7TxJ+dLVPgN8kK4p7J9mF5h1t5+ytwDbg80DffLYT\nv/ALgpJS9gG/ICgtEfxBUFIi+IOgpETwB0FJieAPgpISwR8EJSWCPwhKSgR/EJSU/w9jcw8Ly8JP\nVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9b2a81490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEVCAYAAAAVVdvAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9b285b350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    # processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypeuint8\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[143 146 144]\n",
      " [165 167 164]\n",
      " [155 157 160]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHPRJREFUeJztnVuMXNd1pv916tZXNrvZpESRsmQ7GhgaJ5Y9HNmBncCJ\nkUBjBJANDAz7wdCDEQaDGIgB50HwAGMHyIMTjG34YeABPRKiBI4viW1YCIyZ2JoAQl4UUY5EibpS\nFEWRbDbFS98vdVt5qCJCtfa/uljNrpa0/w8gWL1XnXPW2XVWnar911rL3B1CiPwodtoBIcTOoOAX\nIlMU/EJkioJfiExR8AuRKQp+ITJFwS9Epij4hcgUBb8QmVLeysZmdg+AbwMoAfg/7v716PljY2M+\nOTlF9nX9x49+nWj97BBAu92+7uMVBX8P7dePaLvovG/0Lzb7PRaz9fua9TuP/fixHfNbBP5Xq9Xk\neHzOaT8uXbqMxaWlniar7+A3sxKA/wXg9wCcAfC4mT3s7s+ybSYnp/DlL/9p0lYqcX9ZQEYvRBSQ\n0Xarq6vU1mw2k+NjY2N9+RFRLvOXJnqDWl9fv+5jlUolams0GtTWz1y1Wi26TXTOkY8RzP9oDpnv\nQHzO5SDkhmsVarvtttuS45UK34bN459//S+5ExvYysf+uwGccPeT7l4H8AMA925hf0KIAbKV4D8A\n4LVr/j7THRNCvA3Y9gU/MztsZkfN7Ojy8tJ2H04I0SNbCf6zAG695u+D3bE34O5H3P2Qux8aHeXf\njYUQg2Urwf84gDvM7N1mVgXwWQAP3xi3hBDbTd+r/e7eNLMvAvh/6Eh9D7r78WibSqWMffv2JW3R\nojhb2YxWZaNV5aUl/vVj9+7d1DY3N5cc37VrF92GyThAvDIfKRL9yJHRNtEqe+R/tBq9trZ23X70\ns7q92T6ZLVIPormvVWvUVi74dtUqn+Px8fHk+OjoKN2GnVf0Wr7puT0/M4G7/xzAz7eyDyHEzqBf\n+AmRKQp+ITJFwS9Epij4hcgUBb8QmbKl1f7rpd1uY2VlJWmLpD4my/Qr/9xyyy3UNjQ0RG3z8/PJ\n8UiiiqSyfrPYIilqZGQkOR7NVUTkR5T0w+Yx2l+UBNWv/0wOjq6Per1ObdF21uY+loy/ZhVyjUSy\nHbsGrif7UXd+ITJFwS9Epij4hcgUBb8QmaLgFyJTBrra7+509TVa7WcJMNHKa1Ra6wMf+AC1Xbp0\nidoWFhaS4/2qB9EKdlQuKrL1U2Ou3/JZ/SQf9VvD73oSVq6FKTHReUXXjreD+W3zRLNycH1PkMSw\nWo0nETFFwqz3+7nu/EJkioJfiExR8AuRKQp+ITJFwS9Epij4hciUwSb2tNpYWlxMG4PEByaJRVJZ\nJLFFiRvPP/88tR0/ni5RGElvH/7wh6ntXe96F7WdOnWK2l5//XVq67dDECOS36KEJka/CTqR7BUl\nGDFpOdpfdM5R3cgosacAl6VZMhar7Qf0J6W+2SchRJYo+IXIFAW/EJmi4BciUxT8QmSKgl+ITNmS\n1GdmpwAsAmgBaLr7oXgDoCDZVBZIfUy+iGStSM574YUXqO21116jtgsXLiTHWWsqABgeHqa2vXv3\nUluUWRbtk8lUkQQU2VotLm1F7caYJOZBJib6rBcYyYeNenq75aBlG7tGAQBRVqL3J/UxyTSUFclc\nXY+UeiN0/t9x94s3YD9CiAGij/1CZMpWg98B/KOZPWFmh2+EQ0KIwbDVj/0fc/ezZrYPwC/M7Hl3\nf/TaJ3TfFA4DwOTk5BYPJ4S4UWzpzu/uZ7v/XwDwUwB3J55zxN0PufuhqN+4EGKw9B38ZjZqZuNX\nHwP4fQDP3CjHhBDby1Y+9t8E4KddyaEM4G/d/f9GG7g76g0ii/VRDDLKvtq//2Zqi6S+xx9/nNoW\nSUYiy8oCgDNnzlDbK6+8Qm179+2htiJQopg0Z1G2XyC/FUH3p3Igz5ZK5Hhl7nw5sBXBSUeJbI1y\nWkZrBTJaK5iPRoNLyEURteTioTY6mpZuPZAO6+tpCdM9kFI30Hfwu/tJALwMrhDiLY2kPiEyRcEv\nRKYo+IXIFAW/EJmi4BciUwZawNPMUK2lpZd2g0svTVx//7lz585S28zMTLDdOe5HH8Ugox82RbLR\n6BjfbniEFydl/QTLgSxaqnAZrRVIR5Gs1CbFLFmWHQA0Al2xMO5jo9G7vHWVKCMxyiAMC2Qal+Yq\nwRyPj6d79Y2P88zOEpFFA2X2TejOL0SmKPiFyBQFvxCZouAXIlMU/EJkykBX+wEAnn6/8T7eh9bX\nee28X/7y/1MbSxQCgFJQv43ZorZV1WqV2lqtoIVT0G7s/XfeSW0nT55MjkctvtptvoJdBCpBNagl\nyOZkYmKCbhO1WFtd5a/1pYtXqG1lJd1KLUoKi9SbaLW/7VyxKpWitmfpa6RW4/PbJtdOmMC1Ad35\nhcgUBb8QmaLgFyJTFPxCZIqCX4hMUfALkSkDTuwpUC6lZZSScbnMialW5VLIb/z6B6mtCGSXV155\nmdqOHTuWHI+Sd6L6flHixsgQP7dalc/VHe95b3K8UvCXem0tLYcBwFCNy2+Tu3dT25496RqEUfn2\naB6bDZ40MzvLZcxnn30uOX7+/Hm6zcrKCrVFST9FkJjUDCTC+npaIlxb5YlfTJ6NZNuN6M4vRKYo\n+IXIFAW/EJmi4BciUxT8QmSKgl+ITNlU6jOzBwH8AYAL7v7+7tgUgB8CuB3AKQCfcXeeWtWl0Wjg\n3LnZpC1qn+REvvCgtl8RZjfxbLp6ncsrLFNtOMhuO3jwILXd8Wt3UNueCS6jra1yaW5sOC0t/ucP\n/Se6TavFZbShYS71RRmLLAMyypqMMu3KJf6aHTjA53hsLF0f7/TpV+k2L774IrU9//zz1LZKMggB\noCjxUCsX6XlsNPj1XS6n98ey/ZI+9fCcvwJwz4ax+wE84u53AHik+7cQ4m3EpsHv7o8CuLxh+F4A\nD3UfPwTgUzfYLyHENtPvd/6b3P1q/evz6HTsFUK8jdjygp93SpvQLydmdtjMjprZ0ehnk0KIwdJv\n8M+a2X4A6P5/gT3R3Y+4+yF3PxT9zl0IMVj6Df6HAdzXfXwfgJ/dGHeEEIOiF6nv+wA+DmDazM4A\n+CqArwP4kZl9AcCrAD7Ty8FWV9bwzNPPJm1Li0t0OyZFtYN2UaWCS0pt55lZ6/VFamNS3+4gu+19\n73sftR08cIDaysG5MTkPAObm5pLjHrRDm56a4n7UuJzX7qONWlQAczWQMNfXuQRbGL+Mx0jbs/Hx\ncbpNlF1YqfBjPfnCS9QW1IzF0mL663AkpTIpeyWYw41sGvzu/jli+kTPRxFCvOXQL/yEyBQFvxCZ\nouAXIlMU/EJkioJfiEwZaAHPtjvq9bTk1GoFBQ7raWmOjQM86wkAihLPYmsEkhiTXqLMvVtuuYXa\ngh9GAoEkxnPfgBrxsR5IZWtBH7xqIDkWQSFRNv+RnDc/P09tFy9uTC/5d7zNZ2RsLF0k9fJlvr+X\nX+ZFXPfu3Udt+/bxX7m/cpJnES4tLSfHzfgvYln2XjO4fjeiO78QmaLgFyJTFPxCZIqCX4hMUfAL\nkSkKfiEyZaBSn3sbq/W0fBEkZtE+cy1wWcOMS1QW9FQLi1JaOlNw1xjPEBuu8RNbXkxn4AHASDXd\n0xAA1tfWqY3Jh0MjvBDneoPvrx0UO0UgtZZKaRlwLfD93Jl0cVcAuDBLS0agXOFZjuu70/6fn+H9\n/Y4de5rapvemexACwPQenh155vQZanOStTrcR/2LuHDthude996FEO8IFPxCZIqCX4hMUfALkSkK\nfiEyZaCr/WaGKqmD12rzlXu2glmtcvfD6nJBskrZ+Co7W8EuByus9VWenLEwx5NLFsBrEM7N8QSY\n2lDa/z1TfJWabQMASys86WclsBmRb+aucN+PHz9ObdNBQs3Bgzx5amU1nTTz0gleb+/SZa4ENJr8\n9dw7fTO1VWpBTUlLr/YXFa5K0VqIUdbXxv33/lQhxDsJBb8QmaLgFyJTFPxCZIqCX4hMUfALkSm9\ntOt6EMAfALjg7u/vjn0NwB8CuKqJfMXdf77ZvtrtNlapPMTFuXY7bWs0eGJJhFnwnhfIgCCJRKfP\n8KSNXc/y5IyRQGIrl/l2x49zmWrXrl3J8fm9XKKK5uPy5UvUFiX9DA+lW16deInXx5u9wCW2W2+7\nndomJoepbW5hJjn++sXX6DbuvM5gucJfs8U1Lt3WSzyhqVlJ11f0IV5rkiVIhYlYG+jlzv9XAO5J\njH/L3e/q/ts08IUQby02DX53fxQAf0sTQrwt2cp3/i+a2TEze9DMJm+YR0KIgdBv8H8HwHsB3AVg\nBsA32BPN7LCZHTWzo/1+RxdC3Hj6Cn53n3X3lru3AXwXwN3Bc4+4+yF3P8T62wshBk9fwW9m+6/5\n89MAnrkx7gghBkUvUt/3AXwcwLSZnQHwVQAfN7O70NHnTgH4o14O1m63sbKSzrIqSjwdibV+Klf4\ne1fQ7WoT+D5LZLb+9cmjdJtzZ09R28c++lvUNj3F5avLl3jtv5lz6Tp4J146SbeJsgTdeLbl0Biv\nd1gmk3U6qGW3usKPdTGQHKev8LlaIfJbs71Et5mY5HLe+AT/9FoveEu0iVuCUCvSMuw6TwREZVfa\naKXepb5Ng9/dP5cYfqDnIwgh3pLoF35CZIqCX4hMUfALkSkKfiEyRcEvRKYMtoBn4aiOkWKFQQst\n97QE1GxyaajVjiQPfqxalUs5bSKjXJrnGXNt59lcp149RW1Re6r5BZ79dvHSxeT4+hovthnNR3mY\n6022FrVES9tOvMqz+mbOXqG2SHL8yPn/SG2TU+lWaqVARts1kc5IBICRES5vNhpcgp2+iV9X6+vp\na2R19TzdpuzpE3Dv/Ve0uvMLkSkKfiEyRcEvRKYo+IXIFAW/EJmi4BciUwYq9RVlR213WtZotbiU\nw4qANMBltMsLC9TmvC4ipgKZZ9fQUHK8FUxjpTZNbfUmzwJ7/sknqG1lhZ8bK+xYr/NjlQPdqxoU\nrCxVuURYEKlvdJLLgyMrfH+vnuUS4e238jkul9P3t3qdX2/RPXFtjUtpjQYv/NmKip3W0tdVdTg9\nDgBOCt4W1nuzPt35hcgUBb8QmaLgFyJTFPxCZIqCX4hMGfBqPzC+L21rBSvwzWZ6BbPZ5O6P7BkL\ndshXXvdNphNBAGB8LK0ErC/x/TXW+Arw+dmz1LayxpN3mp6ugwgAXqRXsavDUY3EIMulzFe328Gt\nozqcfm3e/R/2BttwZeH86UVqG9/F20bsv/nW5PhTTx2j29SDEvM+xBN0Rmu7qW2oxue4KNK2So0f\nq9VOB0ypeJFu86bj9vxMIcQ7CgW/EJmi4BciUxT8QmSKgl+ITFHwC5EpvbTruhXAXwO4CZ32XEfc\n/dtmNgXghwBuR6dl12fcnRdhA2DmqFbSspiXeH+t0giRgJzXU2s7b+FUJdIKAAyT1mAA0CbJR9Uq\n92NphWuYJ17mLbQqZZ60NDLGfSwRdago8ff5dpvLgIEJq0u8dmGrkZ7j3RM8WWXP7l3UNn+BS6ZL\nK3yuhkbT+yzKQU29ZZ4ENTrG5cjgsoL33z8ufSxLv569p/X0dudvAviyu98J4CMA/tjM7gRwP4BH\n3P0OAI90/xZCvE3YNPjdfcbdf9V9vAjgOQAHANwL4KHu0x4C8KntclIIceO5ru/8ZnY7gA8CeAzA\nTe4+0zWdR+drgRDibULPwW9mYwB+DOBL7v6GahLe+UKT/FJjZofN7KiZHa2vBb/hFUIMlJ6C38wq\n6AT+99z9J93hWTPb37XvB5DsMuHuR9z9kLsfqg4FKyJCiIGyafCbmQF4AMBz7v7Na0wPA7iv+/g+\nAD+78e4JIbaLXrL6Pgrg8wCeNrMnu2NfAfB1AD8ysy8AeBXAZzbbUQFDtZ2++zfr/CtBazUt5QwP\nczlvNWhP1QiK+LWCdkeszmARCCxDlQnuRyBvRv6jxP2v1tLv5x60L2sGHZ4qpUDaqnPZrlKkbfV5\nfs4WdFibGB2hthOvnKK2yT170uPTU3Sb+cVZaqsF7ctKteAESkHNQFLvsAEuORal9DXn0SRuYNPg\nd/d/BpcPP9HzkYQQbyn0Cz8hMkXBL0SmKPiFyBQFvxCZouAXIlMGWsDT20BrNS1FtAO5ib1HWVB4\nstziWVsWvOd5K5BKiK0UZAKO1LhENfWutAwFAOdnX6O2cpVnsZUqaRtrWwUA3ub+N9e5NFcBn+NK\nkbZZK5BFh7is2BznPp589Ry1Xb50e3J83z5eSHR25hVqq1V4BicrqgkAMH6tOrnkokxAZ9PYu9Kn\nO78QuaLgFyJTFPxCZIqCX4hMUfALkSkKfiEyZaBSX6koYWI43VetPMpdqTfS2U2RFDI0nO6rBwBl\nVuUSvDAiALRaaRmtMC5RlQLb+Cj3sTnJ+8+hCHr1Ef890oCcz70Fc9Vc59JWm2QRVqt8PkaHuSxa\nMp7B+eI6z8KrkOy3+irPmmw3eAZeQTU2oFLm/SEXl3mvwVYzPY9FKZCyicztgX9v2n/PzxRCvKNQ\n8AuRKQp+ITJFwS9Epij4hciUga72F1bCSDld065S4q7ULKh/RigFK6UWJFmwVWoAqJHWT/Wg/qC3\n+LHOnznLjzXEV9mnp3lSyno9fbzllSV+rBpfpS6B1+lbbvJ2XWMTaSWjEiTGtIPEmKbxVezp3VwZ\nWVqYTxsafH4nx/n+hipcdajV+FwVbW5rtdLnXV8PErg8/Tobeq+QrTu/EJmi4BciUxT8QmSKgl+I\nTFHwC5EpCn4hMmVTqc/MbgXw1+i04HYAR9z922b2NQB/COD17lO/4u4/Dw9WqmDvxM3p4wTbtVjt\nvEDOKwq+R6KsAACapCUXAJQrxA/wJJFWg0/x3OUZatu3j3c8nxjlrabao2kpbdco9zFKTCqcS3MT\nQ7zw4tBQWtpqt3kyVvSaFcalyvGRy9R28/R0cnx0mJ/XWI2/ZtUKv1+a8e3Gg0QtIzJms8mvRdZ+\nrRIkYm2kF52/CeDL7v4rMxsH8ISZ/aJr+5a7/8+ejyaEeMvQS6++GQAz3ceLZvYcgAPb7ZgQYnu5\nru/8ZnY7gA8CeKw79EUzO2ZmD5pZkIAuhHir0XPwm9kYgB8D+JK7LwD4DoD3ArgLnU8G3yDbHTaz\no2Z2dGUlaDsthBgoPQW/mVXQCfzvuftPAMDdZ9295e5tAN8FcHdqW3c/4u6H3P3QyAj/fbMQYrBs\nGvzWWYp8AMBz7v7Na8b3X/O0TwN45sa7J4TYLnpZ7f8ogM8DeNrMnuyOfQXA58zsLnTkv1MA/miz\nHRUoMEzqnC0v8hpnlUpaiqpY1JKLy0blYS5tRS2X3NJfW1hrKgBoB7LR5EQ6wxEA0OTvy+06P7cS\nyZqrRDIaohqE/NxGxngNQpD6il5wqc8CH0vgGYRDNS7bTe1Oz/FY8Cm0CHrHtZrpepIAUATty8pB\ntl2bSNmVQDosSPu1Ish+fLNPm+Du/4y0DB9q+kKItzb6hZ8QmaLgFyJTFPxCZIqCX4hMUfALkSkD\nLeDpzotdVmu8VRPLeoqyqGo1Ll8NBW2hHFyKWq+n3ytLRdDuKpDKxka5H8vLQcuoQFocqqZtZSIN\nAbGP7SYvaFoquHzFilI2GlxGK5f5PIaFVSvBdUDmY2JXuhgrANTXV6nNAym4IEU1AaBV5/63mfwZ\ntKNjsFhJoTu/EJmi4BciUxT8QmSKgl+ITFHwC5EpCn4hMmWgUl/b21heS2fGjY7wHmgg8kUDXHax\ndiCVtXhm1soqzx5bW+MSEKNa5dJLI8geW1ghPeYANIJzGyml5cNO2YU05XKQ8RfImAikPm+QrD7n\n95tmILPWg8KqlSBzEpbeZ73J574oc+mzQc4LACplnl0YKK1UFrWCb+REBjTr/X6uO78QmaLgFyJT\nFPxCZIqCX4hMUfALkSkKfiEyZaBSX6vVxiIp1Lm6tky3GxpKy4D1OpfsWK84ALAFLm2tra9TGyuO\nuLCwQLfZv38/tbXB5be1Opccz5w7Q22t1t7k+HggpUYy4Moq77Ww3uBS6/nz58n4LN2mEmTnRdmA\n0XUwM5PuhxhlCc7Och+rVS7nDVeC6zGQD5l4aB5k6LECqcE1tRHd+YXIFAW/EJmi4BciUxT8QmSK\ngl+ITNl0td/MhgA8CqDWff7fu/tXzezdAH4AYA+AJwB83t35cicAwNEmK8vLy3xVeWUlnVDTbPJk\nj6geXFTlrBKs5rIV52glenWVJwMtL3OFY3iYqxXzc1eobXE+bZsKWoOxOncAcGWOJxgtBP5funQ5\nOc6SWABgzxTv8l6ucB8rge3ixYvJ8Wi1//Tp09Q2Pj5ObbuCmoyTQc3ACpn/anBe4UXcI73c+dcB\n/K67fwCddtz3mNlHAPwFgG+5+68BuALgC1t3RwgxKDYNfu+w1P2z0v3nAH4XwN93xx8C8Klt8VAI\nsS309J3fzErdDr0XAPwCwMsA5tz96ufuMwAObI+LQojtoKfgd/eWu98F4CCAuwG8r9cDmNlhMztq\nZkdX1/iv54QQg+W6VvvdfQ7APwH4TQC77d+7ZhwEcJZsc8TdD7n7oeEh3khDCDFYNg1+M9trZru7\nj4cB/B6A59B5E/iv3afdB+Bn2+WkEOLG00tiz34AD5lZCZ03ix+5+z+Y2bMAfmBmfw7gXwE8sNmO\n3LnEErXXWl9PK4iRnFcE9c+ilkZRQtDU1FRyfD1ILPFAUpres4fayjUu87QaXOK8MHshOX7lyhzd\nZmSYJ/0UJT7HY4HsNT6elrZGAzkskvOieeynBdj8PJcwJwJZNJL6qhV+za02uCzatvS1b2VeL7BU\nYvUTe2/xtWnwu/sxAB9MjJ9E5/u/EOJtiH7hJ0SmKPiFyBQFvxCZouAXIlMU/EJkirG2P9tyMLPX\nAbza/XMaQDrlarDIjzciP97I282P29w9XchxAwMN/jcc2Oyoux/akYPLD/khP/SxX4hcUfALkSk7\nGfxHdvDY1yI/3oj8eCPvWD927Du/EGJn0cd+ITJlR4LfzO4xsxfM7ISZ3b8TPnT9OGVmT5vZk2Z2\ndIDHfdDMLpjZM9eMTZnZL8zspe7/vJrl9vrxNTM7252TJ83skwPw41Yz+ycze9bMjpvZn3THBzon\ngR8DnRMzGzKzfzGzp7p+/Fl3/N1m9lg3bn5oZrzabC+4+0D/ASihUwbsPQCqAJ4CcOeg/ej6cgrA\n9A4c97cBfAjAM9eM/SWA+7uP7wfwFzvkx9cA/OmA52M/gA91H48DeBHAnYOek8CPgc4JOrV5x7qP\nKwAeA/ARAD8C8Nnu+P8G8N+2cpyduPPfDeCEu5/0TqnvHwC4dwf82DHc/VEAG2tb34tOIVRgQAVR\niR8Dx91n3P1X3ceL6BSLOYABz0ngx0DxDtteNHcngv8AgNeu+Xsni386gH80syfM7PAO+XCVm9z9\nakvZ8wBu2kFfvmhmx7pfC7b968e1mNnt6NSPeAw7OCcb/AAGPCeDKJqb+4Lfx9z9QwD+C4A/NrPf\n3mmHgM47P66nJMuN5TsA3otOj4YZAN8Y1IHNbAzAjwF8yd3f0Pd8kHOS8GPgc+JbKJrbKzsR/GcB\n3HrN37T453bj7me7/18A8FPsbGWiWTPbDwDd/9P1uLYZd5/tXnhtAN/FgObEzCroBNz33P0n3eGB\nz0nKj52ak+6xr7tobq/sRPA/DuCO7splFcBnATw8aCfMbNTMxq8+BvD7AJ6Jt9pWHkanECqwgwVR\nrwZbl09jAHNinaKKDwB4zt2/eY1poHPC/Bj0nAysaO6gVjA3rGZ+Ep2V1JcB/Pcd8uE96CgNTwE4\nPkg/AHwfnY+PDXS+u30BnZ6HjwB4CcAvAUztkB9/A+BpAMfQCb79A/DjY+h8pD8G4Mnuv08Oek4C\nPwY6JwB+A52iuMfQeaP5H9dcs/8C4ASAvwNQ28px9As/ITIl9wU/IbJFwS9Epij4hcgUBb8QmaLg\nFyJTFPxCZIqCX4hMUfALkSn/BtebKG0lYkmTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9b27b0550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2, tf.shape(a1), tf.shape(a2), tf.shape(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_, sha1, sha2, sha3 = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(inp, sha1, sha2, sha3):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, sha3, \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, sha2,\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, sha1,\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.shape(tf_input),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_, sha1, sha2, sha3)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector, sha1, sha2, sha3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 267291648.0\n",
      "range:(4480, 4608) loss= 8623714.0\n",
      "range:(8960, 9088) loss= 1766394.625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1795680.125\n",
      "range:(4480, 4608) loss= 894263.8125\n",
      "range:(8960, 9088) loss= 844786.5\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 907631.0625\n",
      "range:(4480, 4608) loss= 693906.75\n",
      "range:(8960, 9088) loss= 544853.25\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 529674.25\n",
      "range:(4480, 4608) loss= 460250.1875\n",
      "range:(8960, 9088) loss= 975004.25\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 753818.625\n",
      "range:(4480, 4608) loss= 527953.125\n",
      "range:(8960, 9088) loss= 259654.671875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 2\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 373265.6875\n",
      "range:(4480, 4608) loss= 664563.75\n",
      "range:(8960, 9088) loss= 1212874.625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1113232.25\n",
      "range:(4480, 4608) loss= 460812.84375\n",
      "range:(8960, 9088) loss= 173186.234375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 188721.578125\n",
      "range:(4480, 4608) loss= 125827.070312\n",
      "range:(8960, 9088) loss= 301285.4375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 296571.03125\n",
      "range:(4480, 4608) loss= 428467.21875\n",
      "range:(8960, 9088) loss= 939268.0625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1091917.125\n",
      "range:(4480, 4608) loss= 209373.0625\n",
      "range:(8960, 9088) loss= 134634.421875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 3\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 164209.234375\n",
      "range:(4480, 4608) loss= 154734.53125\n",
      "range:(8960, 9088) loss= 407351.03125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 573466.375\n",
      "range:(4480, 4608) loss= 152659.828125\n",
      "range:(8960, 9088) loss= 126023.53125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 182885.75\n",
      "range:(4480, 4608) loss= 199035.109375\n",
      "range:(8960, 9088) loss= 357477.96875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 378107.46875\n",
      "range:(4480, 4608) loss= 147927.546875\n",
      "range:(8960, 9088) loss= 63707.8242188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 68315.984375\n",
      "range:(4480, 4608) loss= 63555.2421875\n",
      "range:(8960, 9088) loss= 67457.828125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 4\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 84038.1015625\n",
      "range:(4480, 4608) loss= 61504.1523438\n",
      "range:(8960, 9088) loss= 119988.664062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 117522.09375\n",
      "range:(4480, 4608) loss= 172112.390625\n",
      "range:(8960, 9088) loss= 279769.3125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 333000.46875\n",
      "range:(4480, 4608) loss= 217149.40625\n",
      "range:(8960, 9088) loss= 161547.8125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 157213.828125\n",
      "range:(4480, 4608) loss= 63010.203125\n",
      "range:(8960, 9088) loss= 76567.2265625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 70584.65625\n",
      "range:(4480, 4608) loss= 53994.7695312\n",
      "range:(8960, 9088) loss= 119069.554688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 5\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 296736.8125\n",
      "range:(4480, 4608) loss= 110605.34375\n",
      "range:(8960, 9088) loss= 44813.9960938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 61336.8046875\n",
      "range:(4480, 4608) loss= 52603.9726562\n",
      "range:(8960, 9088) loss= 32209.4199219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 31399.4453125\n",
      "range:(4480, 4608) loss= 44482.296875\n",
      "range:(8960, 9088) loss= 258228.25\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 189216.515625\n",
      "range:(4480, 4608) loss= 73946.7890625\n",
      "range:(8960, 9088) loss= 44019.6914062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 33249.3945312\n",
      "range:(4480, 4608) loss= 62359.8554688\n",
      "range:(8960, 9088) loss= 46523.6054688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 6\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 59902.8007812\n",
      "range:(4480, 4608) loss= 49971.5273438\n",
      "range:(8960, 9088) loss= 38968.1367188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 54382.9921875\n",
      "range:(4480, 4608) loss= 21366.6132812\n",
      "range:(8960, 9088) loss= 127969.078125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 145068.703125\n",
      "range:(4480, 4608) loss= 56288.7148438\n",
      "range:(8960, 9088) loss= 47051.9726562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 92100.796875\n",
      "range:(4480, 4608) loss= 86070.9375\n",
      "range:(8960, 9088) loss= 37439.1171875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 21020.671875\n",
      "range:(4480, 4608) loss= 17137.5546875\n",
      "range:(8960, 9088) loss= 19530.6894531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 7\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 18350.0527344\n",
      "range:(4480, 4608) loss= 24096.8515625\n",
      "range:(8960, 9088) loss= 40780.9609375\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 2\n",
      "range:(0, 128) loss= 80567.015625\n",
      "range:(4480, 4608) loss= 70432.0234375\n",
      "range:(8960, 9088) loss= 45871.1835938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 39345.4882812\n",
      "range:(4480, 4608) loss= 30599.0039062\n",
      "range:(8960, 9088) loss= 14814.7666016\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 13273.5859375\n",
      "range:(4480, 4608) loss= 11079.5097656\n",
      "range:(8960, 9088) loss= 39075.8867188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 61992.6875\n",
      "range:(4480, 4608) loss= 91410.4609375\n",
      "range:(8960, 9088) loss= 41985.7460938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 8\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 46783.4179688\n",
      "range:(4480, 4608) loss= 37366.1484375\n",
      "range:(8960, 9088) loss= 16341.0078125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 19731.5429688\n",
      "range:(4480, 4608) loss= 19386.7070312\n",
      "range:(8960, 9088) loss= 38637.9648438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 41895.1679688\n",
      "range:(4480, 4608) loss= 34853.5351562\n",
      "range:(8960, 9088) loss= 35539.625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 57057.5429688\n",
      "range:(4480, 4608) loss= 39993.4882812\n",
      "range:(8960, 9088) loss= 15852.5566406\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 17922.2636719\n",
      "range:(4480, 4608) loss= 19641.5390625\n",
      "range:(8960, 9088) loss= 50099.640625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 9\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 44992.5234375\n",
      "range:(4480, 4608) loss= 14395.9921875\n",
      "range:(8960, 9088) loss= 23233.6816406\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 29374.1738281\n",
      "range:(4480, 4608) loss= 37552.6601562\n",
      "range:(8960, 9088) loss= 14610.6318359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 13274.0498047\n",
      "range:(4480, 4608) loss= 12169.1220703\n",
      "range:(8960, 9088) loss= 23554.734375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 28260.2460938\n",
      "range:(4480, 4608) loss= 19895.171875\n",
      "range:(8960, 9088) loss= 16022.7763672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 24521.4960938\n",
      "range:(4480, 4608) loss= 24311.2226562\n",
      "range:(8960, 9088) loss= 22375.9667969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 10\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 17006.9414062\n",
      "range:(4480, 4608) loss= 11106.484375\n",
      "range:(8960, 9088) loss= 9918.01171875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 14256.4267578\n",
      "range:(4480, 4608) loss= 16879.1308594\n",
      "range:(8960, 9088) loss= 25410.9355469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 20013.5117188\n",
      "range:(4480, 4608) loss= 24689.8417969\n",
      "range:(8960, 9088) loss= 18852.8964844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 16274.7841797\n",
      "range:(4480, 4608) loss= 56549.0585938\n",
      "range:(8960, 9088) loss= 57688.4882812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 52044.3085938\n",
      "range:(4480, 4608) loss= 24941.2578125\n",
      "range:(8960, 9088) loss= 9826.61328125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 11\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 10697.1894531\n",
      "range:(4480, 4608) loss= 7120.18457031\n",
      "range:(8960, 9088) loss= 6875.81005859\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 9072.70507812\n",
      "range:(4480, 4608) loss= 17576.671875\n",
      "range:(8960, 9088) loss= 11765.1152344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 12946.8515625\n",
      "range:(4480, 4608) loss= 8821.359375\n",
      "range:(8960, 9088) loss= 5727.66894531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 5332.3828125\n",
      "range:(4480, 4608) loss= 10599.6972656\n",
      "range:(8960, 9088) loss= 26004.0722656\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 48135.5429688\n",
      "range:(4480, 4608) loss= 28043.1074219\n",
      "range:(8960, 9088) loss= 9487.97949219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 12\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 13818.7695312\n",
      "range:(4480, 4608) loss= 7712.42529297\n",
      "range:(8960, 9088) loss= 13184.9365234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 10143.5087891\n",
      "range:(4480, 4608) loss= 6160.97802734\n",
      "range:(8960, 9088) loss= 2842.8972168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3007.45825195\n",
      "range:(4480, 4608) loss= 5058.85693359\n",
      "range:(8960, 9088) loss= 15408.7558594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 18159.265625\n",
      "range:(4480, 4608) loss= 13484.5888672\n",
      "range:(8960, 9088) loss= 5891.0625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4819.96533203\n",
      "range:(4480, 4608) loss= 9002.52246094\n",
      "range:(8960, 9088) loss= 33339.125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 13\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 75315.5234375\n",
      "range:(4480, 4608) loss= 21126.921875\n",
      "range:(8960, 9088) loss= 13945.6054688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 22617.8613281\n",
      "range:(4480, 4608) loss= 7632.67041016\n",
      "range:(8960, 9088) loss= 3926.05053711\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 5140.80761719\n",
      "range:(4480, 4608) loss= 3089.5402832\n",
      "range:(8960, 9088) loss= 2684.55664062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3190.51977539\n",
      "range:(4480, 4608) loss= 3781.72509766\n",
      "range:(8960, 9088) loss= 10470.2763672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 16109.7138672\n",
      "range:(4480, 4608) loss= 15835.59375\n",
      "range:(8960, 9088) loss= 8262.1171875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 14\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 5153.44140625\n",
      "range:(4480, 4608) loss= 5131.14404297\n",
      "range:(8960, 9088) loss= 3978.93920898\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 4146.02832031\n",
      "range:(4480, 4608) loss= 3580.61376953\n",
      "range:(8960, 9088) loss= 2381.92163086\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2289.75805664\n",
      "range:(4480, 4608) loss= 2162.88720703\n",
      "range:(8960, 9088) loss= 124557.484375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 421929.03125\n",
      "range:(4480, 4608) loss= 50099.2226562\n",
      "range:(8960, 9088) loss= 20187.4199219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 19076.7832031\n",
      "range:(4480, 4608) loss= 13124.2119141\n",
      "range:(8960, 9088) loss= 10837.5341797\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 15\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 9917.71582031\n",
      "range:(4480, 4608) loss= 8678.96191406\n",
      "range:(8960, 9088) loss= 7617.17480469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 7970.81152344\n",
      "range:(4480, 4608) loss= 8811.734375\n",
      "range:(8960, 9088) loss= 6211.91455078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 6996.58349609\n",
      "range:(4480, 4608) loss= 6317.09033203\n",
      "range:(8960, 9088) loss= 4684.58300781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 5533.42138672\n",
      "range:(4480, 4608) loss= 4779.97949219\n",
      "range:(8960, 9088) loss= 4148.74707031\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 5363.15673828\n",
      "range:(4480, 4608) loss= 3493.13623047\n",
      "range:(8960, 9088) loss= 2962.52490234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 16\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3725.80737305\n",
      "range:(4480, 4608) loss= 4023.85546875\n",
      "range:(8960, 9088) loss= 3257.29785156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3603.11547852\n",
      "range:(4480, 4608) loss= 2585.4362793\n",
      "range:(8960, 9088) loss= 2685.77246094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3399.99902344\n",
      "range:(4480, 4608) loss= 3504.84008789\n",
      "range:(8960, 9088) loss= 2302.39892578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3019.11376953\n",
      "range:(4480, 4608) loss= 1940.44299316\n",
      "range:(8960, 9088) loss= 1791.95324707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2250.93798828\n",
      "range:(4480, 4608) loss= 3060.16430664\n",
      "range:(8960, 9088) loss= 1911.77355957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 17\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2001.22473145\n",
      "range:(4480, 4608) loss= 2140.65356445\n",
      "range:(8960, 9088) loss= 2066.72387695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 2766.54003906\n",
      "range:(4480, 4608) loss= 1334.36779785\n",
      "range:(8960, 9088) loss= 1611.0880127\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2052.98779297\n",
      "range:(4480, 4608) loss= 1774.11608887\n",
      "range:(8960, 9088) loss= 1238.53857422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1584.58422852\n",
      "range:(4480, 4608) loss= 1975.26184082\n",
      "range:(8960, 9088) loss= 1373.12683105\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1502.15771484\n",
      "range:(4480, 4608) loss= 1401.69006348\n",
      "range:(8960, 9088) loss= 1478.50732422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 18\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1413.40100098\n",
      "range:(4480, 4608) loss= 1325.01635742\n",
      "range:(8960, 9088) loss= 1092.15319824\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 917.609680176\n",
      "range:(4480, 4608) loss= 1280.44909668\n",
      "range:(8960, 9088) loss= 751.220397949\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 828.953125\n",
      "range:(4480, 4608) loss= 698.277282715\n",
      "range:(8960, 9088) loss= 1718.1496582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1427.0123291\n",
      "range:(4480, 4608) loss= 1622.22045898\n",
      "range:(8960, 9088) loss= 1721.21374512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 10200.9003906\n",
      "range:(4480, 4608) loss= 19717.6777344\n",
      "range:(8960, 9088) loss= 5973.70654297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 19\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 5307.67089844\n",
      "range:(4480, 4608) loss= 3039.60522461\n",
      "range:(8960, 9088) loss= 2206.21313477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3186.54272461\n",
      "range:(4480, 4608) loss= 2099.45385742\n",
      "range:(8960, 9088) loss= 1959.82861328\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1720.13183594\n",
      "range:(4480, 4608) loss= 1652.75256348\n",
      "range:(8960, 9088) loss= 1000.45343018\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1140.1105957\n",
      "range:(4480, 4608) loss= 1052.50854492\n",
      "range:(8960, 9088) loss= 866.18762207\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 1031.2052002\n",
      "range:(4480, 4608) loss= 702.45880127\n",
      "range:(8960, 9088) loss= 709.117126465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 20\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 718.481872559\n",
      "range:(4480, 4608) loss= 662.861206055\n",
      "range:(8960, 9088) loss= 697.427734375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 938.337646484\n",
      "range:(4480, 4608) loss= 476.832489014\n",
      "range:(8960, 9088) loss= 441.454986572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 647.574523926\n",
      "range:(4480, 4608) loss= 473.841247559\n",
      "range:(8960, 9088) loss= 382.968048096\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 420.488342285\n",
      "range:(4480, 4608) loss= 766.278442383\n",
      "range:(8960, 9088) loss= 750.99798584\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 906.19909668\n",
      "range:(4480, 4608) loss= 1950.20544434\n",
      "range:(8960, 9088) loss= 735.494506836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 21\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 661.565063477\n",
      "range:(4480, 4608) loss= 446.810180664\n",
      "range:(8960, 9088) loss= 263.46697998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 506.956085205\n",
      "range:(4480, 4608) loss= 25885.2246094\n",
      "range:(8960, 9088) loss= 5873.453125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 4479.49853516\n",
      "range:(4480, 4608) loss= 2096.89379883\n",
      "range:(8960, 9088) loss= 1830.99157715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2086.9296875\n",
      "range:(4480, 4608) loss= 1380.22131348\n",
      "range:(8960, 9088) loss= 1075.87695312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 961.766967773\n",
      "range:(4480, 4608) loss= 863.015808105\n",
      "range:(8960, 9088) loss= 685.804931641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 22\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 880.559692383\n",
      "range:(4480, 4608) loss= 568.964477539\n",
      "range:(8960, 9088) loss= 464.154846191\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 669.512390137\n",
      "range:(4480, 4608) loss= 364.879577637\n",
      "range:(8960, 9088) loss= 293.672546387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 542.395446777\n",
      "range:(4480, 4608) loss= 586.97076416\n",
      "range:(8960, 9088) loss= 303.326080322\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 318.894683838\n",
      "range:(4480, 4608) loss= 321.839782715\n",
      "range:(8960, 9088) loss= 474.818115234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 554.332397461\n",
      "range:(4480, 4608) loss= 365.644927979\n",
      "range:(8960, 9088) loss= 384.082672119\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 23\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 377.345001221\n",
      "range:(4480, 4608) loss= 284.181732178\n",
      "range:(8960, 9088) loss= 312.01159668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 299.484161377\n",
      "range:(4480, 4608) loss= 345.803131104\n",
      "range:(8960, 9088) loss= 170.306945801\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 209.180343628\n",
      "range:(4480, 4608) loss= 227.105743408\n",
      "range:(8960, 9088) loss= 169.647659302\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 276.999694824\n",
      "range:(4480, 4608) loss= 262.385314941\n",
      "range:(8960, 9088) loss= 626.859802246\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1566.97131348\n",
      "range:(4480, 4608) loss= 282.680297852\n",
      "range:(8960, 9088) loss= 145.324554443\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 24\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 157.872161865\n",
      "range:(4480, 4608) loss= 209.253646851\n",
      "range:(8960, 9088) loss= 207.592758179\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 303.890472412\n",
      "range:(4480, 4608) loss= 139.552429199\n",
      "range:(8960, 9088) loss= 262.684570312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 413.843200684\n",
      "range:(4480, 4608) loss= 3602023.25\n",
      "range:(8960, 9088) loss= 1097776.75\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 568773.375\n",
      "range:(4480, 4608) loss= 69199.4453125\n",
      "range:(8960, 9088) loss= 41043.0703125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 38768.6054688\n",
      "range:(4480, 4608) loss= 32838.4296875\n",
      "range:(8960, 9088) loss= 26230.7363281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 25\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 26029.3339844\n",
      "range:(4480, 4608) loss= 19825.4082031\n",
      "range:(8960, 9088) loss= 18423.2265625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 20956.4199219\n",
      "range:(4480, 4608) loss= 17005.4335938\n",
      "range:(8960, 9088) loss= 14246.5800781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 12727.8222656\n",
      "range:(4480, 4608) loss= 14134.5224609\n",
      "range:(8960, 9088) loss= 10430.0888672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 11740.1298828\n",
      "range:(4480, 4608) loss= 9781.4921875\n",
      "range:(8960, 9088) loss= 9902.60449219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 8750.47363281\n",
      "range:(4480, 4608) loss= 8184.87451172\n",
      "range:(8960, 9088) loss= 7172.72753906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 26\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 7796.98193359\n",
      "range:(4480, 4608) loss= 7407.18847656\n",
      "range:(8960, 9088) loss= 6184.32373047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 7014.57910156\n",
      "range:(4480, 4608) loss= 6036.45654297\n",
      "range:(8960, 9088) loss= 4750.75244141\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 6117.52490234\n",
      "range:(4480, 4608) loss= 5382.36523438\n",
      "range:(8960, 9088) loss= 4471.76416016\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 6425.85791016\n",
      "range:(4480, 4608) loss= 4465.45751953\n",
      "range:(8960, 9088) loss= 3814.51855469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 5042.17529297\n",
      "range:(4480, 4608) loss= 4428.55712891\n",
      "range:(8960, 9088) loss= 3558.74853516\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 27\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 5735.53271484\n",
      "range:(4480, 4608) loss= 3143.05859375\n",
      "range:(8960, 9088) loss= 3657.3605957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 5829.9296875\n",
      "range:(4480, 4608) loss= 3356.35424805\n",
      "range:(8960, 9088) loss= 4894.51025391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 5145.31445312\n",
      "range:(4480, 4608) loss= 14757.1503906\n",
      "range:(8960, 9088) loss= 4676.93408203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4081.82373047\n",
      "range:(4480, 4608) loss= 4583.0390625\n",
      "range:(8960, 9088) loss= 3581.0378418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 4531.35205078\n",
      "range:(4480, 4608) loss= 4572.40039062\n",
      "range:(8960, 9088) loss= 5752.80712891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 28\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 5147.49414062\n",
      "range:(4480, 4608) loss= 2689.16552734\n",
      "range:(8960, 9088) loss= 2407.36938477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3591.50415039\n",
      "range:(4480, 4608) loss= 4321.15820312\n",
      "range:(8960, 9088) loss= 2434.4987793\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 4944.27978516\n",
      "range:(4480, 4608) loss= 3711.84057617\n",
      "range:(8960, 9088) loss= 3863.66992188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 5230.52050781\n",
      "range:(4480, 4608) loss= 2263.89306641\n",
      "range:(8960, 9088) loss= 11621.9550781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 3486.60595703\n",
      "range:(4480, 4608) loss= 3863.93188477\n",
      "range:(8960, 9088) loss= 4309.46337891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 29\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 5088.37988281\n",
      "range:(4480, 4608) loss= 4404.44482422\n",
      "range:(8960, 9088) loss= 2639.82983398\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3179.45849609\n",
      "range:(4480, 4608) loss= 4837.58251953\n",
      "range:(8960, 9088) loss= 4090.8371582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3035.3059082\n",
      "range:(4480, 4608) loss= 2610.34472656\n",
      "range:(8960, 9088) loss= 3018.68408203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3700.67602539\n",
      "range:(4480, 4608) loss= 2362.27270508\n",
      "range:(8960, 9088) loss= 2529.15673828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1857.76574707\n",
      "range:(4480, 4608) loss= 2676.51586914\n",
      "range:(8960, 9088) loss= 3965.33984375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 30\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2989.02392578\n",
      "range:(4480, 4608) loss= 2467.61669922\n",
      "range:(8960, 9088) loss= 3525.49365234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 4498.22949219\n",
      "range:(4480, 4608) loss= 2230.20581055\n",
      "range:(8960, 9088) loss= 1674.2947998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3569.84423828\n",
      "range:(4480, 4608) loss= 1967.82788086\n",
      "range:(8960, 9088) loss= 4810.30322266\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2317.50708008\n",
      "range:(4480, 4608) loss= 4297.98828125\n",
      "range:(8960, 9088) loss= 1974.60095215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2022.76196289\n",
      "range:(4480, 4608) loss= 2465.11523438\n",
      "range:(8960, 9088) loss= 1470.72021484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 31\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3402.30493164\n",
      "range:(4480, 4608) loss= 3253.72875977\n",
      "range:(8960, 9088) loss= 1904.52331543\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1521.61010742\n",
      "range:(4480, 4608) loss= 1747.76611328\n",
      "range:(8960, 9088) loss= 1928.41748047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2968.49023438\n",
      "range:(4480, 4608) loss= 1988.80493164\n",
      "range:(8960, 9088) loss= 5222.35302734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3517.50341797\n",
      "range:(4480, 4608) loss= 1845.31298828\n",
      "range:(8960, 9088) loss= 2206.89624023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1662.70703125\n",
      "range:(4480, 4608) loss= 2028.73583984\n",
      "range:(8960, 9088) loss= 1933.84790039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 32\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2802.12280273\n",
      "range:(4480, 4608) loss= 1371.42663574\n",
      "range:(8960, 9088) loss= 11120.9814453\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 6939.25097656\n",
      "range:(4480, 4608) loss= 1966.9206543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 1169.30175781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1097.78210449\n",
      "range:(4480, 4608) loss= 1705.57946777\n",
      "range:(8960, 9088) loss= 813.482421875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1286.97583008\n",
      "range:(4480, 4608) loss= 1357.11157227\n",
      "range:(8960, 9088) loss= 1115.38330078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1324.30993652\n",
      "range:(4480, 4608) loss= 1123.4510498\n",
      "range:(8960, 9088) loss= 2434.94311523\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 33\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1859.1628418\n",
      "range:(4480, 4608) loss= 2427.97802734\n",
      "range:(8960, 9088) loss= 1091.49719238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1154.39758301\n",
      "range:(4480, 4608) loss= 1330.12817383\n",
      "range:(8960, 9088) loss= 1007.72619629\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1552.66186523\n",
      "range:(4480, 4608) loss= 1863.56933594\n",
      "range:(8960, 9088) loss= 926.629882812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 829.303100586\n",
      "range:(4480, 4608) loss= 1816.48083496\n",
      "range:(8960, 9088) loss= 2909.88598633\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1306.64611816\n",
      "range:(4480, 4608) loss= 1624.81567383\n",
      "range:(8960, 9088) loss= 970.339050293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 34\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1819.05407715\n",
      "range:(4480, 4608) loss= 694.81628418\n",
      "range:(8960, 9088) loss= 745.526000977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 682.453430176\n",
      "range:(4480, 4608) loss= 583.796081543\n",
      "range:(8960, 9088) loss= 884.467285156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1048.95861816\n",
      "range:(4480, 4608) loss= 1371.70092773\n",
      "range:(8960, 9088) loss= 1493.19958496\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 866.702026367\n",
      "range:(4480, 4608) loss= 2250.04223633\n",
      "range:(8960, 9088) loss= 969.786804199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 745.580566406\n",
      "range:(4480, 4608) loss= 714.065246582\n",
      "range:(8960, 9088) loss= 518.723266602\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 35\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 476.437316895\n",
      "range:(4480, 4608) loss= 351.497772217\n",
      "range:(8960, 9088) loss= 855.708068848\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1235.81164551\n",
      "range:(4480, 4608) loss= 836.390075684\n",
      "range:(8960, 9088) loss= 1413.27368164\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1390.71411133\n",
      "range:(4480, 4608) loss= 1092.53027344\n",
      "range:(8960, 9088) loss= 1616.1583252\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3563.66479492\n",
      "range:(4480, 4608) loss= 3671.14306641\n",
      "range:(8960, 9088) loss= 1237.37548828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1042.91882324\n",
      "range:(4480, 4608) loss= 443.572052002\n",
      "range:(8960, 9088) loss= 426.243255615\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 36\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 554.502075195\n",
      "range:(4480, 4608) loss= 371.736175537\n",
      "range:(8960, 9088) loss= 361.098907471\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 288.751678467\n",
      "range:(4480, 4608) loss= 330.191650391\n",
      "range:(8960, 9088) loss= 489.646911621\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 404.555603027\n",
      "range:(4480, 4608) loss= 489.925537109\n",
      "range:(8960, 9088) loss= 387.016113281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 641.98614502\n",
      "range:(4480, 4608) loss= 290.162414551\n",
      "range:(8960, 9088) loss= 367.431335449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 408.73260498\n",
      "range:(4480, 4608) loss= 1472.40478516\n",
      "range:(8960, 9088) loss= 1751.23266602\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 37\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1622.98156738\n",
      "range:(4480, 4608) loss= 497.009368896\n",
      "range:(8960, 9088) loss= 504.674346924\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 810.809814453\n",
      "range:(4480, 4608) loss= 888.592590332\n",
      "range:(8960, 9088) loss= 396.398803711\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 805.491577148\n",
      "range:(4480, 4608) loss= 366.124023438\n",
      "range:(8960, 9088) loss= 542.088867188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 407.619812012\n",
      "range:(4480, 4608) loss= 395.387908936\n",
      "range:(8960, 9088) loss= 640.395751953\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 594.828857422\n",
      "range:(4480, 4608) loss= 349.834259033\n",
      "range:(8960, 9088) loss= 286.656280518\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 38\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 407.104248047\n",
      "range:(4480, 4608) loss= 353.431365967\n",
      "range:(8960, 9088) loss= 514.452758789\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 344.329467773\n",
      "range:(4480, 4608) loss= 401.605743408\n",
      "range:(8960, 9088) loss= 314.280487061\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 299.744110107\n",
      "range:(4480, 4608) loss= 415.865844727\n",
      "range:(8960, 9088) loss= 375.671447754\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 248.430862427\n",
      "range:(4480, 4608) loss= 233.466079712\n",
      "range:(8960, 9088) loss= 256.119232178\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 302.947235107\n",
      "range:(4480, 4608) loss= 361.649902344\n",
      "range:(8960, 9088) loss= 400.508728027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 39\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1040.93884277\n",
      "range:(4480, 4608) loss= 500.289733887\n",
      "range:(8960, 9088) loss= 261.522583008\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 272.744842529\n",
      "range:(4480, 4608) loss= 209.471694946\n",
      "range:(8960, 9088) loss= 847.362854004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 639.206787109\n",
      "range:(4480, 4608) loss= 361.056060791\n",
      "range:(8960, 9088) loss= 248.189117432\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 199.36807251\n",
      "range:(4480, 4608) loss= 228.414962769\n",
      "range:(8960, 9088) loss= 467.555358887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 287.274719238\n",
      "range:(4480, 4608) loss= 172.48500061\n",
      "range:(8960, 9088) loss= 156.050674438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 40\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 401.962402344\n",
      "range:(4480, 4608) loss= 199.460678101\n",
      "range:(8960, 9088) loss= 163.202682495\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 183.26159668\n",
      "range:(4480, 4608) loss= 5627.67138672\n",
      "range:(8960, 9088) loss= 3313.10327148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1937.14355469\n",
      "range:(4480, 4608) loss= 720.870910645\n",
      "range:(8960, 9088) loss= 264.726898193\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 235.458389282\n",
      "range:(4480, 4608) loss= 250.820465088\n",
      "range:(8960, 9088) loss= 224.533828735\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 420.43762207\n",
      "range:(4480, 4608) loss= 168.196166992\n",
      "range:(8960, 9088) loss= 175.78666687\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 41\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 189.04133606\n",
      "range:(4480, 4608) loss= 139.822158813\n",
      "range:(8960, 9088) loss= 144.543319702\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 155.338668823\n",
      "range:(4480, 4608) loss= 142.113876343\n",
      "range:(8960, 9088) loss= 135.418609619\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 122.286094666\n",
      "range:(4480, 4608) loss= 171.555938721\n",
      "range:(8960, 9088) loss= 125.729972839\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 183.281234741\n",
      "range:(4480, 4608) loss= 128.8646698\n",
      "range:(8960, 9088) loss= 126.975112915\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 209.58430481\n",
      "range:(4480, 4608) loss= 115.286125183\n",
      "range:(8960, 9088) loss= 127.387336731\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 42\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 156.419418335\n",
      "range:(4480, 4608) loss= 136.986297607\n",
      "range:(8960, 9088) loss= 162.648880005\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 257.640014648\n",
      "range:(4480, 4608) loss= 139.831893921\n",
      "range:(8960, 9088) loss= 108.723373413\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 129.622390747\n",
      "range:(4480, 4608) loss= 119.328773499\n",
      "range:(8960, 9088) loss= 103.633056641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 109.133056641\n",
      "range:(4480, 4608) loss= 110.800132751\n",
      "range:(8960, 9088) loss= 102.268379211\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 170.589553833\n",
      "range:(4480, 4608) loss= 113.450057983\n",
      "range:(8960, 9088) loss= 118.704612732\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 43\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 199.837356567\n",
      "range:(4480, 4608) loss= 103.786491394\n",
      "range:(8960, 9088) loss= 104.742118835\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 99.1461257935\n",
      "range:(4480, 4608) loss= 98.1222229004\n",
      "range:(8960, 9088) loss= 99.4269104004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 103.71307373\n",
      "range:(4480, 4608) loss= 99.4546356201\n",
      "range:(8960, 9088) loss= 98.013053894\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 116.733901978\n",
      "range:(4480, 4608) loss= 95.8839187622\n",
      "range:(8960, 9088) loss= 93.7615203857\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 96.3879699707\n",
      "range:(4480, 4608) loss= 89.3630447388\n",
      "range:(8960, 9088) loss= 89.5476989746\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 44\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 118.761627197\n",
      "range:(4480, 4608) loss= 374.045562744\n",
      "range:(8960, 9088) loss= 10045.4511719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 4201.69970703\n",
      "range:(4480, 4608) loss= 1147.21459961\n",
      "range:(8960, 9088) loss= 636.887084961\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 610.036437988\n",
      "range:(4480, 4608) loss= 429.541168213\n",
      "range:(8960, 9088) loss= 346.3565979\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 298.480987549\n",
      "range:(4480, 4608) loss= 262.485778809\n",
      "range:(8960, 9088) loss= 213.37387085\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 218.477615356\n",
      "range:(4480, 4608) loss= 195.840377808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 174.657546997\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 45\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 178.243484497\n",
      "range:(4480, 4608) loss= 157.385025024\n",
      "range:(8960, 9088) loss= 145.90284729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 140.875366211\n",
      "range:(4480, 4608) loss= 129.699325562\n",
      "range:(8960, 9088) loss= 129.307342529\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 120.815742493\n",
      "range:(4480, 4608) loss= 129.296096802\n",
      "range:(8960, 9088) loss= 125.521881104\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 123.136108398\n",
      "range:(4480, 4608) loss= 112.455780029\n",
      "range:(8960, 9088) loss= 106.325737\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 103.414413452\n",
      "range:(4480, 4608) loss= 101.51915741\n",
      "range:(8960, 9088) loss= 96.9001159668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 46\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 109.39390564\n",
      "range:(4480, 4608) loss= 98.3213119507\n",
      "range:(8960, 9088) loss= 100.577697754\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 97.8301010132\n",
      "range:(4480, 4608) loss= 94.7309341431\n",
      "range:(8960, 9088) loss= 95.9284286499\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 92.3166732788\n",
      "range:(4480, 4608) loss= 97.4590148926\n",
      "range:(8960, 9088) loss= 89.378288269\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 93.4983825684\n",
      "range:(4480, 4608) loss= 90.0279159546\n",
      "range:(8960, 9088) loss= 89.2361755371\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 89.3949127197\n",
      "range:(4480, 4608) loss= 89.2689056396\n",
      "range:(8960, 9088) loss= 90.010269165\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 47\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 86.7453079224\n",
      "range:(4480, 4608) loss= 87.7520751953\n",
      "range:(8960, 9088) loss= 86.8269577026\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 87.3794937134\n",
      "range:(4480, 4608) loss= 89.2676315308\n",
      "range:(8960, 9088) loss= 84.9368896484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 84.1313934326\n",
      "range:(4480, 4608) loss= 87.6229629517\n",
      "range:(8960, 9088) loss= 84.6156311035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 86.4461288452\n",
      "range:(4480, 4608) loss= 83.1402893066\n",
      "range:(8960, 9088) loss= 83.3539505005\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 81.9120178223\n",
      "range:(4480, 4608) loss= 82.8012008667\n",
      "range:(8960, 9088) loss= 82.5422744751\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 48\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 83.3252792358\n",
      "range:(4480, 4608) loss= 81.5928497314\n",
      "range:(8960, 9088) loss= 81.4182434082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 84.4819488525\n",
      "range:(4480, 4608) loss= 83.364112854\n",
      "range:(8960, 9088) loss= 83.5458450317\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 79.0785369873\n",
      "range:(4480, 4608) loss= 84.3517913818\n",
      "range:(8960, 9088) loss= 78.5234603882\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 80.5499801636\n",
      "range:(4480, 4608) loss= 80.9300689697\n",
      "range:(8960, 9088) loss= 81.2165679932\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 78.1961898804\n",
      "range:(4480, 4608) loss= 80.363243103\n",
      "range:(8960, 9088) loss= 79.693977356\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 49\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 78.2984390259\n",
      "range:(4480, 4608) loss= 80.064994812\n",
      "range:(8960, 9088) loss= 79.1857376099\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 78.7914886475\n",
      "range:(4480, 4608) loss= 79.5141677856\n",
      "range:(8960, 9088) loss= 79.0254211426\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 76.4531784058\n",
      "range:(4480, 4608) loss= 81.0814361572\n",
      "range:(8960, 9088) loss= 77.552192688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 81.7777175903\n",
      "range:(4480, 4608) loss= 79.4058685303\n",
      "range:(8960, 9088) loss= 78.4279174805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 77.9255905151\n",
      "range:(4480, 4608) loss= 77.4745788574\n",
      "range:(8960, 9088) loss= 77.3347930908\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 50\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 80.5171508789\n",
      "range:(4480, 4608) loss= 77.7656021118\n",
      "range:(8960, 9088) loss= 78.4779968262\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 80.0285873413\n",
      "range:(4480, 4608) loss= 76.3886184692\n",
      "range:(8960, 9088) loss= 77.7510757446\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 75.2889022827\n",
      "range:(4480, 4608) loss= 79.9014511108\n",
      "range:(8960, 9088) loss= 75.9124908447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 78.5678405762\n",
      "range:(4480, 4608) loss= 77.3508987427\n",
      "range:(8960, 9088) loss= 77.0673217773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 76.8633270264\n",
      "range:(4480, 4608) loss= 76.6324691772\n",
      "range:(8960, 9088) loss= 75.2685546875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 51\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 76.2765655518\n",
      "range:(4480, 4608) loss= 76.7048034668\n",
      "range:(8960, 9088) loss= 77.3788146973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 76.1283340454\n",
      "range:(4480, 4608) loss= 77.7701416016\n",
      "range:(8960, 9088) loss= 76.3559494019\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 74.5019226074\n",
      "range:(4480, 4608) loss= 77.3945007324\n",
      "range:(8960, 9088) loss= 74.5099182129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 78.3973388672\n",
      "range:(4480, 4608) loss= 75.4322357178\n",
      "range:(8960, 9088) loss= 76.9816589355\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 74.7520446777\n",
      "range:(4480, 4608) loss= 74.2949752808\n",
      "range:(8960, 9088) loss= 73.3813171387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 52\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 73.8689193726\n",
      "range:(4480, 4608) loss= 76.1331787109\n",
      "range:(8960, 9088) loss= 76.4969940186\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 75.0103759766\n",
      "range:(4480, 4608) loss= 76.2240524292\n",
      "range:(8960, 9088) loss= 75.572769165\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 74.2204360962\n",
      "range:(4480, 4608) loss= 76.0948257446\n",
      "range:(8960, 9088) loss= 72.8387069702\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 76.3955001831\n",
      "range:(4480, 4608) loss= 74.0977325439\n",
      "range:(8960, 9088) loss= 75.1887435913\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 74.2471542358\n",
      "range:(4480, 4608) loss= 73.4287490845\n",
      "range:(8960, 9088) loss= 72.2194137573\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 53\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 72.4702301025\n",
      "range:(4480, 4608) loss= 74.6229019165\n",
      "range:(8960, 9088) loss= 74.5298080444\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 75.9250183105\n",
      "range:(4480, 4608) loss= 74.2838363647\n",
      "range:(8960, 9088) loss= 74.2644729614\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 72.1567306519\n",
      "range:(4480, 4608) loss= 75.497833252\n",
      "range:(8960, 9088) loss= 71.7740936279\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 76.3594436646\n",
      "range:(4480, 4608) loss= 72.176902771\n",
      "range:(8960, 9088) loss= 74.2038116455\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 72.2175445557\n",
      "range:(4480, 4608) loss= 72.0065994263\n",
      "range:(8960, 9088) loss= 70.7368392944\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 54\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 70.3053436279\n",
      "range:(4480, 4608) loss= 72.2111129761\n",
      "range:(8960, 9088) loss= 72.0274581909\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 71.1009216309\n",
      "range:(4480, 4608) loss= 72.869354248\n",
      "range:(8960, 9088) loss= 71.9597702026\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 70.4758453369\n",
      "range:(4480, 4608) loss= 72.9216690063\n",
      "range:(8960, 9088) loss= 70.1880645752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 74.1565246582\n",
      "range:(4480, 4608) loss= 71.3110656738\n",
      "range:(8960, 9088) loss= 72.6187896729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 71.4422912598\n",
      "range:(4480, 4608) loss= 70.2689285278\n",
      "range:(8960, 9088) loss= 69.5778274536\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 55\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 69.4125823975\n",
      "range:(4480, 4608) loss= 70.1577987671\n",
      "range:(8960, 9088) loss= 69.7351837158\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 69.1399078369\n",
      "range:(4480, 4608) loss= 69.632598877\n",
      "range:(8960, 9088) loss= 70.3429412842\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 67.2148895264\n",
      "range:(4480, 4608) loss= 70.4460220337\n",
      "range:(8960, 9088) loss= 68.9992904663\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 73.0862579346\n",
      "range:(4480, 4608) loss= 69.2707366943\n",
      "range:(8960, 9088) loss= 69.9492645264\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 69.4244384766\n",
      "range:(4480, 4608) loss= 68.3044586182\n",
      "range:(8960, 9088) loss= 67.2815551758\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 56\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 67.2058944702\n",
      "range:(4480, 4608) loss= 68.6483840942\n",
      "range:(8960, 9088) loss= 67.9287033081\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 67.4608001709\n",
      "range:(4480, 4608) loss= 68.2004547119\n",
      "range:(8960, 9088) loss= 67.6145401001\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 66.5090484619\n",
      "range:(4480, 4608) loss= 69.0485534668\n",
      "range:(8960, 9088) loss= 66.4887466431\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 69.7332000732\n",
      "range:(4480, 4608) loss= 66.8800582886\n",
      "range:(8960, 9088) loss= 68.5551223755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 66.5085525513\n",
      "range:(4480, 4608) loss= 66.6117630005\n",
      "range:(8960, 9088) loss= 65.5344238281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 57\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 65.6123809814\n",
      "range:(4480, 4608) loss= 66.3073425293\n",
      "range:(8960, 9088) loss= 66.3783950806\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 65.6912155151\n",
      "range:(4480, 4608) loss= 66.7949752808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 66.8141479492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 66.424697876\n",
      "range:(4480, 4608) loss= 66.9480361938\n",
      "range:(8960, 9088) loss= 63.6827926636\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 69.7610549927\n",
      "range:(4480, 4608) loss= 65.5720596313\n",
      "range:(8960, 9088) loss= 65.4669952393\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 65.6282272339\n",
      "range:(4480, 4608) loss= 64.9989700317\n",
      "range:(8960, 9088) loss= 62.8639907837\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 58\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 62.0840530396\n",
      "range:(4480, 4608) loss= 65.1510543823\n",
      "range:(8960, 9088) loss= 64.3503723145\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 63.9719696045\n",
      "range:(4480, 4608) loss= 64.2864761353\n",
      "range:(8960, 9088) loss= 64.5102996826\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 62.0695266724\n",
      "range:(4480, 4608) loss= 64.5199813843\n",
      "range:(8960, 9088) loss= 62.1702003479\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 66.1758422852\n",
      "range:(4480, 4608) loss= 62.6033058167\n",
      "range:(8960, 9088) loss= 63.8646354675\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 62.3799934387\n",
      "range:(4480, 4608) loss= 62.1115188599\n",
      "range:(8960, 9088) loss= 61.5459976196\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 59\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 62.1013832092\n",
      "range:(4480, 4608) loss= 63.619808197\n",
      "range:(8960, 9088) loss= 62.5331268311\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 62.8215103149\n",
      "range:(4480, 4608) loss= 63.7465019226\n",
      "range:(8960, 9088) loss= 63.0276184082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 61.209526062\n",
      "range:(4480, 4608) loss= 63.6353607178\n",
      "range:(8960, 9088) loss= 61.0431632996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 65.925567627\n",
      "range:(4480, 4608) loss= 61.5897979736\n",
      "range:(8960, 9088) loss= 61.5802230835\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 61.9258575439\n",
      "range:(4480, 4608) loss= 61.8500709534\n",
      "range:(8960, 9088) loss= 59.390045166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 60\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 58.7157554626\n",
      "range:(4480, 4608) loss= 62.0016860962\n",
      "range:(8960, 9088) loss= 61.5081481934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 60.396697998\n",
      "range:(4480, 4608) loss= 62.7609634399\n",
      "range:(8960, 9088) loss= 61.2116889954\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 62.797832489\n",
      "range:(4480, 4608) loss= 62.219165802\n",
      "range:(8960, 9088) loss= 60.4595985413\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 63.8783073425\n",
      "range:(4480, 4608) loss= 61.0080680847\n",
      "range:(8960, 9088) loss= 60.5814323425\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 61.2870254517\n",
      "range:(4480, 4608) loss= 60.3985176086\n",
      "range:(8960, 9088) loss= 57.768497467\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 61\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 61.1160316467\n",
      "range:(4480, 4608) loss= 61.8284950256\n",
      "range:(8960, 9088) loss= 60.1757125854\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 59.9684677124\n",
      "range:(4480, 4608) loss= 60.9942741394\n",
      "range:(8960, 9088) loss= 60.2672958374\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 60.1227722168\n",
      "range:(4480, 4608) loss= 60.1689224243\n",
      "range:(8960, 9088) loss= 59.594669342\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 64.1263885498\n",
      "range:(4480, 4608) loss= 23179654.0\n",
      "range:(8960, 9088) loss= 709707.625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 276110.15625\n",
      "range:(4480, 4608) loss= 42770.9296875\n",
      "range:(8960, 9088) loss= 26344.84375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 62\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 25618.1386719\n",
      "range:(4480, 4608) loss= 17540.8398438\n",
      "range:(8960, 9088) loss= 16074.7314453\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 15082.2998047\n",
      "range:(4480, 4608) loss= 12989.25\n",
      "range:(8960, 9088) loss= 11451.2802734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 10286.3066406\n",
      "range:(4480, 4608) loss= 9623.01367188\n",
      "range:(8960, 9088) loss= 9026.70996094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 8976.26757812\n",
      "range:(4480, 4608) loss= 8332.80273438\n",
      "range:(8960, 9088) loss= 7248.35302734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 7572.54296875\n",
      "range:(4480, 4608) loss= 6666.96972656\n",
      "range:(8960, 9088) loss= 6295.07080078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 63\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 6462.72070312\n",
      "range:(4480, 4608) loss= 5692.26269531\n",
      "range:(8960, 9088) loss= 5281.37744141\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 5375.57080078\n",
      "range:(4480, 4608) loss= 4940.82080078\n",
      "range:(8960, 9088) loss= 4330.30371094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 4372.18066406\n",
      "range:(4480, 4608) loss= 4472.04248047\n",
      "range:(8960, 9088) loss= 4224.54833984\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 3854.60327148\n",
      "range:(4480, 4608) loss= 3910.32861328\n",
      "range:(8960, 9088) loss= 3630.73461914\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 3769.92211914\n",
      "range:(4480, 4608) loss= 3233.79736328\n",
      "range:(8960, 9088) loss= 3275.98583984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 64\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3378.92773438\n",
      "range:(4480, 4608) loss= 2725.90185547\n",
      "range:(8960, 9088) loss= 2906.77880859\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3031.21289062\n",
      "range:(4480, 4608) loss= 2771.85864258\n",
      "range:(8960, 9088) loss= 2333.73046875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2200.5925293\n",
      "range:(4480, 4608) loss= 2459.1940918\n",
      "range:(8960, 9088) loss= 2272.65429688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2153.99731445\n",
      "range:(4480, 4608) loss= 2100.29052734\n",
      "range:(8960, 9088) loss= 1958.515625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2190.50683594\n",
      "range:(4480, 4608) loss= 1805.99475098\n",
      "range:(8960, 9088) loss= 1973.75622559\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 65\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1923.85632324\n",
      "range:(4480, 4608) loss= 1774.7199707\n",
      "range:(8960, 9088) loss= 1659.0411377\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1671.80029297\n",
      "range:(4480, 4608) loss= 1550.20373535\n",
      "range:(8960, 9088) loss= 1386.62390137\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1477.29858398\n",
      "range:(4480, 4608) loss= 1514.8347168\n",
      "range:(8960, 9088) loss= 1301.80285645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1490.05444336\n",
      "range:(4480, 4608) loss= 1390.58630371\n",
      "range:(8960, 9088) loss= 1150.65136719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1262.56359863\n",
      "range:(4480, 4608) loss= 1148.57849121\n",
      "range:(8960, 9088) loss= 1083.25756836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 66\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1208.65698242\n",
      "range:(4480, 4608) loss= 1010.52203369\n",
      "range:(8960, 9088) loss= 1106.96655273\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1300.43762207\n",
      "range:(4480, 4608) loss= 946.75769043\n",
      "range:(8960, 9088) loss= 959.28302002\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 852.946105957\n",
      "range:(4480, 4608) loss= 919.980285645\n",
      "range:(8960, 9088) loss= 864.694824219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1012.20855713\n",
      "range:(4480, 4608) loss= 798.068908691\n",
      "range:(8960, 9088) loss= 825.418579102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1056.86279297\n",
      "range:(4480, 4608) loss= 701.50567627\n",
      "range:(8960, 9088) loss= 702.237487793\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 67\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 892.837585449\n",
      "range:(4480, 4608) loss= 631.521057129\n",
      "range:(8960, 9088) loss= 699.461242676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 662.395080566\n",
      "range:(4480, 4608) loss= 648.766540527\n",
      "range:(8960, 9088) loss= 556.093505859\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 579.849060059\n",
      "range:(4480, 4608) loss= 729.654907227\n",
      "range:(8960, 9088) loss= 557.635375977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 720.378723145\n",
      "range:(4480, 4608) loss= 515.123901367\n",
      "range:(8960, 9088) loss= 468.973388672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 585.31829834\n",
      "range:(4480, 4608) loss= 457.746734619\n",
      "range:(8960, 9088) loss= 480.510742188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 68\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 648.763366699\n",
      "range:(4480, 4608) loss= 436.10043335\n",
      "range:(8960, 9088) loss= 442.200744629\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 538.680297852\n",
      "range:(4480, 4608) loss= 491.692352295\n",
      "range:(8960, 9088) loss= 602.762329102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 485.450317383\n",
      "range:(4480, 4608) loss= 504.590118408\n",
      "range:(8960, 9088) loss= 387.342803955\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 392.390380859\n",
      "range:(4480, 4608) loss= 347.049377441\n",
      "range:(8960, 9088) loss= 455.266418457\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 872.58782959\n",
      "range:(4480, 4608) loss= 642.892028809\n",
      "range:(8960, 9088) loss= 409.405181885\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 69\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 427.570343018\n",
      "range:(4480, 4608) loss= 396.016723633\n",
      "range:(8960, 9088) loss= 454.092681885\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 695.281738281\n",
      "range:(4480, 4608) loss= 588.576416016\n",
      "range:(8960, 9088) loss= 340.615875244\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 416.504669189\n",
      "range:(4480, 4608) loss= 679.32220459\n",
      "range:(8960, 9088) loss= 580.417480469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 449.997741699\n",
      "range:(4480, 4608) loss= 352.700592041\n",
      "range:(8960, 9088) loss= 364.563659668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 376.396057129\n",
      "range:(4480, 4608) loss= 433.843170166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 511.705230713\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 70\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 667.661987305\n",
      "range:(4480, 4608) loss= 323.362670898\n",
      "range:(8960, 9088) loss= 435.686584473\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 470.601898193\n",
      "range:(4480, 4608) loss= 292.650482178\n",
      "range:(8960, 9088) loss= 265.019958496\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 639.920471191\n",
      "range:(4480, 4608) loss= 518.744445801\n",
      "range:(8960, 9088) loss= 512.81817627\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 618.831665039\n",
      "range:(4480, 4608) loss= 541.223266602\n",
      "range:(8960, 9088) loss= 510.746124268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 782.291015625\n",
      "range:(4480, 4608) loss= 404.041381836\n",
      "range:(8960, 9088) loss= 599.331970215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 71\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 592.702453613\n",
      "range:(4480, 4608) loss= 378.098419189\n",
      "range:(8960, 9088) loss= 414.91229248\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 593.311950684\n",
      "range:(4480, 4608) loss= 367.98324585\n",
      "range:(8960, 9088) loss= 582.13104248\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 822.906494141\n",
      "range:(4480, 4608) loss= 518.795959473\n",
      "range:(8960, 9088) loss= 702.759338379\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 883.921203613\n",
      "range:(4480, 4608) loss= 707.933410645\n",
      "range:(8960, 9088) loss= 791.402954102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 466.401824951\n",
      "range:(4480, 4608) loss= 369.350341797\n",
      "range:(8960, 9088) loss= 494.881622314\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 72\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 380.257843018\n",
      "range:(4480, 4608) loss= 306.609283447\n",
      "range:(8960, 9088) loss= 548.053039551\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 340.557678223\n",
      "range:(4480, 4608) loss= 656.279663086\n",
      "range:(8960, 9088) loss= 458.452819824\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 620.478942871\n",
      "range:(4480, 4608) loss= 255.245910645\n",
      "range:(8960, 9088) loss= 282.547546387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 341.260986328\n",
      "range:(4480, 4608) loss= 675.645202637\n",
      "range:(8960, 9088) loss= 728.39465332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 432.876342773\n",
      "range:(4480, 4608) loss= 293.504364014\n",
      "range:(8960, 9088) loss= 180.382064819\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 73\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 424.815704346\n",
      "range:(4480, 4608) loss= 257.043823242\n",
      "range:(8960, 9088) loss= 304.497070312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 535.015075684\n",
      "range:(4480, 4608) loss= 400.640197754\n",
      "range:(8960, 9088) loss= 293.093292236\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 204.652404785\n",
      "range:(4480, 4608) loss= 450.64276123\n",
      "range:(8960, 9088) loss= 332.447937012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 218.774688721\n",
      "range:(4480, 4608) loss= 138.221282959\n",
      "range:(8960, 9088) loss= 335.610351562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 553.088012695\n",
      "range:(4480, 4608) loss= 215.869033813\n",
      "range:(8960, 9088) loss= 223.735946655\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 74\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 357.171081543\n",
      "range:(4480, 4608) loss= 244.223327637\n",
      "range:(8960, 9088) loss= 219.869308472\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 448.472747803\n",
      "range:(4480, 4608) loss= 240.442550659\n",
      "range:(8960, 9088) loss= 510.005310059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 433.350341797\n",
      "range:(4480, 4608) loss= 860.626647949\n",
      "range:(8960, 9088) loss= 173.406341553\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 292.339233398\n",
      "range:(4480, 4608) loss= 283.101379395\n",
      "range:(8960, 9088) loss= 211.386886597\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 164.690719604\n",
      "range:(4480, 4608) loss= 188.675216675\n",
      "range:(8960, 9088) loss= 827.350585938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 75\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 411.575408936\n",
      "range:(4480, 4608) loss= 217.545303345\n",
      "range:(8960, 9088) loss= 126.118743896\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 663.856079102\n",
      "range:(4480, 4608) loss= 188.44203186\n",
      "range:(8960, 9088) loss= 147.904541016\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 140.349777222\n",
      "range:(4480, 4608) loss= 152.096008301\n",
      "range:(8960, 9088) loss= 177.162979126\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 157.192443848\n",
      "range:(4480, 4608) loss= 451.904968262\n",
      "range:(8960, 9088) loss= 791.096984863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 590.584777832\n",
      "range:(4480, 4608) loss= 548.515441895\n",
      "range:(8960, 9088) loss= 377.910217285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 76\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 208.391494751\n",
      "range:(4480, 4608) loss= 1714.07971191\n",
      "range:(8960, 9088) loss= 198.933044434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1214.03808594\n",
      "range:(4480, 4608) loss= 1800.2565918\n",
      "range:(8960, 9088) loss= 1034.1295166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 332.608276367\n",
      "range:(4480, 4608) loss= 670.138916016\n",
      "range:(8960, 9088) loss= 540.955871582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 594.469116211\n",
      "range:(4480, 4608) loss= 571.027099609\n",
      "range:(8960, 9088) loss= 136.586364746\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 115.644775391\n",
      "range:(4480, 4608) loss= 107.406768799\n",
      "range:(8960, 9088) loss= 537.043151855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 77\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 553.101379395\n",
      "range:(4480, 4608) loss= 120.2240448\n",
      "range:(8960, 9088) loss= 211.669464111\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 669.421142578\n",
      "range:(4480, 4608) loss= 791.285827637\n",
      "range:(8960, 9088) loss= 152.591125488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 131.276367188\n",
      "range:(4480, 4608) loss= 383.252380371\n",
      "range:(8960, 9088) loss= 530.131530762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 405.344055176\n",
      "range:(4480, 4608) loss= 256.364807129\n",
      "range:(8960, 9088) loss= 245.226196289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 212.261154175\n",
      "range:(4480, 4608) loss= 111.197296143\n",
      "range:(8960, 9088) loss= 104.412696838\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 78\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 89.5219268799\n",
      "range:(4480, 4608) loss= 296.148071289\n",
      "range:(8960, 9088) loss= 226.350448608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 274.117126465\n",
      "range:(4480, 4608) loss= 145.12727356\n",
      "range:(8960, 9088) loss= 93.5747070312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 124.940055847\n",
      "range:(4480, 4608) loss= 82.5470046997\n",
      "range:(8960, 9088) loss= 699.210327148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 754.799133301\n",
      "range:(4480, 4608) loss= 297.338195801\n",
      "range:(8960, 9088) loss= 94.5735702515\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 145.304458618\n",
      "range:(4480, 4608) loss= 86.5707397461\n",
      "range:(8960, 9088) loss= 497.418670654\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 79\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 299.988128662\n",
      "range:(4480, 4608) loss= 82.3782272339\n",
      "range:(8960, 9088) loss= 80.3928909302\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 80.8552780151\n",
      "range:(4480, 4608) loss= 80.5963821411\n",
      "range:(8960, 9088) loss= 76.1902770996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 70.2030105591\n",
      "range:(4480, 4608) loss= 76.3668441772\n",
      "range:(8960, 9088) loss= 71.1803436279\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 77.3858108521\n",
      "range:(4480, 4608) loss= 75.6684188843\n",
      "range:(8960, 9088) loss= 77.6787185669\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 78.9913864136\n",
      "range:(4480, 4608) loss= 71.4067153931\n",
      "range:(8960, 9088) loss= 73.0173873901\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 80\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 72.162399292\n",
      "range:(4480, 4608) loss= 75.9858398438\n",
      "range:(8960, 9088) loss= 74.8154983521\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 79.9575119019\n",
      "range:(4480, 4608) loss= 73.4923629761\n",
      "range:(8960, 9088) loss= 71.353225708\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 69.5141525269\n",
      "range:(4480, 4608) loss= 92.3136520386\n",
      "range:(8960, 9088) loss= 69.7583465576\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 614.012207031\n",
      "range:(4480, 4608) loss= 336.422943115\n",
      "range:(8960, 9088) loss= 780.15637207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 655.583007812\n",
      "range:(4480, 4608) loss= 134.82649231\n",
      "range:(8960, 9088) loss= 115.471549988\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 81\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 94.781036377\n",
      "range:(4480, 4608) loss= 171.840774536\n",
      "range:(8960, 9088) loss= 86.5307235718\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 85.9647140503\n",
      "range:(4480, 4608) loss= 98.7027893066\n",
      "range:(8960, 9088) loss= 171.911315918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 125.112541199\n",
      "range:(4480, 4608) loss= 127.666229248\n",
      "range:(8960, 9088) loss= 76.7498855591\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 76.0736236572\n",
      "range:(4480, 4608) loss= 68.9119949341\n",
      "range:(8960, 9088) loss= 71.9418106079\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 87.2623443604\n",
      "range:(4480, 4608) loss= 106.666763306\n",
      "range:(8960, 9088) loss= 67.6837310791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 82\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 70.5758895874\n",
      "range:(4480, 4608) loss= 70.7953796387\n",
      "range:(8960, 9088) loss= 71.0889816284\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 72.2865905762\n",
      "range:(4480, 4608) loss= 68.2940139771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 68.9068450928\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 65.8545303345\n",
      "range:(4480, 4608) loss= 82.8492355347\n",
      "range:(8960, 9088) loss= 74.8143234253\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 75.2551803589\n",
      "range:(4480, 4608) loss= 67.9106369019\n",
      "range:(8960, 9088) loss= 68.4531097412\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 65.7589874268\n",
      "range:(4480, 4608) loss= 303.338623047\n",
      "range:(8960, 9088) loss= 568.379089355\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 83\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 533.619812012\n",
      "range:(4480, 4608) loss= 94.7330093384\n",
      "range:(8960, 9088) loss= 73.7793273926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 82.6109313965\n",
      "range:(4480, 4608) loss= 73.5642700195\n",
      "range:(8960, 9088) loss= 70.8366317749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 73.6344833374\n",
      "range:(4480, 4608) loss= 76.2523803711\n",
      "range:(8960, 9088) loss= 68.2004928589\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 79.1420059204\n",
      "range:(4480, 4608) loss= 86.276473999\n",
      "range:(8960, 9088) loss= 105.844917297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 120.211898804\n",
      "range:(4480, 4608) loss= 78.8982849121\n",
      "range:(8960, 9088) loss= 68.5718765259\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 84\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 68.2158966064\n",
      "range:(4480, 4608) loss= 67.9604644775\n",
      "range:(8960, 9088) loss= 66.3422775269\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 68.7909698486\n",
      "range:(4480, 4608) loss= 67.4572525024\n",
      "range:(8960, 9088) loss= 66.9886322021\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 66.1360092163\n",
      "range:(4480, 4608) loss= 68.3915100098\n",
      "range:(8960, 9088) loss= 66.676109314\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 68.1551895142\n",
      "range:(4480, 4608) loss= 66.7783737183\n",
      "range:(8960, 9088) loss= 65.8860855103\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 66.6225738525\n",
      "range:(4480, 4608) loss= 67.0673141479\n",
      "range:(8960, 9088) loss= 64.3519592285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 85\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 74.2117080688\n",
      "range:(4480, 4608) loss= 66.1844406128\n",
      "range:(8960, 9088) loss= 66.9686126709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 69.1213302612\n",
      "range:(4480, 4608) loss= 69.980140686\n",
      "range:(8960, 9088) loss= 89.8157730103\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 92.1417388916\n",
      "range:(4480, 4608) loss= 66.4321365356\n",
      "range:(8960, 9088) loss= 63.1181182861\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 69.0690460205\n",
      "range:(4480, 4608) loss= 65.72631073\n",
      "range:(8960, 9088) loss= 67.5078430176\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 65.5411529541\n",
      "range:(4480, 4608) loss= 75.0484619141\n",
      "range:(8960, 9088) loss= 66.6261978149\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 86\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 65.2364425659\n",
      "range:(4480, 4608) loss= 64.4646987915\n",
      "range:(8960, 9088) loss= 63.2235832214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 63.66015625\n",
      "range:(4480, 4608) loss= 71.0708084106\n",
      "range:(8960, 9088) loss= 65.3697280884\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 70.8650360107\n",
      "range:(4480, 4608) loss= 65.8662719727\n",
      "range:(8960, 9088) loss= 62.4337158203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 65.7123641968\n",
      "range:(4480, 4608) loss= 66.9298477173\n",
      "range:(8960, 9088) loss= 62.5476341248\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 63.9405593872\n",
      "range:(4480, 4608) loss= 63.9471969604\n",
      "range:(8960, 9088) loss= 60.8582229614\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 87\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 61.9692382812\n",
      "range:(4480, 4608) loss= 62.5107421875\n",
      "range:(8960, 9088) loss= 63.4944496155\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 62.6241798401\n",
      "range:(4480, 4608) loss= 63.6306762695\n",
      "range:(8960, 9088) loss= 62.7963905334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 60.3876991272\n",
      "range:(4480, 4608) loss= 64.7349853516\n",
      "range:(8960, 9088) loss= 64.1108703613\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 67.4439544678\n",
      "range:(4480, 4608) loss= 158.881835938\n",
      "range:(8960, 9088) loss= 204.56362915\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 138.541015625\n",
      "range:(4480, 4608) loss= 67.8761749268\n",
      "range:(8960, 9088) loss= 61.5300331116\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 88\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 61.1534004211\n",
      "range:(4480, 4608) loss= 62.0931968689\n",
      "range:(8960, 9088) loss= 61.5072174072\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 61.2661170959\n",
      "range:(4480, 4608) loss= 61.3105278015\n",
      "range:(8960, 9088) loss= 61.4724197388\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 60.4165840149\n",
      "range:(4480, 4608) loss= 61.4843177795\n",
      "range:(8960, 9088) loss= 59.6124992371\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 4\n",
      "range:(0, 128) loss= 62.6474876404\n",
      "range:(4480, 4608) loss= 60.466342926\n",
      "range:(8960, 9088) loss= 59.865814209\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 59.6194915771\n",
      "range:(4480, 4608) loss= 59.9916343689\n",
      "range:(8960, 9088) loss= 58.2922897339\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 89\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 57.445148468\n",
      "range:(4480, 4608) loss= 60.4157562256\n",
      "range:(8960, 9088) loss= 59.6591491699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 59.8488388062\n",
      "range:(4480, 4608) loss= 60.0036468506\n",
      "range:(8960, 9088) loss= 60.3386764526\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 59.40417099\n",
      "range:(4480, 4608) loss= 60.5941085815\n",
      "range:(8960, 9088) loss= 59.2863883972\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 61.6884689331\n",
      "range:(4480, 4608) loss= 59.3380699158\n",
      "range:(8960, 9088) loss= 59.797832489\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 65.7644577026\n",
      "range:(4480, 4608) loss= 60.1593971252\n",
      "range:(8960, 9088) loss= 57.1503219604\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 90\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 56.3889884949\n",
      "range:(4480, 4608) loss= 59.6262397766\n",
      "range:(8960, 9088) loss= 59.089931488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 58.3248443604\n",
      "range:(4480, 4608) loss= 58.5921554565\n",
      "range:(8960, 9088) loss= 58.6704025269\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 57.8086471558\n",
      "range:(4480, 4608) loss= 59.2680587769\n",
      "range:(8960, 9088) loss= 57.6273498535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 60.8368873596\n",
      "range:(4480, 4608) loss= 59.1029396057\n",
      "range:(8960, 9088) loss= 57.7529602051\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 57.8502960205\n",
      "range:(4480, 4608) loss= 58.5898513794\n",
      "range:(8960, 9088) loss= 56.158706665\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 91\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 59.8550376892\n",
      "range:(4480, 4608) loss= 62.7909545898\n",
      "range:(8960, 9088) loss= 60.0454711914\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 57.8423576355\n",
      "range:(4480, 4608) loss= 58.4776382446\n",
      "range:(8960, 9088) loss= 57.9619102478\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 56.9458808899\n",
      "range:(4480, 4608) loss= 58.7630615234\n",
      "range:(8960, 9088) loss= 57.2479324341\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 59.7924156189\n",
      "range:(4480, 4608) loss= 57.6207389832\n",
      "range:(8960, 9088) loss= 56.8553390503\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 56.522151947\n",
      "range:(4480, 4608) loss= 57.9333610535\n",
      "range:(8960, 9088) loss= 55.6303634644\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 92\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 55.8923225403\n",
      "range:(4480, 4608) loss= 58.0508384705\n",
      "range:(8960, 9088) loss= 57.1627349854\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 56.5565757751\n",
      "range:(4480, 4608) loss= 57.6908454895\n",
      "range:(8960, 9088) loss= 57.1957931519\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 56.2557487488\n",
      "range:(4480, 4608) loss= 58.1694602966\n",
      "range:(8960, 9088) loss= 56.436630249\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 59.2694778442\n",
      "range:(4480, 4608) loss= 57.0466575623\n",
      "range:(8960, 9088) loss= 56.452911377\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 56.051109314\n",
      "range:(4480, 4608) loss= 57.2119865417\n",
      "range:(8960, 9088) loss= 54.5754737854\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 93\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 54.4626121521\n",
      "range:(4480, 4608) loss= 57.6261558533\n",
      "range:(8960, 9088) loss= 56.7016792297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 56.1435394287\n",
      "range:(4480, 4608) loss= 58.8121757507\n",
      "range:(8960, 9088) loss= 57.3711509705\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 55.763759613\n",
      "range:(4480, 4608) loss= 80.8762588501\n",
      "range:(8960, 9088) loss= 56.1457176208\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 59.0540313721\n",
      "range:(4480, 4608) loss= 56.4122695923\n",
      "range:(8960, 9088) loss= 55.5769042969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 56.1636161804\n",
      "range:(4480, 4608) loss= 56.8445129395\n",
      "range:(8960, 9088) loss= 53.8236846924\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 94\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 53.7524871826\n",
      "range:(4480, 4608) loss= 57.008769989\n",
      "range:(8960, 9088) loss= 56.2307167053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 55.6874771118\n",
      "range:(4480, 4608) loss= 56.5256843567\n",
      "range:(8960, 9088) loss= 56.7134895325\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 55.4216957092\n",
      "range:(4480, 4608) loss= 58.6449966431\n",
      "range:(8960, 9088) loss= 55.4212799072\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 57.9008712769\n",
      "range:(4480, 4608) loss= 56.0402450562\n",
      "range:(8960, 9088) loss= 55.0119819641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 54.8294868469\n",
      "range:(4480, 4608) loss= 55.9417991638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 53.2340202332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 95\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 53.2181816101\n",
      "range:(4480, 4608) loss= 56.5237159729\n",
      "range:(8960, 9088) loss= 55.6950111389\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 54.8542442322\n",
      "range:(4480, 4608) loss= 55.8484992981\n",
      "range:(8960, 9088) loss= 55.7537460327\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 54.8064575195\n",
      "range:(4480, 4608) loss= 56.5572776794\n",
      "range:(8960, 9088) loss= 54.9894294739\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 57.5483093262\n",
      "range:(4480, 4608) loss= 55.7095031738\n",
      "range:(8960, 9088) loss= 54.5217399597\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 54.4083175659\n",
      "range:(4480, 4608) loss= 55.504486084\n",
      "range:(8960, 9088) loss= 52.7264251709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 96\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 52.9399299622\n",
      "range:(4480, 4608) loss= 56.0314559937\n",
      "range:(8960, 9088) loss= 55.3023681641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 54.7181816101\n",
      "range:(4480, 4608) loss= 55.5462989807\n",
      "range:(8960, 9088) loss= 55.5033340454\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 54.6814575195\n",
      "range:(4480, 4608) loss= 55.6044120789\n",
      "range:(8960, 9088) loss= 54.5802345276\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 57.0422019958\n",
      "range:(4480, 4608) loss= 55.2040977478\n",
      "range:(8960, 9088) loss= 54.1318626404\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 54.0916862488\n",
      "range:(4480, 4608) loss= 55.272983551\n",
      "range:(8960, 9088) loss= 52.479850769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 97\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 52.8097343445\n",
      "range:(4480, 4608) loss= 55.7784385681\n",
      "range:(8960, 9088) loss= 54.8962936401\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 54.0434417725\n",
      "range:(4480, 4608) loss= 55.0995635986\n",
      "range:(8960, 9088) loss= 54.7981834412\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 54.0827445984\n",
      "range:(4480, 4608) loss= 55.2604408264\n",
      "range:(8960, 9088) loss= 54.3519096375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 56.5697250366\n",
      "range:(4480, 4608) loss= 54.8940811157\n",
      "range:(8960, 9088) loss= 53.7344207764\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 53.7016983032\n",
      "range:(4480, 4608) loss= 55.7207107544\n",
      "range:(8960, 9088) loss= 52.2768974304\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 98\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 52.3365364075\n",
      "range:(4480, 4608) loss= 55.5354919434\n",
      "range:(8960, 9088) loss= 54.6204071045\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 53.7574234009\n",
      "range:(4480, 4608) loss= 54.7849121094\n",
      "range:(8960, 9088) loss= 54.5324821472\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 53.9558830261\n",
      "range:(4480, 4608) loss= 55.0125427246\n",
      "range:(8960, 9088) loss= 54.0229377747\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 56.2819786072\n",
      "range:(4480, 4608) loss= 54.6247634888\n",
      "range:(8960, 9088) loss= 53.4228096008\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 53.4781227112\n",
      "range:(4480, 4608) loss= 54.7115974426\n",
      "range:(8960, 9088) loss= 51.9054222107\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 99\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 52.2129592896\n",
      "range:(4480, 4608) loss= 55.3294715881\n",
      "range:(8960, 9088) loss= 54.4579467773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 53.603767395\n",
      "range:(4480, 4608) loss= 54.8337860107\n",
      "range:(8960, 9088) loss= 60.3475990295\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 56.7708435059\n",
      "range:(4480, 4608) loss= 55.0009117126\n",
      "range:(8960, 9088) loss= 53.886264801\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 56.1315536499\n",
      "range:(4480, 4608) loss= 54.5509490967\n",
      "range:(8960, 9088) loss= 53.1573257446\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 53.2507667542\n",
      "range:(4480, 4608) loss= 54.580619812\n",
      "range:(8960, 9088) loss= 51.5718955994\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 100\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 51.891418457\n",
      "range:(4480, 4608) loss= 55.0953979492\n",
      "range:(8960, 9088) loss= 54.2113418579\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 53.3526268005\n",
      "range:(4480, 4608) loss= 54.3175926208\n",
      "range:(8960, 9088) loss= 54.0992469788\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 53.4745254517\n",
      "range:(4480, 4608) loss= 54.3604774475\n",
      "range:(8960, 9088) loss= 53.4422073364\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 55.6848526001\n",
      "range:(4480, 4608) loss= 54.2414131165\n",
      "range:(8960, 9088) loss= 52.7421798706\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 52.9196472168\n",
      "range:(4480, 4608) loss= 54.2498664856\n",
      "range:(8960, 9088) loss= 51.2206268311\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 0\n",
    "    for ep in range(no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(np.ceil(float(len(batch_images)) / min_batch_size))):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                if(index % 35 ==0):\n",
    "                    print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_4\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
