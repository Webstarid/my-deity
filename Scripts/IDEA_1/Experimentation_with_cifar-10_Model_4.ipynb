{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 100 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 5 # all kernels are 5x5\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 128 # we look at 5000 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAEVCAYAAADZzOErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkSz+P9sPFr0n0fu0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7d7ec3590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk1L+P89ha3/RbKLvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8217d01d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7db984650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI3TixwMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7db7660d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypefloat32\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[ 0.72941178  0.70980394  0.68627453]\n",
      " [ 0.69803923  0.63529414  0.65882355]\n",
      " [ 0.68627453  0.63137257  0.65882355]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHSJJREFUeJztnWusnWd15/9rv/tyLj4+duzYOI5LLqQENxCHOZhQ0oqB\ngaaoUkCqEHxA+YDqqirSIHVmFDHSwEjzgVYDCI0qKjOJmo4YLlNApMiUhBQabkpznCZ2Eifkgk3s\n+H49931b82Fvq455/uvsc9vHyfP/SZb3edZ+3nedZ7/r3Wc//73WMneHECI/SqvtgBBidVDwC5Ep\nCn4hMkXBL0SmKPiFyBQFvxCZouAXIlMU/EJkioJfiEwpL2Wymd0J4EsACgD/290/Fz1/dHSdb978\nhqQt+qIh+xbiYr+bGH2rsR3YiqJIjpfM6BxumcePdjuYySkV6ft5yRZ3n28t0g/2u1mwVhEWrORi\nDhlP4a9LqcTXMbyGw6s17U10DbD1PXHiGC5cON/Tiiw6+M2sAPDXAN4P4DCAx8zsAXd/hs3ZvPkN\n+Ov/dV/S1mjwxak3m8nxlvPFaQeLPUeOBwD1RoPa1q4bTY4PVPgylgM/Wk3u/+TkBLVFjIyMJMcH\nBwfpnOgim5qepjYvpW+GANBopNe4EtyEoiu2UgRrHPjB53CbgV8ftYEBagteTjRbLWorEf8np2fo\nnLl62sf/8p93cScuP2/Pz/xNdgJ4wd1fcvc6gK8DuGsJxxNC9JGlBP9WAC9f8vPh7pgQ4jXAim/4\nmdkuMxs3s/Hz58+t9OmEED2ylOA/AmDbJT9f2x17Fe6+293H3H1sdHTdEk4nhFhOlhL8jwG4ycyu\nN7MqgI8CeGB53BJCrDSL3u1396aZfRLAD9CR+u5z96fDOW3H7Nxs0nbuPN/drg7UkuPlMne/VqlS\nW7nE95XL4Fu2g2SeB+rBRLBbPjPDd3ObwTHXreN/QTWIWuGBMjI7O8f9CHapW4sQW61c4cZ2IH02\n6tTmVX5MJnGWCz6nCNSDVpOvhwXzIonz/IXzyfFGg79mBb2+e9c9l6Tzu/seAHuWcgwhxOqgb/gJ\nkSkKfiEyRcEvRKYo+IXIFAW/EJmypN3+heJwtFppCcttETl6QRpVOVA8iiAzqxnITa3ZtEy52NYH\nlQqXmxab1cfmTU5xOS8ShyI5Fc5lL56xGC0Wt1UCOa9W47IuO2Q1OF67yZO7mFQNAKUy9yNKGKsQ\n+bNa5Ws/M8elz17RO78QmaLgFyJTFPxCZIqCX4hMUfALkSn93e13R6tNdohLfKe32UrvbA4MDNM5\nFqgH7Rbfea1GW99khzUq7QRSUw8AWq3F7ehPTk5SG0sgGRoaonOiWoLNYOe7HbxmrNadB4lTHqgH\n5Qpf42ger7kXKEWBwjEX7LJPT01RGwKFaXAw/dpMzwQJVyTxayFdt/XOL0SmKPiFyBQFvxCZouAX\nIlMU/EJkioJfiEzpq9QHOJrNtHxRBOklAyQJoxkkWXggrczNRYkbvHZeidR9G65Fbbe4XAPjElXU\nYWdqkktKQ2TemuB4rO4fANQbQc26IpLLSL3DIHHKSDs0ALBAzmsHdQad1PCrBxLm8CBP+hkeCLoU\n1XlNRivxY86R67jJZHEABekStZB2aHrnFyJTFPxCZIqCX4hMUfALkSkKfiEyRcEvRKYsSeozs4MA\nJgC0ADTdfSx6flGUsH40nYk3MxfUbyun5ZXpyQt0zlydyyRzQRukC7Nc6rNa2o+zZ87QOaOD3I+B\nKs+0CxLmsCbI0GN388nzZ+mcs2dOUNtgIHtt2ryB2trt9DqeOcf9OHL4FWrbvv0WaisHLcBKlrZ5\nkNnZDNqoVarp1nEAsHH9GmqbmOXZgFPEZsZlxTbJSvQFtFBbDp3/37v7qWU4jhCij+jPfiEyZanB\n7wAeNLO9ZrZrORwSQvSHpf7Zf4e7HzGzTQAeMrNn3f2RS5/QvSnsAoBNmzYt8XRCiOViSe/87n6k\n+/8JAN8BsDPxnN3uPubuY6Ojo0s5nRBiGVl08JvZsJmNXHwM4AMAnloux4QQK8tS/uzfDOA73Syi\nMoD/6+7/GE1otVo4ez4tzw0MraXzGk1WOJNLPIGSg2rQ3smNSzKzRL6aq/PMPa/wc1UHuWzUDKTK\nKpE+AaDVTMtUM9NcYvvJT/ZQ281vuZnaSsVvUxuTqfbuHadznj3wPLWNjl5FbVuv2UptJdLyqhK8\nLu02vwbqTS6lDdb49VgUXF5mmZ9N0toOABotIvV570VhFx387v4SgFsXO18IsbpI6hMiUxT8QmSK\ngl+ITFHwC5EpCn4hMqWvBTybzRbOnD2XtG0oc9mrMTedHPcGz75aE8hoJVLUEQCqBZdXarV0v7gp\n50UTowVuzPICpNUKl42adV7As2RpjXNy4iSdU6+fX/DxAODcufRrCQBnz6aPeeCZA3ROO1CpTp/m\nuWMD5HUBgJGR9BfLaoEsV6twKbUa9PGrN7k8G9Snpe/AUdHSail9wN7Ld+qdX4hsUfALkSkKfiEy\nRcEvRKYo+IXIlL7u9luphNpguv7cbJAcUyun71GDg+l6gACwfoS3p4q2lWuz3I851rpqgB9vgG9E\n0yQcAPA232UfGeYHnbyQVgKeffoZOufE0ePUdmj0ELW9dPAxapueSa/jUFB/cPOma6gtmletcWWn\nXk8n6bSDBJhmmysB0/UgYyzaa3eeEFSU0urC2mH+O7NSfUXQpu5y9M4vRKYo+IXIFAW/EJmi4Bci\nUxT8QmSKgl+ITOmr1AczWJE+ZSNIimjOpRNgWPssAJguc9llKKjht3YNlw9nidRXa/J76MAgX+LZ\nGV4rbi6w1ef4Wh0/diw5/uwzz9E5Z0/yxJ4nZ3hN1snZ4DVrpqW0xlpewXnT1W+gtlMneWLS4ACX\ndYeG0i20KlUu51mJv2YTkxN8XiDnVSv8mOUifR2XgtZbBUnsoRpg8vhCiCxR8AuRKQp+ITJFwS9E\npij4hcgUBb8QmTKv1Gdm9wH4IwAn3P2W7thVAL4B4DoABwF8xN15P6gu3nY0ZtJZUd22X0lmptOy\n1wRpnwUA5ye4VDYY1PcbGUlLQwBvGxbeQQP5B22+/AdffIHa9u/bR21nSa27qYl0mzQAqFX5ehTO\nf7uRQZ5d2GymX5vNGzbQOedPcznvl9OT1DZx7jS17dz5G71jAQBDVS7pNtu8tmJBWmsBQK3KJeRA\n6UO1kl7joCsbQLISl7uG398CuPOysXsAPOzuNwF4uPuzEOI1xLzB7+6PADhz2fBdAO7vPr4fwIeW\n2S8hxAqz2M/8m939aPfxMXQ69gohXkMsecPP3R3BdwrNbJeZjZvZ+OQE/2qkEKK/LDb4j5vZFgDo\n/n+CPdHdd7v7mLuPrRkZWeTphBDLzWKD/wEAd3cf3w3gu8vjjhCiX/Qi9X0NwHsAbDSzwwA+A+Bz\nAL5pZp8AcAjAR3o5WbVc4JqN6Xf/VosXVJyqpG2tFpf6rOCiR2F83uw0z3BjaqQHMuX0JNdrnn/+\neWr73gP8fjozxWWvWjX9kg4NcBlqLsjOa7V4wcpmg79mbSJxbr2Gbw+dPcPV4qGgaOnRV35NbefP\n3Zgcv+V3fpvOmSPt4QCgsZZnA1YDqa9FpM+IctAajFEE1/1vHH++J7j7x4jpfT2fRQhxxaFv+AmR\nKQp+ITJFwS9Epij4hcgUBb8QmdLXAp61aoEbtqWzutqB1Ndsrk2OlwJZw4NebG3nskujwbMBmezl\nwTIeOswz1X7w4B5qO3mS989bO8Iz0piPjaDF3Mxsur8fAFQqXGJjch4ATE6lj7l37zids2nzJmpD\niZ9rZpZLc7/85bPJ8Z3vuJXOWbeGy3ntIMtxkPShBAAPFLgW6R1Zb/IXjWXBRjHxG8/t+ZlCiNcV\nCn4hMkXBL0SmKPiFyBQFvxCZouAXIlP6KvWVywU2bkzLdt6OeoylpZBY1uDHm6vzAo0eFAWdJEUk\n63V+rqf2P0FtR47wbLRa0Euu2eRFJFkPt2aL+9hqc0mpWuLFPcvB+g8OpjPcZud4RuKpU9zHiYlz\n1NYmUhkAHPp12sdf/OwROueOd+ygtq1br6G2IugB6bS3HrdF8mCZXB+VqFLoZeidX4hMUfALkSkK\nfiEyRcEvRKYo+IXIlL7u9puVUCGtskqlhd+HHLz2XDuo7zdQ5eeK6gKOkjp4L798NDkOAD//xc+o\njakYANAIdvS9zbeBm0TlKCFSD3gy09QUb/NlZZ70w3J+PFJh5maordHgCk107Rw5kk76+Yfvptua\nAcDJl3irtPe9/z9Q25vezOsCloMWceUasQX9utqkMVekmV2O3vmFyBQFvxCZouAXIlMU/EJkioJf\niExR8AuRKb2067oPwB8BOOHut3THPgvgTwBcLFD3aXfnBekuUnKgyjSgIIuBtckKJK92cF/z6J5X\ncHnF22nb4/ufonNeOc5r8c01uFRZDWSeRp0n4hRE7Ak6ioXrUW8FcqpzOZL3NuNiVNP5ucoFv1RL\nbe5/QWTAC6TGIACMP7Gf2o6c5BLhO991O7XtuI0nCw2QVmSbr9lC59QG03NsAVpfL+/8fwvgzsT4\nF919R/ff/IEvhLiimDf43f0RAGf64IsQoo8s5TP/J81sn5ndZ2brl80jIURfWGzwfxnAjQB2ADgK\n4PPsiWa2y8zGzWz81CneglkI0V8WFfzuftzdW97pjPEVADuD5+529zF3H9u4UX8gCHGlsKjgN7NL\ntyE/DIBvdwshrkh6kfq+BuA9ADaa2WEAnwHwHjPbgU4S0UEAf9rLydwBZ7XknMtXTuShqF0U1QcB\ntCK5Kah1d/iVY8nx7//gYTpnrh5JZfze2w6kz6jcIctwqweyYgTPOwRagTTHpFsL8s7aTX62KBuw\nWuG184pKWjItCp7lOGP8dXnxCM/gPPb9H1Dbvz7zNLW99W23JMdvfyf9gxq/9cZtyXEWKynmDX53\n/1hi+N6ezyCEuCLRN/yEyBQFvxCZouAXIlMU/EJkioJfiEzpbwFPGAqSGVcJinEy9aIc3LuaQcbf\n5IUJatv7OG+v9X0i5Ty3/znuR5P/XkURFBINWlCFGXpksRpNXpg0qvoY1VUdJAVNAaDtkUiYJiqe\n2pjlGYTtoLint9LFMdu1QX6uQGaNZMWTwXX1wk9/Tm0HX0nLh1uvv4HO2bh1a3J8Iauud34hMkXB\nL0SmKPiFyBQFvxCZouAXIlMU/EJkSl+lPm830ZhNVwRrtvh9qFRKZ2DVAxntwgSXf374T/9MbXv2\nPEhtBw++nByfmeYZiQuqqPgqgnmhKW2MpMPoHeBN119Pbe/9vXdTW1FOX1rNoCBoVMP15NnT1Hbs\nxAlqO3M2fb1dmOB9Aafn+Os5M8evq6IIJMJAFi1V0q/A1m3X0DmDw0PpYy2g56Xe+YXIFAW/EJmi\n4BciUxT8QmSKgl+ITOnrbj/gMKSTNyxItCiRVk2NSZ4IcnaCJ1n84rG91PbKCd6fZIacrh0so5W4\njxZk6Fh0Xw6SZtgRo9putQG+9n/4gQ9Q2x/c8S5qK5PaeTONOp1THUq3oAKA9Rs3Utv56UlqOzdx\nITl+PHidnzvwErXt2fN9ajt9+iS1rV2T3p0HgH/3trckx4er/BqoWHodbQGpPXrnFyJTFPxCZIqC\nX4hMUfALkSkKfiEyRcEvRKb00q5rG4C/A7AZnZSS3e7+JTO7CsA3AFyHTsuuj7h72IbXrEClOpK0\nFSRRAeAtr+rgstHPAjnvEGm7BQBTdX7MmVY64cPKQVJS0PopshVRQpClZTQAAKmDVyGJNgCw7dpr\nqe3WW2+lto1Xc/mNqZjrg+SXM5PnqQ3GE4K2bLma2jZvTdtuuulGOuedb7+N2t66ndfVe/55Xstx\n48YN1HbzW96cHB+q8WugNUek7AXUTuzlnb8J4C/cfTuA2wH8uZltB3APgIfd/SYAD3d/FkK8Rpg3\n+N39qLs/3n08AeAAgK0A7gJwf/dp9wP40Eo5KYRYfhb0md/MrgNwG4BHAWx294s1h4+h87FACPEa\noefgN7M1AL4F4FPu/qrvTHrnu6PJDyhmtsvMxs1s/PTpcEtACNFHegp+M6ugE/hfdfdvd4ePm9mW\nrn0LgGQ5FXff7e5j7j62YcP65fBZCLEMzBv81sk+uRfAAXf/wiWmBwDc3X18N4DvLr97QoiVopes\nvncD+DiA/WZ2sZfVpwF8DsA3zewTAA4B+Mi8RzJDu5yuZVYq8cyyejMtvz38ox/TOQ8+9DNqO3d2\nitoaQesnQ7rum5W4JFOQ+oMAgDaXr9pBFl4pKOLHMgXLZe7H9t9JZ5UBwNoNXEar10apbYDUrKtU\nuUy5af1V1NZs8dcFQduzkqWlryj7rVjDfXzX740tytYM2qVNTaWzEltRvcNS2n9WwzHFvMHv7j8F\nzxR9X89nEkJcUegbfkJkioJfiExR8AuRKQp+ITJFwS9EpvS1gOf5C5P4xx/+PGlrtrlEce5MOoPp\nxz/+CZ9z7hx3JMiYq1S4JGZUjgwyqdo8i82JDAUARVDcs1yKbOnxgRpvFxUsPcb3Pkltw8OHqK1G\npL5qNSh2GqxHo8nba0UZfwVR7UrBGoaERVc5pUCONDIzKvDKmJ4OJNHLfVrw0YUQrwsU/EJkioJf\niExR8AuRKQp+ITJFwS9EpljUw225GRpe4zdvf2vakTYvnNlupyWgKOspkknY8ea3pc/nQdHEtgeZ\nWcG5LMjOCmpgokIkpVYzXXwUAMrBAdutSG6qUUtBNLZYYgvWKrCFGhtdxyBrMnhLLEXGALYeEdE1\nXCYFWZ946kVMTM30pBHqnV+ITFHwC5EpCn4hMkXBL0SmKPiFyJS+Jva4A41GepfVggQYEEWiHe2y\nB/XxIoXDPUjEaad3bNvBnOYik34QqASN4Peuk/X1YD0iZaEUtAaLhKJghfmkgMUkucTnizyMWl5F\n105vHl0O+91Ki9jtb7aWt12XEOJ1iIJfiExR8AuRKQp+ITJFwS9Epij4hciUeaU+M9sG4O/QacHt\nAHa7+5fM7LMA/gTAye5TP+3ue8JjwVEQGaVNZDQgkl6itlVc8ggVGY/uh8zHYE5QXy5QCINzxXks\nTP6MJMyISPbyEm9Bxc4WyWGsll3HGEiwfFbgSUTvctmSTxXMi14yJy3iFqI29qLzNwH8hbs/bmYj\nAPaa2UNd2xfd/X8u4HxCiCuEXnr1HQVwtPt4wswOANi60o4JIVaWBX3mN7PrANwG4NHu0CfNbJ+Z\n3Wdm65fZNyHECtJz8JvZGgDfAvApd78A4MsAbgSwA52/DD5P5u0ys3EzG4/aFAsh+ktPwW9mFXQC\n/6vu/m0AcPfj7t7yThmbrwDYmZrr7rvdfczdx9j3kYUQ/Wfe4LdO1sG9AA64+xcuGd9yydM+DOCp\n5XdPCLFS9PJW/G4AHwew38ye6I59GsDHzGwHOurCQQB/Ot+B3NuYm5tM2gy8TZZ7+uOCEbkDiGvx\nhfIVOVfHlq4z6IHGY4G2ZUHbsKguYFj7j/kfHM+i9Qgkx2Zr4XXwSmEqYLBWJX6pRj5akW4bFmU5\nlkM5ktMKjlkqBTX8yO8d1Qsse/p4C1Ebe9nt/yk5ZqjpCyGubPQNPyEyRcEvRKYo+IXIFAW/EJmi\n4BciU/r6rZvBgQG8dfubk7Zf/foInVcnnbxCOS+Q7NrtSCLkck27nT5mnDHHJZ6oQGMkzYVZZ2Re\nKWh3FcmRMP7+MFxNy2gAUJCsyhJZQwBAJGEG2ZFzzShzMr3GrcCPEhaerdixBVJlJN2SeRa1SmPr\nsYAqonrnFyJTFPxCZIqCX4hMUfALkSkKfiEyRcEvRKb0Weqr4ZY3vylpO3dhis6bnEpnArZaXJKZ\nmeWSR2OaaIfzHLPZbCTHA4UKpYJnKxZBpteiW9MRIgEoOtVAhV8iv/uOHdR24xvTld5Kbb72w0M1\naqvVBqhtthFIfWT9ozqt9eY0tcU9IINjMr0aQKORvq4iWMbf7vu/1/sxFnxWIcTrAgW/EJmi4Bci\nUxT8QmSKgl+ITFHwC5EpfZX63J3KZa0Wl1BmZmaS45VAhop0l6Lg9zwnhRE789KaXlSIMyrcWC64\n/1GvwXYg3NEMwyDzMEwuDKTPucnz1HbTde9Mjg8P8LX/ra1voLaRtWuobWJ2ltoaZBnXrB+lc4ra\n4taqUuFZjqUSn8j6WUQFPJkj39nzcz7n8uP3/EwhxOsKBb8QmaLgFyJTFPxCZIqCX4hMmXe338wG\nADwCoNZ9/t+7+2fM7HoAXwewAcBeAB931s+qS6koY826DUlbtcoTYFgyxdTkHJ8T1PCLlIWoTRbD\ngi3gUqAslMtBC6fgmFGtONaKjNWJA+L6clG9w1/96tfUNjmVVmiKoC0bS+ACgJHRIWor80PCyI55\nq5X2DwDK7aB1XLCO9Ra//KMmtUx9KkWvGamtuJB8sF7e+ecAvNfdb0WnHfedZnY7gL8E8EV3fxOA\nswA+sYDzCiFWmXmD3ztcvCVXuv8cwHsB/H13/H4AH1oRD4UQK0JPn/nNrOh26D0B4CEALwI45/9W\nH/swgHQCtxDiiqSn4Hf3lrvvAHAtgJ0Abu71BGa2y8zGzWx8aooX7BBC9JcF7fa7+zkAPwLwLgDr\nzOziLsa1AJJdN9x9t7uPufvY8PDwkpwVQiwf8wa/mV1tZuu6jwcBvB/AAXRuAn/cfdrdAL67Uk4K\nIZafXhJ7tgC438wKdG4W33T375nZMwC+bmb/A8C/Arh3vgM1mi0cPjmRdqQ2QueNjF6dHJ+e5tJQ\nK5BdUI9aaHF5hdX3KwdaU7nC69IVgZzHWoMBQIskRwFAu5l+Sc25vFlEXaGCBCkLEqsmZ9My7PoN\nPKEG1UFqKg3yvxrXDayntiaRyxrBGjaboWJNOXXqJLU9ue9JamNS8Y03XE/nrL8q/TvXG737Pm/w\nu/s+ALclxl9C5/O/EOI1iL7hJ0SmKPiFyBQFvxCZouAXIlMU/EJkinnUY2i5T2Z2EsCh7o8bAZzq\n28k58uPVyI9X81rz443untbGL6Ovwf+qE5uNu/vYqpxcfsgP+aE/+4XIFQW/EJmymsG/exXPfSny\n49XIj1fzuvVj1T7zCyFWF/3ZL0SmrErwm9mdZvacmb1gZveshg9dPw6a2X4ze8LMxvt43vvM7ISZ\nPXXJ2FVm9pCZPd/9n6eqrawfnzWzI901ecLMPtgHP7aZ2Y/M7Bkze9rM/mN3vK9rEvjR1zUxswEz\n+xcze7Lrx3/vjl9vZo924+YbZsb7g/WCu/f1H4ACnTJgNwCoAngSwPZ++9H15SCAjatw3t8H8HYA\nT10y9lcA7uk+vgfAX66SH58F8J/6vB5bALy9+3gEwC8BbO/3mgR+9HVN0CnCu6b7uALgUQC3A/gm\ngI92x/8GwJ8t5Tyr8c6/E8AL7v6Sd0p9fx3AXavgx6rh7o8AOHPZ8F3oFEIF+lQQlfjRd9z9qLs/\n3n08gU6xmK3o85oEfvQV77DiRXNXI/i3Anj5kp9Xs/inA3jQzPaa2a5V8uEim939aPfxMQCbV9GX\nT5rZvu7HghX/+HEpZnYdOvUjHsUqrsllfgB9XpN+FM3NfcPvDnd/O4A/BPDnZvb7q+0Q0LnzA0HH\nhpXlywBuRKdHw1EAn+/Xic1sDYBvAfiUu1+41NbPNUn40fc18SUUze2V1Qj+IwC2XfIzLf650rj7\nke7/JwB8B6tbmei4mW0BgO7/J1bDCXc/3r3w2gC+gj6tiZlV0Am4r7r7t7vDfV+TlB+rtSbdcy+4\naG6vrEbwPwbgpu7OZRXARwE80G8nzGzYzEYuPgbwAQBPxbNWlAfQKYQKrGJB1IvB1uXD6MOaWKeI\n3b0ADrj7Fy4x9XVNmB/9XpO+Fc3t1w7mZbuZH0RnJ/VFAP91lXy4AR2l4UkAT/fTDwBfQ+fPxwY6\nn90+gU7Pw4cBPA/ghwCuWiU//g+A/QD2oRN8W/rgxx3o/Em/D8AT3X8f7PeaBH70dU0AvA2dorj7\n0LnR/LdLrtl/AfACgP8HoLaU8+gbfkJkSu4bfkJki4JfiExR8AuRKQp+ITJFwS9Epij4hcgUBb8Q\nmaLgFyJT/j90TVi8eevRaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8217d0990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2, tf.shape(a1), tf.shape(a2), tf.shape(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_, sha1, sha2, sha3 = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(inp, sha1, sha2, sha3):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, sha3, \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, sha2,\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, sha1,\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.shape(tf_input),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_, sha1, sha2, sha3)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector, sha1, sha2, sha3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../Models/IDEA_1/Model_cifar_4/model_cifar_4-400\n",
      "epoch: 401\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.449699103832\n",
      "range:(4480, 4608) loss= 0.460291177034\n",
      "range:(8960, 9088) loss= 0.460178166628\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.49857288599\n",
      "range:(4480, 4608) loss= 0.468072354794\n",
      "range:(8960, 9088) loss= 0.429507613182\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.441324532032\n",
      "range:(4480, 4608) loss= 0.476051598787\n",
      "range:(8960, 9088) loss= 0.464723169804\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.456406861544\n",
      "range:(4480, 4608) loss= 0.436771929264\n",
      "range:(8960, 9088) loss= 0.445618569851\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.449156045914\n",
      "range:(4480, 4608) loss= 0.436626374722\n",
      "range:(8960, 9088) loss= 0.41020232439\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 402\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.417949199677\n",
      "range:(4480, 4608) loss= 0.434644311666\n",
      "range:(8960, 9088) loss= 0.427983045578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.4463763237\n",
      "range:(4480, 4608) loss= 0.438543558121\n",
      "range:(8960, 9088) loss= 0.492753475904\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.444433033466\n",
      "range:(4480, 4608) loss= 0.435423731804\n",
      "range:(8960, 9088) loss= 0.531950116158\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.470688760281\n",
      "range:(4480, 4608) loss= 0.465559184551\n",
      "range:(8960, 9088) loss= 0.426133394241\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.424571990967\n",
      "range:(4480, 4608) loss= 0.43998247385\n",
      "range:(8960, 9088) loss= 0.416077673435\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 403\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.420677125454\n",
      "range:(4480, 4608) loss= 0.443455070257\n",
      "range:(8960, 9088) loss= 0.433702290058\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.431456178427\n",
      "range:(4480, 4608) loss= 0.444633245468\n",
      "range:(8960, 9088) loss= 0.427345395088\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.447691559792\n",
      "range:(4480, 4608) loss= 0.452545523643\n",
      "range:(8960, 9088) loss= 0.423400908709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.426172673702\n",
      "range:(4480, 4608) loss= 0.426611691713\n",
      "range:(8960, 9088) loss= 0.420289427042\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.435437917709\n",
      "range:(4480, 4608) loss= 0.418252110481\n",
      "range:(8960, 9088) loss= 0.429326176643\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 404\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.82871794701\n",
      "range:(4480, 4608) loss= 4534.22216797\n",
      "range:(8960, 9088) loss= 164.914749146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 105.636360168\n",
      "range:(4480, 4608) loss= 31.660572052\n",
      "range:(8960, 9088) loss= 22.5540943146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 21.4362831116\n",
      "range:(4480, 4608) loss= 22.3179512024\n",
      "range:(8960, 9088) loss= 16.6251068115\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 17.031791687\n",
      "range:(4480, 4608) loss= 16.0620136261\n",
      "range:(8960, 9088) loss= 13.9929409027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 13.7091989517\n",
      "range:(4480, 4608) loss= 12.893655777\n",
      "range:(8960, 9088) loss= 13.5675764084\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 405\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 18.8777503967\n",
      "range:(4480, 4608) loss= 11.3380966187\n",
      "range:(8960, 9088) loss= 10.0647649765\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 12.9266176224\n",
      "range:(4480, 4608) loss= 12.6619110107\n",
      "range:(8960, 9088) loss= 16.1723518372\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 16.146982193\n",
      "range:(4480, 4608) loss= 10.98087883\n",
      "range:(8960, 9088) loss= 11.9797773361\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 13.8624677658\n",
      "range:(4480, 4608) loss= 8.85297012329\n",
      "range:(8960, 9088) loss= 12.1655378342\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 8.12193393707\n",
      "range:(4480, 4608) loss= 7.91931438446\n",
      "range:(8960, 9088) loss= 8.16743850708\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 406\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 13.7959794998\n",
      "range:(4480, 4608) loss= 7.20394229889\n",
      "range:(8960, 9088) loss= 6.39110183716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 9.21131515503\n",
      "range:(4480, 4608) loss= 8.38275051117\n",
      "range:(8960, 9088) loss= 5.7824177742\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 8.65239334106\n",
      "range:(4480, 4608) loss= 8.06374359131\n",
      "range:(8960, 9088) loss= 6.19334554672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 6.67808389664\n",
      "range:(4480, 4608) loss= 8.15970897675\n",
      "range:(8960, 9088) loss= 6.21863174438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 7.36675024033\n",
      "range:(4480, 4608) loss= 7.71880674362\n",
      "range:(8960, 9088) loss= 7.77890729904\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 407\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 7.21355962753\n",
      "range:(4480, 4608) loss= 6.51518201828\n",
      "range:(8960, 9088) loss= 7.2320690155\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 8.58530521393\n",
      "range:(4480, 4608) loss= 5.06794595718\n",
      "range:(8960, 9088) loss= 4.50675439835\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 4.80700206757\n",
      "range:(4480, 4608) loss= 6.09948205948\n",
      "range:(8960, 9088) loss= 5.00685930252\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 5.28054332733\n",
      "range:(4480, 4608) loss= 5.25771808624\n",
      "range:(8960, 9088) loss= 7.8072013855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 8.83696269989\n",
      "range:(4480, 4608) loss= 5.14752578735\n",
      "range:(8960, 9088) loss= 4.07772350311\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 408\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 4.57205533981\n",
      "range:(4480, 4608) loss= 6.11549663544\n",
      "range:(8960, 9088) loss= 5.74850416183\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 5.31764602661\n",
      "range:(4480, 4608) loss= 3.84854316711\n",
      "range:(8960, 9088) loss= 5.83330917358\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 4.42795562744\n",
      "range:(4480, 4608) loss= 4.07207345963\n",
      "range:(8960, 9088) loss= 4.3966088295\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4.561668396\n",
      "range:(4480, 4608) loss= 5.76085138321\n",
      "range:(8960, 9088) loss= 4.45402336121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 5.13588285446\n",
      "range:(4480, 4608) loss= 3.63021183014\n",
      "range:(8960, 9088) loss= 5.820291996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 409\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 6.60307455063\n",
      "range:(4480, 4608) loss= 4.14364862442\n",
      "range:(8960, 9088) loss= 3.83212566376\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 6.4866361618\n",
      "range:(4480, 4608) loss= 4.4058599472\n",
      "range:(8960, 9088) loss= 3.62401843071\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 3.39300465584\n",
      "range:(4480, 4608) loss= 3.1559035778\n",
      "range:(8960, 9088) loss= 3.96576547623\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 4.15226173401\n",
      "range:(4480, 4608) loss= 4.21605634689\n",
      "range:(8960, 9088) loss= 3.34995913506\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2.98554325104\n",
      "range:(4480, 4608) loss= 3.27501916885\n",
      "range:(8960, 9088) loss= 2.84893512726\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 410\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 3.32556843758\n",
      "range:(4480, 4608) loss= 4.25073862076\n",
      "range:(8960, 9088) loss= 2.4619550705\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 3.26669096947\n",
      "range:(4480, 4608) loss= 2.70149922371\n",
      "range:(8960, 9088) loss= 3.34893155098\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.69091296196\n",
      "range:(4480, 4608) loss= 3.62966489792\n",
      "range:(8960, 9088) loss= 3.91370034218\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 3.92915034294\n",
      "range:(4480, 4608) loss= 2.70676112175\n",
      "range:(8960, 9088) loss= 2.31369757652\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2.62716221809\n",
      "range:(4480, 4608) loss= 2.31185984612\n",
      "range:(8960, 9088) loss= 2.61605739594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 411\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2.91738319397\n",
      "range:(4480, 4608) loss= 2.88313102722\n",
      "range:(8960, 9088) loss= 2.31603860855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 2.78626012802\n",
      "range:(4480, 4608) loss= 2.24611759186\n",
      "range:(8960, 9088) loss= 2.40140414238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.25436592102\n",
      "range:(4480, 4608) loss= 2.22695446014\n",
      "range:(8960, 9088) loss= 1.88423800468\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2.4156036377\n",
      "range:(4480, 4608) loss= 1.92617022991\n",
      "range:(8960, 9088) loss= 2.3270573616\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2.08539390564\n",
      "range:(4480, 4608) loss= 3.77048921585\n",
      "range:(8960, 9088) loss= 2.03527665138\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 412\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2.29323267937\n",
      "range:(4480, 4608) loss= 1.88407123089\n",
      "range:(8960, 9088) loss= 1.8044372797\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 2.05192399025\n",
      "range:(4480, 4608) loss= 2.07580709457\n",
      "range:(8960, 9088) loss= 5.59036016464\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.32902431488\n",
      "range:(4480, 4608) loss= 1.9409198761\n",
      "range:(8960, 9088) loss= 1.66690862179\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.6423201561\n",
      "range:(4480, 4608) loss= 2.94365882874\n",
      "range:(8960, 9088) loss= 4.41490840912\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 2.94291830063\n",
      "range:(4480, 4608) loss= 1.73695921898\n",
      "range:(8960, 9088) loss= 1.74034404755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 413\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2.19412183762\n",
      "range:(4480, 4608) loss= 1.57979094982\n",
      "range:(8960, 9088) loss= 1.68223190308\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.631970644\n",
      "range:(4480, 4608) loss= 1.39838385582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8960, 9088) loss= 3.05339741707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.19386196136\n",
      "range:(4480, 4608) loss= 10.4499692917\n",
      "range:(8960, 9088) loss= 1.87699520588\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 2.26938414574\n",
      "range:(4480, 4608) loss= 1.64381861687\n",
      "range:(8960, 9088) loss= 1.30877053738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.6421380043\n",
      "range:(4480, 4608) loss= 1.35126459599\n",
      "range:(8960, 9088) loss= 1.3067677021\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 414\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.23283410072\n",
      "range:(4480, 4608) loss= 1.71838116646\n",
      "range:(8960, 9088) loss= 1.26176047325\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.1647247076\n",
      "range:(4480, 4608) loss= 1.06858813763\n",
      "range:(8960, 9088) loss= 2.68748521805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.44586145878\n",
      "range:(4480, 4608) loss= 1.13638019562\n",
      "range:(8960, 9088) loss= 1.07066655159\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.1921043396\n",
      "range:(4480, 4608) loss= 1.21106517315\n",
      "range:(8960, 9088) loss= 1.37954258919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.31687271595\n",
      "range:(4480, 4608) loss= 4.25516080856\n",
      "range:(8960, 9088) loss= 2.89353370667\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 415\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 2.32389330864\n",
      "range:(4480, 4608) loss= 3.76162385941\n",
      "range:(8960, 9088) loss= 1.54356002808\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.26548719406\n",
      "range:(4480, 4608) loss= 0.958811581135\n",
      "range:(8960, 9088) loss= 1.28297817707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.23626041412\n",
      "range:(4480, 4608) loss= 1.9804058075\n",
      "range:(8960, 9088) loss= 1.29076611996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 1.43294644356\n",
      "range:(4480, 4608) loss= 1.09201693535\n",
      "range:(8960, 9088) loss= 1.05999827385\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.14492607117\n",
      "range:(4480, 4608) loss= 1.28556573391\n",
      "range:(8960, 9088) loss= 1.31130981445\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 416\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 1.20060110092\n",
      "range:(4480, 4608) loss= 0.852026045322\n",
      "range:(8960, 9088) loss= 0.903890311718\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 1.18174731731\n",
      "range:(4480, 4608) loss= 0.854348003864\n",
      "range:(8960, 9088) loss= 3.84314680099\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.99103474617\n",
      "range:(4480, 4608) loss= 2.67930960655\n",
      "range:(8960, 9088) loss= 0.93086117506\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.947756290436\n",
      "range:(4480, 4608) loss= 0.789415359497\n",
      "range:(8960, 9088) loss= 0.756344795227\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.797432541847\n",
      "range:(4480, 4608) loss= 0.910251319408\n",
      "range:(8960, 9088) loss= 0.702561497688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 417\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.779541730881\n",
      "range:(4480, 4608) loss= 0.759917974472\n",
      "range:(8960, 9088) loss= 0.9417552948\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.708277404308\n",
      "range:(4480, 4608) loss= 0.718649864197\n",
      "range:(8960, 9088) loss= 0.696298956871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.71463984251\n",
      "range:(4480, 4608) loss= 0.90231859684\n",
      "range:(8960, 9088) loss= 0.702219128609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.695131182671\n",
      "range:(4480, 4608) loss= 0.651795506477\n",
      "range:(8960, 9088) loss= 0.641302645206\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.638228058815\n",
      "range:(4480, 4608) loss= 0.865502119064\n",
      "range:(8960, 9088) loss= 0.659218788147\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 418\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.690282642841\n",
      "range:(4480, 4608) loss= 0.664857566357\n",
      "range:(8960, 9088) loss= 0.686920881271\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.683747172356\n",
      "range:(4480, 4608) loss= 0.639894843102\n",
      "range:(8960, 9088) loss= 0.617360651493\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.615507781506\n",
      "range:(4480, 4608) loss= 1.15217459202\n",
      "range:(8960, 9088) loss= 0.670180559158\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.711885809898\n",
      "range:(4480, 4608) loss= 0.680620908737\n",
      "range:(8960, 9088) loss= 0.962894678116\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.700245857239\n",
      "range:(4480, 4608) loss= 0.632818102837\n",
      "range:(8960, 9088) loss= 0.617993354797\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 419\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.621200025082\n",
      "range:(4480, 4608) loss= 0.666085958481\n",
      "range:(8960, 9088) loss= 0.757574081421\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.751377403736\n",
      "range:(4480, 4608) loss= 2.38943386078\n",
      "range:(8960, 9088) loss= 1.24988019466\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 2.0046350956\n",
      "range:(4480, 4608) loss= 0.912997484207\n",
      "range:(8960, 9088) loss= 0.639734148979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.620598554611\n",
      "range:(4480, 4608) loss= 0.604752182961\n",
      "range:(8960, 9088) loss= 0.568130373955\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.60273206234\n",
      "range:(4480, 4608) loss= 0.797902107239\n",
      "range:(8960, 9088) loss= 0.64282643795\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 420\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.62881731987\n",
      "range:(4480, 4608) loss= 0.576540470123\n",
      "range:(8960, 9088) loss= 0.577607691288\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.602453112602\n",
      "range:(4480, 4608) loss= 0.680824756622\n",
      "range:(8960, 9088) loss= 0.799948692322\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.715402781963\n",
      "range:(4480, 4608) loss= 0.581363797188\n",
      "range:(8960, 9088) loss= 0.543757975101\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.558372020721\n",
      "range:(4480, 4608) loss= 0.599407374859\n",
      "range:(8960, 9088) loss= 1.00379657745\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 1.2089458704\n",
      "range:(4480, 4608) loss= 0.561973810196\n",
      "range:(8960, 9088) loss= 0.705886900425\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 421\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.667098522186\n",
      "range:(4480, 4608) loss= 0.765819072723\n",
      "range:(8960, 9088) loss= 0.556859970093\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.562386751175\n",
      "range:(4480, 4608) loss= 0.552871108055\n",
      "range:(8960, 9088) loss= 0.94634771347\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 1.39067387581\n",
      "range:(4480, 4608) loss= 0.572177052498\n",
      "range:(8960, 9088) loss= 0.502598404884\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.509100854397\n",
      "range:(4480, 4608) loss= 0.542025268078\n",
      "range:(8960, 9088) loss= 0.673893153667\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.60984057188\n",
      "range:(4480, 4608) loss= 0.540562033653\n",
      "range:(8960, 9088) loss= 0.482556045055\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 422\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.546651363373\n",
      "range:(4480, 4608) loss= 0.801511287689\n",
      "range:(8960, 9088) loss= 0.572947204113\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.531669735909\n",
      "range:(4480, 4608) loss= 0.502881526947\n",
      "range:(8960, 9088) loss= 0.591307878494\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.635470747948\n",
      "range:(4480, 4608) loss= 0.620328307152\n",
      "range:(8960, 9088) loss= 0.882890105247\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.919298946857\n",
      "range:(4480, 4608) loss= 0.693966329098\n",
      "range:(8960, 9088) loss= 0.60076290369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.567491948605\n",
      "range:(4480, 4608) loss= 0.62320971489\n",
      "range:(8960, 9088) loss= 0.568249166012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 423\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.564510285854\n",
      "range:(4480, 4608) loss= 0.59883928299\n",
      "range:(8960, 9088) loss= 0.574633538723\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.608272194862\n",
      "range:(4480, 4608) loss= 0.4962708354\n",
      "range:(8960, 9088) loss= 0.474578738213\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.565333247185\n",
      "range:(4480, 4608) loss= 0.502489209175\n",
      "range:(8960, 9088) loss= 0.553365111351\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.595236301422\n",
      "range:(4480, 4608) loss= 0.519851863384\n",
      "range:(8960, 9088) loss= 0.495489656925\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.515573322773\n",
      "range:(4480, 4608) loss= 0.537143945694\n",
      "range:(8960, 9088) loss= 0.453873455524\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 424\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.458140313625\n",
      "range:(4480, 4608) loss= 0.5089199543\n",
      "range:(8960, 9088) loss= 0.438025087118\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.437861055136\n",
      "range:(4480, 4608) loss= 0.440591871738\n",
      "range:(8960, 9088) loss= 0.424171358347\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.445606738329\n",
      "range:(4480, 4608) loss= 0.424079030752\n",
      "range:(8960, 9088) loss= 0.52235263586\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.50335007906\n",
      "range:(4480, 4608) loss= 0.424498200417\n",
      "range:(8960, 9088) loss= 0.436675846577\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.443112254143\n",
      "range:(4480, 4608) loss= 0.425907492638\n",
      "range:(8960, 9088) loss= 0.443910032511\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 425\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.484816193581\n",
      "range:(4480, 4608) loss= 0.482327580452\n",
      "range:(8960, 9088) loss= 0.423549860716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.422593951225\n",
      "range:(4480, 4608) loss= 0.413076281548\n",
      "range:(8960, 9088) loss= 0.428763836622\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.437714517117\n",
      "range:(4480, 4608) loss= 0.417265355587\n",
      "range:(8960, 9088) loss= 0.411831736565\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.427596032619\n",
      "range:(4480, 4608) loss= 0.414026737213\n",
      "range:(8960, 9088) loss= 0.408474087715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.415468543768\n",
      "range:(4480, 4608) loss= 0.417425811291\n",
      "range:(8960, 9088) loss= 0.416411757469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 426\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.412526518106\n",
      "range:(4480, 4608) loss= 0.467696219683\n",
      "range:(8960, 9088) loss= 0.43416249752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.428784251213\n",
      "range:(4480, 4608) loss= 0.416231393814\n",
      "range:(8960, 9088) loss= 0.415878593922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.41333591938\n",
      "range:(4480, 4608) loss= 0.41528108716\n",
      "range:(8960, 9088) loss= 0.39788171649\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.423041909933\n",
      "range:(4480, 4608) loss= 0.430978775024\n",
      "range:(8960, 9088) loss= 0.414647758007\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.417060047388\n",
      "range:(4480, 4608) loss= 0.412398964167\n",
      "range:(8960, 9088) loss= 0.395726501942\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 427\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.400391668081\n",
      "range:(4480, 4608) loss= 0.412402033806\n",
      "range:(8960, 9088) loss= 0.397112011909\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.394771456718\n",
      "range:(4480, 4608) loss= 0.399966716766\n",
      "range:(8960, 9088) loss= 0.403138846159\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.40948754549\n",
      "range:(4480, 4608) loss= 0.410730868578\n",
      "range:(8960, 9088) loss= 0.393672168255\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.417662799358\n",
      "range:(4480, 4608) loss= 0.407359242439\n",
      "range:(8960, 9088) loss= 0.389503419399\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.402831614017\n",
      "range:(4480, 4608) loss= 0.409708678722\n",
      "range:(8960, 9088) loss= 0.393233448267\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 428\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.407589912415\n",
      "range:(4480, 4608) loss= 0.410699665546\n",
      "range:(8960, 9088) loss= 0.406189471483\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.405112504959\n",
      "range:(4480, 4608) loss= 0.393843591213\n",
      "range:(8960, 9088) loss= 0.401512145996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.397095263004\n",
      "range:(4480, 4608) loss= 0.400606334209\n",
      "range:(8960, 9088) loss= 0.388050109148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.415121018887\n",
      "range:(4480, 4608) loss= 0.424459189177\n",
      "range:(8960, 9088) loss= 0.407660007477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.421293199062\n",
      "range:(4480, 4608) loss= 0.416392683983\n",
      "range:(8960, 9088) loss= 0.388388574123\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 429\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.396462351084\n",
      "range:(4480, 4608) loss= 0.412333905697\n",
      "range:(8960, 9088) loss= 0.396094143391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.399015128613\n",
      "range:(4480, 4608) loss= 0.396418511868\n",
      "range:(8960, 9088) loss= 0.397397816181\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.396598666906\n",
      "range:(4480, 4608) loss= 0.394859433174\n",
      "range:(8960, 9088) loss= 0.386718451977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.409778684378\n",
      "range:(4480, 4608) loss= 0.40179258585\n",
      "range:(8960, 9088) loss= 0.387610435486\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.397963136435\n",
      "range:(4480, 4608) loss= 0.408288359642\n",
      "range:(8960, 9088) loss= 0.384951531887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 430\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.38816216588\n",
      "range:(4480, 4608) loss= 0.406785964966\n",
      "range:(8960, 9088) loss= 0.397165954113\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.396795749664\n",
      "range:(4480, 4608) loss= 0.398162484169\n",
      "range:(8960, 9088) loss= 0.396970748901\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.395411431789\n",
      "range:(4480, 4608) loss= 0.398776143789\n",
      "range:(8960, 9088) loss= 0.388011485338\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.406134188175\n",
      "range:(4480, 4608) loss= 0.395763367414\n",
      "range:(8960, 9088) loss= 0.386459857225\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.396197557449\n",
      "range:(4480, 4608) loss= 0.404480755329\n",
      "range:(8960, 9088) loss= 0.382283747196\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 431\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.384733408689\n",
      "range:(4480, 4608) loss= 0.402303636074\n",
      "range:(8960, 9088) loss= 0.387751191854\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.393961131573\n",
      "range:(4480, 4608) loss= 0.389826357365\n",
      "range:(8960, 9088) loss= 0.392249315977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.392455726862\n",
      "range:(4480, 4608) loss= 0.398122668266\n",
      "range:(8960, 9088) loss= 0.385349452496\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.404345631599\n",
      "range:(4480, 4608) loss= 0.397519767284\n",
      "range:(8960, 9088) loss= 0.38498210907\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.397311925888\n",
      "range:(4480, 4608) loss= 0.403337061405\n",
      "range:(8960, 9088) loss= 0.384471297264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 432\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.386598527431\n",
      "range:(4480, 4608) loss= 0.398970484734\n",
      "range:(8960, 9088) loss= 0.388077557087\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.392509579659\n",
      "range:(4480, 4608) loss= 0.38701108098\n",
      "range:(8960, 9088) loss= 0.3964407444\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.3936393857\n",
      "range:(4480, 4608) loss= 0.401007562876\n",
      "range:(8960, 9088) loss= 0.384848356247\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.406782507896\n",
      "range:(4480, 4608) loss= 0.396855056286\n",
      "range:(8960, 9088) loss= 0.383339822292\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.396246254444\n",
      "range:(4480, 4608) loss= 0.40050560236\n",
      "range:(8960, 9088) loss= 0.383890151978\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 433\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.386575937271\n",
      "range:(4480, 4608) loss= 1737249.25\n",
      "range:(8960, 9088) loss= 16489.2207031\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 4578.17919922\n",
      "range:(4480, 4608) loss= 1100.63000488\n",
      "range:(8960, 9088) loss= 710.027954102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 681.527526855\n",
      "range:(4480, 4608) loss= 550.730712891\n",
      "range:(8960, 9088) loss= 420.3409729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 407.53414917\n",
      "range:(4480, 4608) loss= 370.0625\n",
      "range:(8960, 9088) loss= 309.19140625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 322.716918945\n",
      "range:(4480, 4608) loss= 295.160919189\n",
      "range:(8960, 9088) loss= 236.12197876\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 434\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 272.429168701\n",
      "range:(4480, 4608) loss= 237.799865723\n",
      "range:(8960, 9088) loss= 205.698501587\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 203.859741211\n",
      "range:(4480, 4608) loss= 192.818099976\n",
      "range:(8960, 9088) loss= 187.637481689\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 189.921661377\n",
      "range:(4480, 4608) loss= 182.385025024\n",
      "range:(8960, 9088) loss= 161.12902832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 162.849899292\n",
      "range:(4480, 4608) loss= 147.946182251\n",
      "range:(8960, 9088) loss= 147.414031982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 147.670974731\n",
      "range:(4480, 4608) loss= 128.817230225\n",
      "range:(8960, 9088) loss= 118.725563049\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 435\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 131.577209473\n",
      "range:(4480, 4608) loss= 124.618408203\n",
      "range:(8960, 9088) loss= 109.109924316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 106.508789062\n",
      "range:(4480, 4608) loss= 109.349090576\n",
      "range:(8960, 9088) loss= 108.492141724\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 113.92049408\n",
      "range:(4480, 4608) loss= 97.355796814\n",
      "range:(8960, 9088) loss= 94.0433425903\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 97.4595108032\n",
      "range:(4480, 4608) loss= 89.7313461304\n",
      "range:(8960, 9088) loss= 94.6834487915\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 87.8630142212\n",
      "range:(4480, 4608) loss= 74.2834625244\n",
      "range:(8960, 9088) loss= 73.4303436279\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 436\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 81.378692627\n",
      "range:(4480, 4608) loss= 74.5965881348\n",
      "range:(8960, 9088) loss= 75.0723495483\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 81.5484085083\n",
      "range:(4480, 4608) loss= 61.5317001343\n",
      "range:(8960, 9088) loss= 67.1975479126\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 68.253288269\n",
      "range:(4480, 4608) loss= 71.9900970459\n",
      "range:(8960, 9088) loss= 60.2472991943\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 89.7057571411\n",
      "range:(4480, 4608) loss= 63.1965065002\n",
      "range:(8960, 9088) loss= 60.4933052063\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 66.6426086426\n",
      "range:(4480, 4608) loss= 55.1168937683\n",
      "range:(8960, 9088) loss= 52.3220901489\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 437\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 65.7449264526\n",
      "range:(4480, 4608) loss= 57.9649047852\n",
      "range:(8960, 9088) loss= 49.070690155\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 53.1970214844\n",
      "range:(4480, 4608) loss= 52.979637146\n",
      "range:(8960, 9088) loss= 56.916721344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 43.0404167175\n",
      "range:(4480, 4608) loss= 63.4714126587\n",
      "range:(8960, 9088) loss= 46.3025131226\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 52.1884078979\n",
      "range:(4480, 4608) loss= 40.7013015747\n",
      "range:(8960, 9088) loss= 57.0169029236\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 51.6780052185\n",
      "range:(4480, 4608) loss= 47.0148200989\n",
      "range:(8960, 9088) loss= 47.6167526245\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 438\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 48.4383049011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(4480, 4608) loss= 42.7089347839\n",
      "range:(8960, 9088) loss= 36.9629058838\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 47.4578132629\n",
      "range:(4480, 4608) loss= 54.0975227356\n",
      "range:(8960, 9088) loss= 60.8848152161\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 44.4400596619\n",
      "range:(4480, 4608) loss= 37.9198799133\n",
      "range:(8960, 9088) loss= 32.7869987488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 39.9475402832\n",
      "range:(4480, 4608) loss= 33.5393676758\n",
      "range:(8960, 9088) loss= 49.2649154663\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 38.5401153564\n",
      "range:(4480, 4608) loss= 35.3102111816\n",
      "range:(8960, 9088) loss= 28.7809104919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 439\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 32.9933204651\n",
      "range:(4480, 4608) loss= 30.4799556732\n",
      "range:(8960, 9088) loss= 36.5462188721\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 35.0914344788\n",
      "range:(4480, 4608) loss= 28.6058540344\n",
      "range:(8960, 9088) loss= 31.5061435699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 32.3526077271\n",
      "range:(4480, 4608) loss= 30.9807605743\n",
      "range:(8960, 9088) loss= 29.4339027405\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 29.9116210938\n",
      "range:(4480, 4608) loss= 27.2336025238\n",
      "range:(8960, 9088) loss= 31.2692737579\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 30.7138538361\n",
      "range:(4480, 4608) loss= 26.5975036621\n",
      "range:(8960, 9088) loss= 41.7568702698\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 440\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 40.721572876\n",
      "range:(4480, 4608) loss= 30.861536026\n",
      "range:(8960, 9088) loss= 23.8120193481\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 26.6718215942\n",
      "range:(4480, 4608) loss= 22.3710823059\n",
      "range:(8960, 9088) loss= 23.813533783\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 28.1078166962\n",
      "range:(4480, 4608) loss= 29.7838401794\n",
      "range:(8960, 9088) loss= 27.4407806396\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 32.0774230957\n",
      "range:(4480, 4608) loss= 20.9191932678\n",
      "range:(8960, 9088) loss= 60.3790969849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 43.3096389771\n",
      "range:(4480, 4608) loss= 26.4189395905\n",
      "range:(8960, 9088) loss= 25.1971340179\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 441\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 31.07538414\n",
      "range:(4480, 4608) loss= 26.1788158417\n",
      "range:(8960, 9088) loss= 18.2363262177\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 26.571723938\n",
      "range:(4480, 4608) loss= 18.7723922729\n",
      "range:(8960, 9088) loss= 19.0087814331\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 20.3564891815\n",
      "range:(4480, 4608) loss= 19.0530204773\n",
      "range:(8960, 9088) loss= 17.3427391052\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 19.3015403748\n",
      "range:(4480, 4608) loss= 19.7806816101\n",
      "range:(8960, 9088) loss= 32.2494354248\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 32.8793640137\n",
      "range:(4480, 4608) loss= 23.99650383\n",
      "range:(8960, 9088) loss= 17.4746685028\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 442\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 19.6626205444\n",
      "range:(4480, 4608) loss= 17.1214694977\n",
      "range:(8960, 9088) loss= 17.8764076233\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 16.7686691284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-dc01fee8715c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mminX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mminY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m35\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 158000\n",
    "    for ep in range(400, 500):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(np.ceil(float(len(batch_images)) / min_batch_size))):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                if(index % 35 ==0):\n",
    "                    print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_4\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174294"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
