{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 200 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 3 # all kernels are 3x3\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 128 # we look at 64 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEVCAYAAAAPaTtOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2df838fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXmUJXd13z+33tbrdM++S6MNkFgkkYkMAWwZsQgMRpyT\nQyAJVnwIQxwTmxMwFiIB4XAMOIAgyQGfkSVLGIwQBoyMwQEUJTJJkJCENhAW0sxImn2f7ul+3W+p\nmz+qJrxp/e6v33RPvx5R93NOn36vbi23flW3lt/33fsTVcVxnOKRLLYDjuMsDh78jlNQPPgdp6B4\n8DtOQfHgd5yC4sHvOAXlWRv8InK5iOxcbD/OJETkoyJyUET2LrYvACJynYh8cYHW/a9E5AcLse6i\ncFqDX0R2iEhdRI6LyF4RuVlEhk7nNhYDEVEROX+x/YghImcB7wUuUtU1i7D9M/ZivJAXocXYTr6t\nHSLyqvmsYyHu/G9U1SHgEuBS4AMLsA3nmZwFHFLV/SGjiJR77I9zpqOqp+0P2AG8quP7nwB/2/H9\nN4AfA2PA08B1HbZNgAJXA08BB4EPdtj7gZuBI8BPgT8AdnbYLwT+J3AU+Anwmx22m4HPAd8BjgP/\nG1gDfCZf38+ASyP7pcD5+efrgK8CXwTGgYeB55Bd5Pbn+/WajmV/G3g0n3cb8K4Z634/sAfYDfzr\nGduqAZ/M22Mf8KdAf8C/VwF1IM337+aO9nxHvvxd+by/mbfP0by9Lpxx/P4AeAiYAG4EVuftNg58\nH1ga2P7gjO0fB9blbXUb8IV8+Z8AmzuWWwd8DTgAbAd+L3IMlgO35+fOPcB/An7QYf9s3vZjwH3A\nK/LpVwINoJn79eBsxwVYAXwrb6PDwN8DScxnaztdxMxG4Ov5+g4B/y2ffh7wP/JpB4EvAaO57S/y\ntq7n23r/nOJ1oYIf2EAWGJ/tsF8OvJDsieNFZCf0VTOC/wayQL8YmD5xcgIfzw/CsrzBHiEPfqAC\nPA5cC1SBV+YH9bkdwX8Q+EdAX96o24HfAkrAR4E7TyH4p4DXAmWyE3s78MHcj3cC22dc8M4DBPg1\nYBJ4cccJsxd4PjBAdkHp3Nb1ZCf8MmAY+BvgY4aPl3PyxfBEe36BLDj7yS5SE8Crc1/fn7dbteP4\n/ZAs4NeTXczuJ3uCO9FuH+5m+zPa6vV5O38M+GFuS8iC9EP5MTuXLAhfa6z/VrILySDwAmAXJwf/\nvyS7QJTJXn/2An0dfnxxxvpix+VjZBfaSv73iny+qM/Gdq4BvmXsUwl4MD/Og3kbvzy3nZ8fpxqw\nErgL+Ewo1uYcrwsQ/MfJAk+BO8ivVsb8nwGun3Gybuiw3wO8Nf+8Dbiyw7aFXwT/K/KDnXTYv0z+\nZEEW/Dd02P4d8GjH9xcCR08h+L/XYXtjvs+l/PtwPn9wv4G/Bn4//3wTHcGcH3DN/wtZoJ7XYX8p\nHReWLoP/3I5p/xG4reN7QhZEl3ccv3/RYf8a8PkZ7fbXpxj83+/4fhFQzz//CvDUjPk/APy5ESRN\n4Hkd0/6YjuAPLHMEuLjDjy9a8waOyx8B3zxxzDvmifrczXZmLPtSsjt+uYt5rwJ+PCPW5hX8C/HO\nf5WqDpOdDM8je4QCQER+RUTuFJEDInIM+Ded9pzOnupJ4ESH4Tqyx7oTPNnxeR3wtKqmM+zrO77v\n6/hcD3w/lY7JmcseVNV2x3dOrE9EXiciPxSRwyJylOwueGKfZ+5T5+eVZE8D94nI0XzZv8unnwqd\n61xHR7vl7fU0C9dO8Mzj2Zf3P5wNrDuxb/n+XUv21DGTlWR3dOv4IyLvE5FHReRYvq4Rnnludc4f\nOy7/meyJ6Lsisk1Ersmnn4rP3bAReFJVWwH/VovIrSKyS0TGyJ4Kzf2ZCwsm9anq/yK7436yY/Jf\nkj3GblTVEbJHK+lylXvIGusEZ3V83g1sFJFkhn3XKbp9WhGRGtnd85PAalUdBb7NL/Z5D9nr0Qk6\n9+8gWbA9X1VH878RzTpTT4XOtM3dZCfwCf8k3+bpaKdTTQ99muwpZrTjb1hVXx+Y9wDQwjj+IvIK\nsleYt5D1SYwCx/hFO5/k22zHRVXHVfW9qnouWR/JvxeRK7rweS5tcJbRGfvH+fpeqKpLyF5rOmPl\nVLf1DBZa5/8M8GoRuTj/PgwcVtUpEbkM+OensK7bgA+IyFIR2UD2CHqCu8nuKu8XkYqIXE72OH7r\nvPdgflTJ3tkOAC0ReR3wmg77bcBvi8iFIjJA9lgO/P+78g3A9SKyCkBE1ovIa+fhz23Ab4jIFSJS\nIXs3ngb+zzzWeYJ9wHIRGely/nuAcRH5QxHpF5GSiLxARP7xzBnzp6qvA9eJyICIXETWMXyCYbKL\nwwGgLCIfApbM8G1Tx80helxE5A0icn5+cTwGtMk62GbzeeZ2ummDPcDHRWRQRPpE5GUd+3QcOCYi\n68k6YjvZR9bnMGcWNPhV9QBZh9OH8kn/FvgjERnPp912Cqv7CNmj3nbgu2Q9nie20yAL9teR3TE/\nB/yWqv5svvswH1R1HPg9sv08Qnaxu73D/h3gvwB3kj1m/jA3Tef///DE9PzR7/vAc+fhzz+Q3UH+\nK1k7vZFMmm3MdZ0d6/4ZWT/LtvyReN0s87eBN5BJwttzf/6M7HE9xLvJXjn2kj1R/nmH7b+TvRI9\nRnaOTHHyK8JX8/+HROT+2Y4LcAFZWx8H/i/wOVW9swufT9oOgIhcKyLfibTBG8n6eJ4CdgL/LDd/\nBHgx2cXnb8kufp18DPgPeVu/L7T+2ZC888A5AxCRC8lUjFroPdBxTifP2p/3/rIgIm8WkZqILAU+\nAfyNB77TCzz4F593kenpT5C9W/7O4rrjFAV/7HecguJ3fscpKB78jlNQPPgdp6B48DtOQfHgd5yC\n4sHvOAXFg99xCooHv+MUFA9+xykoHvyOU1A8+B2noHjwO05B8eB3nILiwe84BWVeo7iIyJVkgyWU\ngD9T1Y/PMr+KVa7TNESYYzqyRLYVX2Xv0p+l67qmMxcM+1ip2Id6eHDAtPX310xbtWyvs1QK31di\nexVr3TRyYDS1bY1WuC5Ks2nXS2m127atZdui64ws126nwemxfbZIU0VVuzp55pzPLyIlspppryar\nPfYj4G2q+lNrmSQRrdZKhu3UH0Lm6ntsW7ETSY2DFLuYRIkslkRqQGokTKQUtq1bb1d9/tXLLjZt\nl15o14jcsHq5aRsdGgxOT0rh4w/QjrR9vdG0bdO2bfe+g8Hpew4cMpc5dHTMtO0/eMS07TG2BXBg\n/zHTdmxsIjh9OnIxwTg/Jo5P0W6nXZ2Q83nsvwx4XFW35QUgbwXeNI/1OY7TQ+YT/Os5uULqTk4e\n/MFxnDOYBR+5VUS2kA2t5TjOGcR8gn8XJ4+gsoHAyC+quhXYCtk7/zy25zjOaWQ+j/0/Ai4QkXNE\npAq8lZMHPnAc5wxmznd+VW2JyLvJRkspATep6k9iy4gIpUhvr4UlhZw8LucztzUXW7lq+2dtL01t\nP2KKREx1mEs7AbTSsKQ0MTEdnA5w5Mi4aZuq2z3OzYa938cnpoLT+/v6zWX6BmxbTApuNe3e/orR\nxLWy3b6xO6KmdnuUIn3s5bK91opxzrUj53dSqgSnTyb2cX6GT13PGUBVv002wKHjOM8y/Bd+jlNQ\nPPgdp6B48DtOQfHgd5yC4sHvOAVlwX/h14mqmrJYTC5LjYSPmGSXJBHdJWaKKGzWOisSll0AKhXb\nlhiZbwDtSGZZMyJtYZiOj0+aixw+ZCedTByvz8nWrBuS04jdwH19faatEjlm7aYtb9UMiW1oIJKt\nGNHshgdsH1ujS2ybfThNSW+qYe9XvR5OBtJIHM3E7/yOU1A8+B2noHjwO05B8eB3nILiwe84BaWn\nvf0IqIR7I5NyJNnG6IKfa527JJbYE+mB7xusBqdX++ye4zRScqsRKU1lKRwZkaQl45A2IiWhnt65\n17Rtf2q3aUubDdO2dlW4bNjKlWvs9RkJXAD1uq1WtCJ+WLbUqO0HMGKUIINZ6j9G7qXNSGUtNRK8\nNKI61KfCSsCBvXaS1kz8zu84BcWD33EKige/4xQUD37HKSge/I5TUDz4Haeg9FTqkwRKA2H5Iolc\nh0qGTWLySWTEIonYYpXzrDpsjcSWmibadnJGqWo3f7kUlhUB0oa9PWvfylVbjhyr27LiQ489adoa\nLVuqXLZiWXC6luwMl2hp58gIRpJEEquq4XYcHLC3VqvZyTtDkaHNksQ+e1qRM0uModT6l9jbahvu\njx35mbnMTPzO7zgFxYPfcQqKB7/jFBQPfscpKB78jlNQPPgdp6DMS+oTkR3AONAGWqq6eZb5qdbC\n0kspUjzPSAQkjQwX1W7ZUk4aGQapERnGqTQQbq7ysN2Mw1V7CCoxhtYCSMdsibBdDw+FBaDTxn63\nbR+bkXqBidj+r1lr16w7+9xwVt/IMtsPifjRF5Hf+mr2PaxqqIClxD4HWpaOBlRKEUFyud0eEqkp\naSh91Fu23DvVDLdVKZKVOpPTofP/uqoePA3rcRynh/hjv+MUlPkGvwLfFZH7RGTL6XDIcZzeMN/H\n/per6i4RWQV8T0R+pqp3dc6QXxS2AEhsDGPHcXrKvO78qror/78f+AZwWWCeraq6WVU3R4ajdxyn\nx8w5HEVkUESGT3wGXgM8croccxxnYZnPY/9q4Bt5QcMy8Jeq+nexBSRJzCGZrMw9gLYhazQjWWVp\nREaLZo9FpJLyYNj3ZattiadUsQtFav24aavVItLQoJ3Flk6Gl0sjI3ydvXGtabvi1/+JaXvO+RtM\n26oV4TYpie3I2GG7+GRM1h0dsLPfhvpGgtMHbBWNRmRsrelIIdTlo7YfK5bZ58ja8aXB6fuO2MOo\n7dx3IDi9FCkwOpM5B7+qbgMunuvyjuMsLv4W7jgFxYPfcQqKB7/jFBQPfscpKB78jlNQelvAE6Fc\nCm9SIvpbW8PySmxst3YkQyw6Dt6Evc50IpxNV23YEs+yiCy3etkq07ahz17nWUOjpm1kIDzOXDuS\nyTgYkcpWrxo2bf2pXUi0fehI2BApcllu2j4enzhq2iaO2XllA0NhiW243/ZjfMLOqEzKto9JasuA\nUo5kmfaF5bmpfjs8d7Wtto8K2Sfhd37HKSge/I5TUDz4HaegePA7TkHx4HecgtLT3n4FmkZvpETy\nfZtG/bNWpA5b/5Ddm7tixO7BXrdipWlbszI8BNXwsL2tJYN2Ew9W7H3WcbtO376jh03bxEA4OWb5\nsrAKAFBv2b3buybtZJslRk86QL9Rc69qFdUDUDvpRxuTpk0iQ2EdOrA/OH1oNHwsIas1aTtin3O0\nI739avfCSzu831WreCUw2hcefu1UEnv8zu84BcWD33EKige/4xQUD37HKSge/I5TUDz4Haeg9FTq\ng0jaQSwfwVAvRobsoaTOX7/atD3/vI2mbcMSWwKqGolEzSlbDhs7YCedtKu2RJXU7H07XK+btuON\nsGy0bKUtYY4st/dZI8k7aSQbq52El2uktqzYHxkqbcnScC0+gImGncQ1fmgsPH3cPmZLl9qJUxKR\nRVOx/ZiMyID1qbCMmRrHEmC435D6IsOCzcTv/I5TUDz4HaegePA7TkHx4HecguLB7zgFxYPfcQrK\nrFKfiNwEvAHYr6ovyKctA74CbAJ2AG9RVaNoWwdpSloPS0CJRGq7GRLb8lFb/lm3dLlpWxOR887d\naMuAawwJSLDln2NHw1llAOVIjbbygC317ToYHqoJYOxYeIin6kA4yw4gjahD45MTpq1Wtu8dfQPh\nzMnJKXt95aqdbVkZsY/16kE7u1AHDgWn79m911xmcsyuF1iNZJKWIjJmq2FLpkpYIkxKMf3blhW7\npZs7/83AlTOmXQPcoaoXAHfk3x3HeRYxa/Cr6l3AzATyNwG35J9vAa46zX45jrPAzPWdf7Wq7sk/\n7yUbsddxnGcR8/55r6qqiP07TxHZAmwBSErd//TQcZyFZa53/n0ishYg/2/2aqnqVlXdrKqbk1P4\n3bHjOAvLXIP/duDq/PPVwDdPjzuO4/SKbqS+LwOXAytEZCfwYeDjwG0i8g7gSeAt3WxMATWG0Uoj\nshGVsAx4qGkXdfz5oT2m7XDTlnKe2LvLtJ23fkNw+uiALVOuHI0M5dVXNW0TkcKZkbqfLF8WliNb\nLTurbPvTT5m21atsWXTNOrurJzWy32JSX3XQljeTQVuq7F9m+/i8DecEpy9ZY0uwO3fY7ZGU7MYf\nXWI/2epAWIIFKI+H2yQ2bFgq4dAtlexz8RnbnW0GVX2bYbqi6604jnPG4b/wc5yC4sHvOAXFg99x\nCooHv+MUFA9+xykoPS3gmSQJA8NDQVu7artiFbpM++1r14E+OyPqCMdN2459tnx4/46dwekXbVpn\nLvOc9XbhzLMj8ltZ7aytJDIeW6Uv3I71ll30c8SQBwHWrLf3bWiJPf6ftsNZbKvW2u2xfLVta1tV\nXIGjk/YxWzka9v+cF73E9mPTi0xbc9o+Ltqwz6vRQ0+atiUHw7LjocO2PFiqhQuTlivdh7Tf+R2n\noHjwO05B8eB3nILiwe84BcWD33EKige/4xSU3o7VlwhaC8t21Uok178alu0SO9ELGYhc1ww5DCCN\nFBLdtTMsyaxbtd5cZrweHlMNYKxuZ/UlTJm2+pQtAQ0ZhTrLib1ftZrdkNt37DZtg0P2vm0656zg\n9HM2nW+vb6mdnTfdso9n47CdAdlqh491uRzJtoxkMjYT+5hp2z5my1fZBUhH94Ul5CWRDNNaJWyr\nGhmwIfzO7zgFxYPfcQqKB7/jFBQPfscpKB78jlNQetvbr0q7FU74UGNILgDRsBJQ67NrvvXV7KST\n6pDd05vW7R7b4Wr4WiltO0EHta+vzbadfNRs2n4YzQFAuRLet6lj9vqe3mYnnWzYuNa01Y3acwB9\n5XCdxOF+e1S3lXbJOgZG7OO5dNROTKIWHratHeu1j7R9c9KuDUnd3jc5bteNrNXD7bg0Uuq+bSTI\nVZLu7+d+53ecguLB7zgFxYPfcQqKB7/jFBQPfscpKB78jlNQuhmu6ybgDcB+VX1BPu064J3AgXy2\na1X127OvC2pGYk9b7OuQ1MI2LdtJDDH1bXrMlnLSY+HaaAArl4bllVKkFfdEhg3TxPZjyVDFtK1Z\nucK0XXjBBcHpy0eXmss8tf0J07b9icdMW9sYkgtgaiIspT29LSz1AkwcsSXYVettyXFg9XNMmxg1\n7dpNcxEObP+5aTv0+D2mbaRit0dtwE6CaqRhSW+8YUvBeyfCNRkbqS2Zz6SbO//NwJWB6der6iX5\n36yB7zjOmcWswa+qdwGHe+CL4zg9ZD7v/O8WkYdE5CYRsZ8pHcc5I5lr8H8eOA+4BNgDfMqaUUS2\niMi9InJvu9X9+4jjOAvLnIJfVfepaltVU+AG4LLIvFtVdbOqbi6VXVxwnDOFOUWjiHR2vb4ZeOT0\nuOM4Tq/oRur7MnA5sEJEdgIfBi4XkUsABXYA7+pmY5pA01A82pHLUMmoS1bFlvqq0/YKK7aCwuCg\nXb9tpG84OL3etIdwGhu3h3CaKtn+D07Y/h+MrDM1avVd9NxzzGVWnm3LaOMNW/rcuWObvVw9XFdv\nYMjOptsd6VZ+6vhB07b0cPi4AGw6N3w8y5Eh2x574CHTdnTndtO2dpWdeTiw3PZR+8LL7a/bJ+rf\nPxJu+/F6JDVyBrMGv6q+LTD5xq634DjOGYm/hDtOQfHgd5yC4sHvOAXFg99xCooHv+MUlJ4W8FSB\nRjksi7XVljWqGpavksi1K7GTx6hEdrsyaGeWTRoFFVsVO2OrWbOlrSORQpzHJm3JZv/4pGl7cm/4\nJxffufNBc5la1W6PszfYQ5EN9680bQ/fH86MWzJoF7IcXLnRtB2YDmexAawetdvqilY4E3Okajf+\nk9vsIcpSbDlvOFlj2lr2IWPiWDgmntxtFwTd+diB4PTmdCSddQZ+53ecguLB7zgFxYPfcQqKB7/j\nFBQPfscpKB78jlNQejtWX5qSTIY1uMQoYghQNoowamQss2akYGJ5ZIlpk+URKWc0vE6xk/OoR8b+\nmxyz5auxA7atHilA2jgWlkzrx23tU4ysSYC7d4QlJYChEXusxCXGeIgDahd0Ser7TdvxCTtzsiY7\nTdvu/eFswJE+W4Lds9teX7NlS2nJT+0xD61CogCNenid44fsjMoBo2psrHDtTPzO7zgFxYPfcQqK\nB7/jFBQPfscpKB78jlNQetrbL6lQroeHoUqxe3MRIxmISPbOoN0TXVoeURZGIslC/WFboxkZtiq1\ne+3bSaRrNnJkpmPDZE2GM0ha03ZblSt9pq1/1K4917/M7jFfunbUsNjHef/Ofabt2KFjpm38gN0r\n/tiDYfWjhj0cGmqfH7Hi8/WGPQZYmkaGo7NqUaZ2stvoiFFPcjoyDtkM/M7vOAXFg99xCooHv+MU\nFA9+xykoHvyOU1A8+B2noHQzXNdG4AvAarLhubaq6mdFZBnwFWAT2ZBdb1FVu+gYoKmdxKCJLQGV\n+sLXqFJk3K1Sn70+qdhSWdOoFwhQnw6vs960E20mpidMW0zd1JItKqWRtmqm4e01I0XkSpH1DY2M\nmLbRVXbyVLsc9mNiMjyMF8DgEnufS5EaeMd320NvTRwJS5xTasubsUytRmrLs822LadWSva5Wu4P\nb29gmZ1kVlphJJntj4xFN4Nu7vwt4L2qehHwEuB3ReQi4BrgDlW9ALgj/+44zrOEWYNfVfeo6v35\n53HgUWA98Cbglny2W4CrFspJx3FOP6f0zi8im4BLgbuB1aq6JzftJXstcBznWULXP+8VkSHga8B7\nVHVM5Bc/gVRVFZHgy4aIbAG2ACSR4huO4/SWru78IlIhC/wvqerX88n7RGRtbl8LBMuwqOpWVd2s\nqpuTkosLjnOmMGs0SnaLvxF4VFU/3WG6Hbg6/3w18M3T757jOAtFN4/9LwPeDjwsIg/k064FPg7c\nJiLvAJ4E3jLrmhJIBsPXmyRyGerrD2dgDfbZUtNAZAitqtq7nbZs2et4KyxTtSLyT7lmy0ZJ2ba1\nI7X/pGo3VnkovN+lWqSBDakJ4Fjdzpir77N9rFTCr3hLItmW/SXbj6lpW54ttW2JsGpk78Wy7JrY\nmXGlmi2lDQ3bQ72tWGPXjRxaGV6uusyWIwdGwjLgvicOmcvMZNbgV9UfANbL+hVdb8lxnDMKfwl3\nnILiwe84BcWD33EKige/4xQUD37HKSi9LeBZEipD4aKPSdn+9V+rHL5G1SNDJzFhyzXliFxTifmh\n4eVE7GtoKTJMUysytlJT7QyxUsWWtirGEFopdoaYGLIcgBhFSwGaqS2LtiYN25Td9keO2cVOj+86\navsxba9TDQ051vZEMvBix5OIdDutEXm2FV5ubL+drcj+sOzc8AKejuPMhge/4xQUD37HKSge/I5T\nUDz4HaegePA7TkHpqdSHQrsdllHSiExiSWlqSG8ApYitYfgAIA1bvrK211RbeqtP2dlosfHzpht2\nxlxLbVuzaexb2z7UA1V7zL1YMmC1ZK+zvxbOSGuO2+1x7JjdHg1bBaTVtqXKpoYlvXZiL5OU7XH8\nWm37PC2ldjvWm/Z51TgczpxM1F6mmhht3zq9BTwdx/klxIPfcQqKB7/jFBQPfscpKB78jlNQetvb\nL0LJ6CEWu8McbYZ7bNPIQlNlu3u4VLV7bFvYtf9KFWs5+xqaROrLJW17uTTWg51GeqqNimuRXBWI\n1MdLjd5yiA8pVusLJxitHLZr+JXH7LYfV9vHdstWCdpG+7djbRgZBk4S25ZEGjkxktMAKoakUhI7\nPBOrV/8UquP7nd9xCooHv+MUFA9+xykoHvyOU1A8+B2noHjwO05BmVXqE5GNwBfIhuBWYKuqflZE\nrgPeCRzIZ71WVb89Z08MOQ/AKqsnkeSMdiRpZnJy0rSlppwH1SRsK0dktEqkiWODFjdKdpJIs89O\n+CgNhRNqyjVbRiOJJNQ07DpyU1N2Ox4bC9fck4nD5jIT++xzYHwsUu8wkowlRiNHyi5G5c1aJAmK\niAzYJlJn0BjAViID2yrGPkv3Wl83On8LeK+q3i8iw8B9IvK93Ha9qn6y6605jnPG0M1YfXuAPfnn\ncRF5FFi/0I45jrOwnNI7v4hsAi4F7s4nvVtEHhKRm0Rk6Wn2zXGcBaTr4BeRIeBrwHtUdQz4PHAe\ncAnZk8GnjOW2iMi9InJv2oz8htdxnJ7SVfCLSIUs8L+kql8HUNV9qtpW1RS4AbgstKyqblXVzaq6\nOam4uOA4ZwqzRqOICHAj8Kiqfrpj+tqO2d4MPHL63XMcZ6Hoprf/ZcDbgYdF5IF82rXA20TkEjL5\nbwfwrtlWpGlKw8ggkylbbqoYck1fZcBeJlKHTUuRYZUiQ4BNt8LyitZt38uReoElYygpiA+FJVb9\nNqA8EN7vweGwBAhQjrTj9IS9ranUbuP2RLhNpo7b8mDM1m5FhhQT+3haarBGZLnBoch5VYtk2tVs\nP/qG7XVaEmE7MqTYdD2ctZqm3b9ad9Pb/wPCiYJz1/Qdx1l0/CXccQqKB7/jFBQPfscpKB78jlNQ\nPPgdp6D0tICnpFCaMIbrsuszMtVsBqc36hPmMrVpW/IoTdkZbu3BiCMVo7liQzFFCnjGpL4kktEV\no21kLDYTWzaiGpGvItmWUrf3bWJf2I+JXfYxa03Y65NIZUoj2TJfMLzOcuQHZ30RyS6x1U00kg04\nbQzJBaDGvmnbPq/SRni/NCItz8Tv/I5TUDz4HaegePA7TkHx4HecguLB7zgFxYPfcQpKT6U+TZV0\nIizbtZu2RJEaY9o1Ixl4jdQuPFmLZEuV00ihy35jnMFI0URrbEKAcnluzT89PWXa2hpuxwGxNapy\npJrlZCTbciwiX40fGQ9Ob9Yj0mFk7MI2tgyokUw2KwGyP1LQtK/fzoBsa/j8BWi0bFs50v6W+9OT\n9vrahrysqUt9juPMgge/4xQUD37HKSge/I5TUDz4HaegePA7TkHprdSn0DDGVUsjUp9YY+RV7HHT\nqv0RKWcSDIo0AAAFyklEQVTAlnJKkbH6VIxCi7GiiWpnZlmyHBARtkAjw7FVa2FJaWDALiBZatv7\nfHRPWLIDOPj0EdOWjoX3IElt52MilUZ2OmazGitt28tMRQqytolkR8bG/4ucIxXjPJaI5FhPwwU8\nTwW/8ztOQfHgd5yC4sHvOAXFg99xCooHv+MUlFl7+0WkD7gLqOXz/5WqflhEzgFuBZYD9wFvV1W7\nm5Qs6cDq7ddob3+4Zzap2b39YPdgN6ZjySV20kxSDvso5YhCEKnFlxg98wD9w4OmbXhkiWnrGw6v\ns9m09+vwnkivvZ0fxVBi+9GuhJNSJqfsIbmyMV8tW6RQXyQxKTVqKI6P236khqoDsHT5qO1GxMWx\nCVs1mZwMH5tqZMi5vlpYCUgSe7+eMW8X80wDr1TVi8mG475SRF4CfAK4XlXPB44A7+h6q47jLDqz\nBr9mnLj+V/I/BV4J/FU+/RbgqgXx0HGcBaGrd34RKeUj9O4Hvgc8ARzVX9Qq3gmsXxgXHcdZCLoK\nflVtq+olwAbgMuB53W5ARLaIyL0icq9GftHmOE5vOaXeflU9CtwJvBQYFZETHYYbgF3GMltVdbOq\nbo5VvHEcp7fMGvwislJERvPP/cCrgUfJLgL/NJ/tauCbC+Wk4zinn24Se9YCt4hIiexicZuqfktE\nfgrcKiIfBX4M3DjbihRot8LSi0STM8KvCw1jGC+AdMp+xShHknciIzVRMQrCVaq25NiKXF6nIzXf\nJsZtaWhg0E7SWTK8PDh9fMxOMJoet4coK7VtuWk4st8TYvk/t8SemDGJDHtWMYYiq/TZp/7AoJ1Q\n04oMoTVlSHZAbLexHojLkfqPwwPDwemHkmP2hmauf7YZVPUh4NLA9G1k7/+O4zwL8V/4OU5B8eB3\nnILiwe84BcWD33EKige/4xQU6eWv7kTkAPBk/nUFcLBnG7dxP07G/TiZZ5sfZ6vqym5W2NPgP2nD\n2c99Ny/Kxt0P98P98Md+xykqHvyOU1AWM/i3LuK2O3E/Tsb9OJlfWj8W7Z3fcZzFxR/7HaegLErw\ni8iVIvIPIvK4iFyzGD7kfuwQkYdF5AERubeH271JRPaLyCMd05aJyPdE5Of5/6WL5Md1IrIrb5MH\nROT1PfBjo4jcKSI/FZGfiMjv59N72iYRP3raJiLSJyL3iMiDuR8fyaefIyJ353HzFRGJVbCdHVXt\n6R9ZWd0ngHOBKvAgcFGv/ch92QGsWITt/irwYuCRjml/AlyTf74G+MQi+XEd8L4et8da4MX552Hg\nMeCiXrdJxI+etglZAvBQ/rkC3A28BLgNeGs+/U+B35nPdhbjzn8Z8LiqbtOs1PetwJsWwY9FQ1Xv\nAg7PmPwmskKo0KOCqIYfPUdV96jq/fnncbJiMevpcZtE/OgpmrHgRXMXI/jXA093fF/M4p8KfFdE\n7hORLYvkwwlWq+qe/PNeYPUi+vJuEXkofy1Y8NePTkRkE1n9iLtZxDaZ4Qf0uE16UTS36B1+L1fV\nFwOvA35XRH51sR2C7MrPLIVtFpDPA+eRjdGwB/hUrzYsIkPA14D3qOpYp62XbRLwo+dtovMomtst\nixH8u4CNHd/N4p8Ljaruyv/vB77B4lYm2iciawHy//sXwwlV3ZefeClwAz1qExGpkAXcl1T16/nk\nnrdJyI/FapN826dcNLdbFiP4fwRckPdcVoG3Arf32gkRGRSR4ROfgdcAj8SXWlBuJyuECotYEPVE\nsOW8mR60iWRlnW8EHlXVT3eYetomlh+9bpOeFc3tVQ/mjN7M15P1pD4BfHCRfDiXTGl4EPhJL/0A\nvkz2+Ngke3d7B9mYh3cAPwe+DyxbJD/+AngYeIgs+Nb2wI+Xkz3SPwQ8kP+9vtdtEvGjp20CvIis\nKO5DZBeaD3Wcs/cAjwNfBWrz2Y7/ws9xCkrRO/wcp7B48DtOQfHgd5yC4sHvOAXFg99xCooHv+MU\nFA9+xykoHvyOU1D+H+KPfLET0oKqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3291701d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEVCAYAAAAVVdvAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXu0ZHV157/fqvvq7ttNv6DpFzYCiSIJj7khZkUdRqKC\njwBrzRCdjKKL2E4io65FoohrtJPlGnWiIvNQVxMIEI3YBhXiaASRDDETgYY0byLIu+kn3U2/b99b\nteePcy6evp6976/OrVtVTb6ftXp13d+u3++363dO7Tp1vrX3j2YGIYRIodZtB4QQRw4KGEKIZBQw\nhBDJKGAIIZJRwBBCJKOAIYRIpqcCBsmzSD7XbT96CZKfJrmd5OZu+wIAJNeQ/NoMjf1ekj+ZibF7\nBZK/T/KWafTv6hpNGTBIPkXyAMm9JDeTvJbkcCecm0lIGskTu+1HBMnjAFwK4GQzO7YL8/dsAJ/J\nwDWT85jZ183sze0ar9OkXmG8w8yGAZwG4HQAH585l0SB4wC8YGZby4wk+zrsj5hBjoTj2dJXEjPb\nDOCHyAIHAIDk20j+M8ndJJ8luaZgW5V/kl9E8pn80voTBfus/IplJ8mHAfxGcT6Sryb59yR3kXyI\n5O8WbNeS/DLJH+RXP/9I8liSX8rHe5Tk6SmvK/8U+RbJr5HcQ/IBkr9C8uMkt+av682F57+P5CP5\nc58g+YFJ432U5CaSz5P8g+LVDMlBkp/P12MLya+SnFXi0+8AuBXAsvz1XVtYz4tJPgPgx/lzfzdf\nn135er26MM5TJP+E5P0k95G8muSSfN32kPwRyQUl888B8IPC/HtJLsvNAySvz/s/RHKk0G8ZyRtJ\nbiP5JMkPBeu+iOTN+blzF4ATJtmvzNd+N8l7SL4+bz8HwOUAfi/3676pjgvJxSS/l6/RDpL/QLIW\n+ezNMxUkLyP589yPh0leULAd9pUiP54fJPkYgMcKbR/KX8N2kn8+4WvJXKVrlNvWkFzXjmP1EmYW\n/gPwFIDfyR+vAPAAgCsL9rMA/Bqy4PPrALYAOD+3rQJgAK4CMAvAqQBGAbw6t38WwD8AWAhgJYAH\nATyX2/oBPI7sgA0AeCOAPQB+NbdfC2A7gH8DYAjZm+dJAO8BUAfwaQC3B6/LAJyYP14D4CCAtwDo\nA3B9PtYncj/eD+DJQt+3ITu5CeDfAtgP4Izcdg6AzQBeA2A2gK9NmusKADfnr3kugL8F8BnHx7Mm\n1mPSel4PYE6+pr8CYB+AN+W+fjRft4HC8fspgCUAlgPYCuBeZFeKE+v2qZT5J63VW/N1/gyAn+a2\nGoB7AHwyP2avBPAEgLc4498AYF3+Wk4BsBHATwr2/wRgUX5MLs3Xdajgx9cmjRcdl88A+Gq+Rv0A\nXp8/L/TZmecyAN8Lzq3/AGBZPvbv5cdnaW5776TXaMg+GBYCmFVouz1vOw7AzwD8gdN/qjVqy7F6\nab7EgLEX2ZvVANwGYH7w/C8BuGLSCb6iYL8LwDvzx08AOKdgW41fBIzX5y++VrB/A8CaQsC4qmD7\nLwAeKfz9awB2tRAwbi3Y3pG/5nr+99z8+aWvG8B3AXw4f3wNCgEAwIkTcyE7QfcBOKFg/y0UglFi\nwHhloe2/AlhX+LuG7I13VuH4/X7BfiOAr0xat++2GDB+VPj7ZAAH8se/CeCZSc//OIC/LBm7DmAM\nwKsKbf8NhTdDSZ+dAE713shTHJc/A3DTxDEvPCf0OWWehPfQBgDn5Y/fi18OGG8sOTeL74s/AnBb\nWf+ENZr2sSr+S/1Kcr6Zzc1PoFcBWDxhIPmbJG/PL2teBPCfi/ac4h3+/QAmbpouA/BswfZ04fEy\nAM+aWXOSfXnh7y2FxwdK/m7l5uzkvtvNrFH4GxPjkTyX5E/zS9tdyCL4xGue/JqKj49GdtVxT35p\nvAvA3+XtrVAccxkK65av17OYuXUCfvl4DjH7/v0KZF9hdhVe3+XIrm4mczSyT0Xv+IPkH+dfMV7M\nxzoKv3xuFZ8fHZc/R3bldUt+qX9Z3t6Kz0mQfA/JDYXxTon8xuFrUNb2NLLjXDbXVGvUjmP1Ei3d\nZDGz/0vyWgCfB3B+3vzXAP4XgHPN7CDJLyFenCKbkH0VeSj/+7iC7XkAK0nWCkFj4vKsa5AcRPYp\n/R4AN5nZGMnvIrt6ALLXtKLQZWXh8XZkb9DXmNnGabhRTDF+HtnV1IR/zOeczvhl86TwLLKrpZMS\nnrsNwDgyXx/N2146/vl38Y8COBvAQ2bWJLkTv1jnw3yb6riY2R5kl+yXkjwFwI9J3p3gc0trQPIV\nyL6Cnw3gn8ysQXJDwe/UOSa/L54vmWuqNYpo5Vi9RJXfYXwJwJtInpr/PRfAjjxYnAngP7Yw1joA\nHye5gOQKZJfHE9yJLCJ+lGQ/ybOQfVW4oYLP7WQAwCDyE57kuQCKMtk6AO9jdsN2NrKvDABe+vS/\nCsAVJI8BAJLLSb5lGv6sA/A2kmeT7Ef2phgF8P+mMeYEWwAsInlU4vPvArCH5MeY3dCukzyF5G9M\nfmJ+9fZtAGtIziZ5MoCLCk+ZiyygbAPQR/KTAOZN8m1V4WZgeFxIvp3kiXlAfRFAA0AzwefJ80zF\nHGQBYFs+7/uQXWG0yp/k74uVAD4M4Jslz5lqjSKSj1WRlgOGmW1DdtPtk3nTHwH4M5J78rZ1LQz3\np8gut54EcAuAvyrMcwhZgDgX2SfzlwG8x8weLRmnY+SfVB9C9jp3IguQNxfsPwDwP5DdtHoc2Q1H\nIHsTA8DHJtpJ7gbwIwC/Og1//gXZja//iWyd3oFMBj9UdczC2I8iu2/0RH7ZWnpZXHh+A8Dbkalo\nT+b+/AWyy+QyLkH2dWgzsntSf1mw/RDZ17WfITtHDuLwy/Rv5f+/QPLeqY4LgJOQrfVeAP8E4Mtm\ndnuCz4fNAwAkLyf5A2cNHgbwhXyOLciu/v7Ref0RNyG7KbkBwP8BcHXJc6ZaI5cKxwoAwPxmh5gh\nmEmcDwIYNLPxbvsjeh+SBuAkM3u8275Mpqd+Gv5ygeQFzH5vsQDA5wD8rYKFeDmggDEzfADZ7x1+\njuy78h921x0h2oO+kgghktEVhhAiGQUMIUQyChhCiGQUMIQQyShgCCGSUcAQQiSjgCGESEYBQwiR\njAKGECIZBQwhRDIKGEKIZBQwhBDJKGAIIZJRwBBCJNPWnZaYbfxyJbIS8n9hZp+Nnj8wvMhmL1xZ\namPLNVMB0O8TZfFHCf6xH9541UoGhHNVM4Wvu9qAUcd2TzYTtO5jdDwre1/hwDDwIypT8eLGB7eb\nWauV6UtpW8AgWQfwv5FtqPMcgLtJ3pzXOCxl9sKVeMOlPy619ff5Fz9Es7S9GQSMMfPHazT9frXo\nlHCOUXTwopOvXvN9rNf9Q1ULXGw0G6XtYZAM1jHq2HTmAvw1CXcHjPwI3zzl50dmdGysNl70Jq45\n5ykAIFgrr18Nfp/GoVHXdvPHTnraNbZIO7+SnAngcTN7Ii9AewOA89o4vhCiy7QzYCzH4RWLn8Ph\nm+kIIY5wOn7Tk+RqkutJrj+094VOTy+EmAbtDBgbcfguXytQsvuWma01sxEzGxkYXtTG6YUQM007\nA8bdAE4ieTzJAQDvxOEbyQghjnDappKY2TjJS5DtxlQHcI2ZPRT3ARpN546wf0MYnpgQ3JNGo+nf\nzW5GqkYFpTBSGSIFpRndcaf/6pr0F2vUUXlqwWINHdrr2hq1uj9Xfcgf1A6WNtcDtQDs902BqhGd\nCe76B8clUkJiWdW3RhK6bwuUw1BRah9t/R2GmX0fwPfbOaYQonfQLz2FEMkoYAghklHAEEIko4Ah\nhEhGAUMIkUxbVZIqNK1cDmo47QBcCawRSFXNwGaRDBplWzl+hFE4kr9CP3yi+Rq18kPM8X1un33P\n3+f7MWuBP9exr3FttfFyL6PliGTyULJkID/WvJUMpNNIWw8SFy14BdUyTyPptDOf/brCEEIko4Ah\nhEhGAUMIkYwChhAiGQUMIUQy3VVJSNApPVer+7HMu/sc54kFCkqYx+Tf6a45t/irlr+L+kVJa6Gc\n4CVwNfe4XcZe2ODaGsNLXVvfkle5tjoGStu9cosAEAllYRXBMDHN7RXYqiWRRS/AohfnjBnPJZVE\nCNFjKGAIIZJRwBBCJKOAIYRIRgFDCJGMAoYQIpmuJ595STi1YBcwX2EMEogCHS5S4Wr0a1h6PsY1\nPf25IqJuDQt2RXNky76x3W6f/fu3urZm3a+zWbf9rq3BueV9grWqh3JmROufg9FxiZLPwgSzoA5r\nVJOUrs4fOunb2oiuMIQQyShgCCGSUcAQQiSjgCGESEYBQwiRjAKGECKZtsuqJJ8CsAdAA8C4mY24\nTzYDrH2Zp2H+XyRjBf2irELz/IhqSoalSqNtFH2awaD9PFTeZ7cvne7f6Uuug/X5rq12yM+AHR2a\nV9peD7Isw5MzksmDbv5WiVGvYPU57vcKNfQoC9qxOe8VALBAwm0nM/U7jH9nZttnaGwhRJfQVxIh\nRDIzETAMwC0k7yG5egbGF0J0iZn4SvI6M9tI8hgAt5J81MzumDDmQWQ1AAwtWDED0wshZoq2X2GY\n2cb8/60AvgPgzEn2tWY2YmYjA3MWtXt6IcQM0taAQXIOmWUZkZwD4M0AHmznHEKI7tHuryRLAHwn\nz9bsA/DXZvZ33pNJupmdFlbmLbdF2xqGtrj6rmsK6gNXIiz0G/hfDxTBPo6Vtu/b64tY4/t2uLah\nuf5VYX/Tlxh9KTw6ZtWIj3WFcyc6LlECqW8KcYv9BnK9J/G3m7YGDDN7AsCp7RxTCNE7SFYVQiSj\ngCGESEYBQwiRjAKGECIZBQwhRDLdLwLsFdKttS4T1QKJqxH2DDJZg2LEnvxlUVZhpNAFxjCXsh4J\nkOVSZ60x6vboaxzw/Rjd69r6m+USLgDUHO330HhUgbna51kzkOQ9+TSSVUMCqTMins872oGOX+vM\nW1lXGEKIZBQwhBDJKGAIIZJRwBBCJKOAIYRIpusqiUe03aB3gznqUwtqesbqRBWVpFoSWUT02hrB\nkOM2UNo+MDjL7TPQ558WY6PBdojjB11bzZOwosMc2JpB5l+oQLhjBpOF9UOjcyc6H4PPaivX9KLx\nvD7tRlcYQohkFDCEEMkoYAghklHAEEIko4AhhEhGAUMIkUzXZVUvx6weSFn+FnSBxBUoUvVoa8NI\nVnUGDZOfwhqhUZHQYD3oH0ar9Ze2zxle7PbZN3uOa9t9yE9MGw9k1YG6syXmePSag8+zIDkxkh8r\nbrBYqVdUazWS+WtNr/5pJCV3ZqtEXWEIIZJRwBBCJKOAIYRIRgFDCJGMAoYQIhkFDCFEMpVkVZLX\nAHg7gK1mdkrethDANwGsAvAUgAvNbGc4DoCaIwfVguw7L0OwFkhtka0Zaa6BcOYppKyaGRv1izI3\n+4JtFJ36ooNzlrl9+hcf69r43BO+Iwf8TFbP/Rp9OTAu6xodl9YzgqOs00g7rQXnTl9UDzZ43V62\nbXB6VK4t2ipVZ7kWwDmT2i4DcJuZnQTgtvxvIcTLiEoBw8zuADB5x97zAFyXP74OwPnT8EsI0YO0\n8zpmiZltyh9vRraTuxDiZcSMfPGx7Etk6TcukqtJrie5fnTv9pmYXggxQ7QzYGwhuRQA8v+3lj3J\nzNaa2YiZjQwG+QxCiN6jnQHjZgAX5Y8vAnBTG8cWQvQAVWXVbwA4C8Biks8B+BSAzwJYR/JiAE8D\nuDBlLE86i5ROT8qyQIezqOZqtH2hb3Llu6pFgGvR1oCRfFf3+9Ub5fJd3+yj3D5zj17h2nZv9mXV\nxoHdro2OvBtlCkeSZbQelWTVUFqPpPAg/7XakJXOq0aY6dw+KgUMM3uXYzp7Gr4IIXoc/dJTCJGM\nAoYQIhkFDCFEMgoYQohkFDCEEMl0tQgwOYWUGHUsba6mY4WFeQNbFfkuer2R/xakTPYHqaxeFuah\net3tM2uen8k6PHu2a2uMveD7YeOl7bX6kNsnysAME1nDUr/eueOPFiaJRudVeM5VmC9Mqu7tbFUh\nxL9CFDCEEMkoYAghklHAEEIko4AhhEhGAUMIkUzX91aNpdByvCzGuJhvtSKvof7lUEkqnmKuaJ3q\ngazq7e85Wi/fcxUAZs062rUtXDDftb3Y3OXa0Bgtba71D/tdwv1Co713/X5er/A8jKT1cN/V6JwL\npHzPlUiKbf1tVAldYQghklHAEEIko4AhhEhGAUMIkYwChhAima6rJB61mp8c5W172AxvIwdzhQVE\n/Zjqb5VY7ZZ1M6jLGI3YV0ElGev3VZLagK+ELJzv2/aN+1slNsfLVRIO+sfZArUjonVda6rEv2Cu\nSNmK5ovG9BSUirVn24muMIQQyShgCCGSUcAQQiSjgCGESEYBQwiRjAKGECKZlmVVktcAeDuArWZ2\nSt62BsD7AWzLn3a5mX0/YTTUHTmrv8+X2+jIqo1mw+1Ti/KAAkktiqjNClpWLdjWsOlmHQHRTniR\nRNdwDrHRH7A55Ps4NHfQtc17cZ9rO3Boj9Mp2ObR/NOT5h9rC/bFbDpyfZQoxrAAZ+v1Q6e2Occm\nSFizDmWfVbnCuBbAOSXtV5jZafm/hGAhhDjSaDlgmNkdAHbMgC9CiB6nnfcwLiF5P8lrSC5o47hC\niB6hXQHjKwBOAHAagE0AvuA9keRqkutJrh/du71N0wshOkFbAoaZbTGzhmU//r8KwJnBc9ea2YiZ\njQwOL27H9EKIDtGWgEFyaeHPCwA82I5xhRC9RRVZ9RsAzgKwmORzAD4F4CySpyFLmnsKwAfSxgL6\nHIk0rHnoZTEG2Y1RZIyyIiO1ypNjo20NIwk32u0uzm4M/PckxuCFWZ/vSN+gL6sePbDbtW06uKm0\nvdE43u3Dmp9RG9X7tOBoN52VrAXj1cLanNGxDrr5JtRcWTjwo9mZn1S1HDDM7F0lzVe3wRchRI+j\nX3oKIZJRwBBCJKOAIYRIRgFDCJGMAoYQIpmuFwE2T5oM0jO97ekiqaoeaJaNSFYNUlI9iTQsKNuo\nVsk1rCtcq5Kl60uWfZzr2gb7j3JtK44dd21btz1Q2r5rzPdjwcpTXNsYZ7s2g5/pXHeyXBllglbc\nRjEuAhz0c35qELgIeD9PaDO6whBCJKOAIYRIRgFDCJGMAoYQIhkFDCFEMgoYQohkuiqrGoCGI0tF\nMlfNkUgjZSncAzOQXJth1qwzl+/GVBt1uiYGsT2Ukz0JOipeW/MlywZnubbF8+a4toWbt5W2P/bo\n37t95i86xrVxrp/lWgvWqm5jrs2jGYwXZbJGxYNjFdT72UCku3dmd1VdYQghklHAEEIko4AhhEhG\nAUMIkYwChhAime4mn5lh3EkGqodb0JUni0VKQj2KjcEd60ZwZ9pTXqJt8KJEpmhHvmZkbPjJVn6t\nSl8taNT9ufYH63jgkN/v2OFyBWVw3zNun7E95XVAAcAClaQZnDuDzjo2A9kiOmbBUoWfxnFimuNH\npMjE2lzb0BWGECIZBQwhRDIKGEKIZBQwhBDJKGAIIZJRwBBCJFNlq8SVAK4HsARZlsxaM7uS5EIA\n3wSwCtl2iRea2c5oLEMgFwY1PT3hifVqe9NFimVUzrHubpXoE8m0oSOhyZdVXejLqs1AKzxU90+Z\n/UFe11FzymuBDjYPuH1Gd+9wbYPLqkmMNe/ciQ508LFaD07Terhlpt+PzlaJzaC+bGdE1WpXGOMA\nLjWzkwG8FsAHSZ4M4DIAt5nZSQBuy/8WQryMaDlgmNkmM7s3f7wHwCMAlgM4D8B1+dOuA3B+u5wU\nQvQG07qHQXIVgNMB3AlgiZlN/DRvM7KvLEKIlxGVAwbJYQA3AviIme0u2iz7zXTp1yqSq0muJ7l+\ndO/2qtMLIbpApYBBsh9ZsPi6mX07b95CcmluXwpga1lfM1trZiNmNjI4vLjK9EKILtFywGB2O/lq\nAI+Y2RcLppsBXJQ/vgjATdN3TwjRS1TJVv1tAO8G8ADJDXnb5QA+C2AdyYsBPA3gwpTBrFkesyzM\nHnTaA1kyqs0ZSWr1elBL05HUAvUr3HoxSLJEJJw1gszTZq3cyb7gs6I/qNtpDf+UOTDm+zF3eKi0\nfXDAl4SbB/3xBuBvyxid1X2NgdL2MMM4WN++SMINasXWovPbGTLaZbNTP6lqOWCY2U/g/6rh7Om5\nI4ToZfRLTyFEMgoYQohkFDCEEMkoYAghklHAEEIk09UiwATcPMtI5vK2pwsLoUYZqZE05mQO5h1b\nac6NgZzWLC+InBmDflGyquO+Nf3xGkEG5sFDvsS4e3SvaxuaUy6rzhrsd/uMjvtz9VfY8hAA6iyX\nVaOjFmVB90XSabTFYpQ97Z1XUR3o+KxrG7rCEEIko4AhhEhGAUMIkYwChhAiGQUMIUQyChhCiGS6\nu7cqAE+xqgUakidJVawBDAYZpIQvddZcibRa8dcoW9WTkrMxI2nPmct87bTh7HcLAM1A6tyxe5dr\nG5g1WNpe7/dkTiBax3qQmcyab+uvsLdqPSh83B995FYs2ut50nT3yY1l2naiKwwhRDIKGEKIZBQw\nhBDJKGAIIZJRwBBCJNPl5DOi37m96ysQfrJYX6hOBHesozvugZrgxtvA96oRuuoWi95WfowkmWC8\nA/sPurYd2/1tI0ZHD5W2z1pwnNtn8JhVrg31ctUFiNWyAecABLl4U5wfVbe+DOrPetuEBlmBjBIX\n24iuMIQQyShgCCGSUcAQQiSjgCGESEYBQwiRjAKGECKZlmVVkisBXA9gCbIcmrVmdiXJNQDeD2Bb\n/tTLzez74VgwN4koSqbxTGHCWuBHM0wFivzw+kWSWTWJLvLf27IRAPo8WTUYbyzY8nDvzp2u7eDG\n511b/6wXS9tXjJzh9pmz5Hjfj1BWDRIGnfZIPh8LEu4i0b1e849alPznjRqd3/UOZZ9V+R3GOIBL\nzexeknMB3EPy1tx2hZl9vn3uCSF6iSp7q24CsCl/vIfkIwCWt9sxIUTvMa17GCRXATgdwJ150yUk\n7yd5DckF0/RNCNFjVA4YJIcB3AjgI2a2G8BXAJwA4DRkVyBfcPqtJrme5PqDe1+oOr0QogtUChgk\n+5EFi6+b2bcBwMy2mFnDsrs5VwE4s6yvma01sxEzGxkaXlTVbyFEF2g5YJAkgKsBPGJmXyy0Ly08\n7QIAD07fPSFEL1FFJfltAO8G8ADJDXnb5QDeRfI0ZJriUwA+kDJY3akfGYlEXpQLtzWsKJ1atEWh\nN2Sgc3qJiNmAQTZitNUj/b0SG7VyWz0QBA9se9i17d74kGsb2+dLrt7yzw+yXwfHxl1bX3+wP2R0\nzOhIpMH61oPzqj+S/xvVJPS6Y4oSjKNase2kikryE5S/n8PfXAghjnz0S08hRDIKGEKIZBQwhBDJ\nKGAIIZJRwBBCJNPlIsCGmpeZF8QyN1s10MbCrekCvaoZyF/VhNqAcMtGnzEL5MeaY3vxcbfP9od/\n6M+161nfEfqn0+jB0dL2g/t3u31qzQOura/pv2arRad1eb+oXm8fZ7m2aMvGZsP3MdzW08lyjTJc\n3cLBbUZXGEKIZBQwhBDJKGAIIZJRwBBCJKOAIYRIRgFDCJFMV2VVGGDefpGRjujYLJAlQ0I5s8Ie\npJHvUdZpII1ZJBl7G6gCqB3cUdq+64Gb3D57H7vbH2/AP2X65/iVGnmwXCJtjPuva9yRQAGgaX6W\nKxsDQb/ytWIgxUYqbZRFHBUBDkyufDoe7J9q2ltVCNFrKGAIIZJRwBBCJKOAIYRIRgFDCJGMAoYQ\nIpnuyqqAK2kykjod2ZJVawAHcmYkq7p+hIWDIwk3yoz1bUNB6uP+beV7v2x8+hm3z+y5x7i2+cuO\nc23HLj/Rte3esau0fc/Q0W4f1PwsUTN/v1M2AjnTOeXpFEvO5goyQcPCzUF2aVAg2JxM3Eg6pVNM\nu93oCkMIkYwChhAiGQUMIUQyChhCiGQUMIQQybSskpAcAnAHgMG8/9+Y2adIHg/gBgCLANwD4N1m\ndmjK8Zz2ZnRH2GkPt0oM9pKLtq2L5BVPQYnVjoAokymqOTnuKwY2NFzafuwZF7p9li3y1Yl5s4PE\nrkBMmLNidmn78KFy/wCgXl/g2izYHjI6D+pW3s9CNSxI7ouOdbAgzagmqaN4RO+JaLx2UuUKYxTA\nG83sVACnATiH5GsBfA7AFWZ2IoCdAC5un5tCiF6g5YBhGXvzP/vzfwbgjQD+Jm+/DsD5bfFQCNEz\nVLqHQbKe79y+FcCtAH4OYJfZS/XunwPgF0cQQhyRVAoYZtYws9MArABwJoBXpfYluZrkepLrD+4t\n/xWiEKI3mZZKYma7ANwO4LcAzCdf2slmBYCNTp+1ZjZiZiNDw4umM70QosO0HDBIHk1yfv54FoA3\nAXgEWeD49/nTLgLg14ATQhyRVEk+WwrgOpJ1ZAFnnZl9j+TDAG4g+WkA/wzg6qkGMjM0x8vloEA1\n88cLJK5aldqc+agenmgWKLixvBt1C6xBSU/Mm1ee3DVr4fFun8G6r4b3Nfa7tgMH9rq28b75pe2z\nZ89z+xxCoMoHiV1RbVdPPo0U7Vp0flSUTqNtFD351EtKAwAbn/IXDG2h5YBhZvcDOL2k/Qlk9zOE\nEC9T9EtPIUQyChhCiGQUMIQQyShgCCGSUcAQQiTDqMbkjE9ObgPwdP7nYgDbu+bML5AfhyM/DudI\n9OMVZhYUT02nqwGjCMn1ZjYiP+SH/OhdP/SVRAiRjAKGECKZXgoYa7vtQI78OBz5cTj/qv3omXsY\nQojep5euMIQQPU5PBAyS55D8F5KPk7ysi348RfIBkhtIru/gvNeQ3ErywULbQpK3knws/9+viDuz\nfqwhuTFfkw0k39oBP1aSvJ3kwyQfIvnhvL2jaxL40dE1ITlE8i6S9+V+/GnefjzJO/P3zTdJ+hWa\n24WZdfUfgDqyEn+vBDAA4D4AJ3fJl6cALO7CvG8AcAaABwtt/x3AZfnjywB8rkt+rAHwxx1ej6UA\nzsgfzwXwMwAnd3pNAj86uibIKh8M54/7AdwJ4LUA1gF4Z97+VQB/ONO+9MIVxpkAHjezJyzbluAG\nAOd12ac7rC8UAAAB60lEQVSOYmZ3ANgxqfk8ZMWUgQ4VVXb86DhmtsnM7s0f70FWoGk5OrwmgR8d\nxTJ6ovB2LwSM5QCeLfzdzQLCBuAWkveQXN0lHyZYYmab8sebASzpoi+XkLw//8oy41+NipBchaz+\nyp3o4ppM8gPo8Jr0SuHtXggYvcTrzOwMAOcC+CDJN3TbISD7hMEUeyDNIF8BcAKyPWg2AfhCpyYm\nOQzgRgAfMbPdRVsn16TEj46viU2j8HY76YWAsRHAysLfbgHhmcbMNub/bwXwHXS3gtgWkksBIP9/\nazecMLMt+cnaBHAVOrQmJPuRvUm/bmbfzps7viZlfnRrTfK5Wy683U56IWDcDeCk/I7vAIB3Ari5\n006QnENy7sRjAG8G8GDca0a5GVkxZaCLRZUn3qA5F6ADa0KSyGrCPmJmXyyYOromnh+dXpOeKrzd\nqTu9U9wFfiuyO9A/B/CJLvnwSmQKzX0AHuqkHwC+gezSdgzZd9GLke1RexuAxwD8CMDCLvnxVwAe\nAHA/sjfs0g748TpkXzfuB7Ah//fWTq9J4EdH1wTAryMrrH0/suD0ycI5exeAxwF8C8DgTB8b/dJT\nCJFML3wlEUIcIShgCCGSUcAQQiSjgCGESEYBQwiRjAKGECIZBQwhRDIKGEKIZP4/SbRnU9hNzQ8A\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2e32f0dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOeX/A8O+gkgayCDnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2e30e3790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypefloat32\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[ 0.12156863  0.09803922  0.09803922]\n",
      " [ 0.11372549  0.11764706  0.10980392]\n",
      " [ 0.14117648  0.12941177  0.1254902 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkSgKfiESRcEvRKIo+IVIlP8HYkMVTHTx2CMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2e30815d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_ = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(inp):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, tf.stack([batch_size, 4, 4, 16]), \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, tf.stack([batch_size, 8, 8, 8]),\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, tf.stack([batch_size, 16, 16, 4]),\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.stack([batch_size, 32, 32, 3]),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(128, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(128, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../Models/IDEA_1/Model_cifar_1/model_cifar_1-100\n",
      "epoch: 101\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35168915987\n",
      "range:(9600, 9728) loss= 0.350566923618\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335323512554\n",
      "range:(9600, 9728) loss= 0.35751748085\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351193130016\n",
      "range:(9600, 9728) loss= 0.366610705853\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.365054607391\n",
      "range:(9600, 9728) loss= 0.355799466372\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362801641226\n",
      "range:(9600, 9728) loss= 0.354630589485\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 102\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350748181343\n",
      "range:(9600, 9728) loss= 0.350144088268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335747301579\n",
      "range:(9600, 9728) loss= 0.357336372137\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351291507483\n",
      "range:(9600, 9728) loss= 0.365745425224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36628049612\n",
      "range:(9600, 9728) loss= 0.355475753546\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362855494022\n",
      "range:(9600, 9728) loss= 0.353996157646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 103\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351281285286\n",
      "range:(9600, 9728) loss= 0.351919710636\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.339179575443\n",
      "range:(9600, 9728) loss= 0.357682585716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351725280285\n",
      "range:(9600, 9728) loss= 0.365127444267\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.365921497345\n",
      "range:(9600, 9728) loss= 0.355698466301\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362640351057\n",
      "range:(9600, 9728) loss= 0.354833602905\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 104\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35320854187\n",
      "range:(9600, 9728) loss= 0.351248651743\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336518406868\n",
      "range:(9600, 9728) loss= 0.359227657318\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.353379607201\n",
      "range:(9600, 9728) loss= 0.365705132484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364394783974\n",
      "range:(9600, 9728) loss= 0.355155348778\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362881928682\n",
      "range:(9600, 9728) loss= 0.354277282953\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 105\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35248208046\n",
      "range:(9600, 9728) loss= 0.350396215916\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335155814886\n",
      "range:(9600, 9728) loss= 0.356283426285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351089954376\n",
      "range:(9600, 9728) loss= 0.365144729614\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36625701189\n",
      "range:(9600, 9728) loss= 0.356336414814\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.363258063793\n",
      "range:(9600, 9728) loss= 0.354401528835\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 106\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350002169609\n",
      "range:(9600, 9728) loss= 0.351218193769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335711121559\n",
      "range:(9600, 9728) loss= 0.357357144356\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350613951683\n",
      "range:(9600, 9728) loss= 0.365416616201\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364820599556\n",
      "range:(9600, 9728) loss= 0.356501042843\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.363852441311\n",
      "range:(9600, 9728) loss= 0.35404920578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 107\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350737154484\n",
      "range:(9600, 9728) loss= 0.350411534309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334890604019\n",
      "range:(9600, 9728) loss= 0.356738865376\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.352401793003\n",
      "range:(9600, 9728) loss= 0.36473441124\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364404708147\n",
      "range:(9600, 9728) loss= 0.355266869068\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361817955971\n",
      "range:(9600, 9728) loss= 0.354664027691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 108\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351493954659\n",
      "range:(9600, 9728) loss= 0.350569486618\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335938513279\n",
      "range:(9600, 9728) loss= 0.356427431107\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.35019659996\n",
      "range:(9600, 9728) loss= 0.36412280798\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364053875208\n",
      "range:(9600, 9728) loss= 0.354966342449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36170604825\n",
      "range:(9600, 9728) loss= 0.353969663382\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 109\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352528601885\n",
      "range:(9600, 9728) loss= 0.351434051991\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336207509041\n",
      "range:(9600, 9728) loss= 0.355580806732\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350091427565\n",
      "range:(9600, 9728) loss= 0.36477714777\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.366558670998\n",
      "range:(9600, 9728) loss= 0.355239450932\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36292052269\n",
      "range:(9600, 9728) loss= 0.352601081133\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 110\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350351661444\n",
      "range:(9600, 9728) loss= 0.351349115372\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336718380451\n",
      "range:(9600, 9728) loss= 0.355659782887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.3517819345\n",
      "range:(9600, 9728) loss= 0.366092026234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364694237709\n",
      "range:(9600, 9728) loss= 0.35584166646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.363042354584\n",
      "range:(9600, 9728) loss= 0.354493439198\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 111\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351904153824\n",
      "range:(9600, 9728) loss= 0.350962311029\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335074901581\n",
      "range:(9600, 9728) loss= 0.355663180351\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350512981415\n",
      "range:(9600, 9728) loss= 0.364713847637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364131331444\n",
      "range:(9600, 9728) loss= 0.355893105268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362100660801\n",
      "range:(9600, 9728) loss= 0.353686988354\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 112\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350899934769\n",
      "range:(9600, 9728) loss= 0.350491464138\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335118412971\n",
      "range:(9600, 9728) loss= 0.355060517788\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349846303463\n",
      "range:(9600, 9728) loss= 0.363249361515\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363440454006\n",
      "range:(9600, 9728) loss= 0.355056643486\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361064732075\n",
      "range:(9600, 9728) loss= 0.351937264204\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 113\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351373970509\n",
      "range:(9600, 9728) loss= 0.349954426289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334540843964\n",
      "range:(9600, 9728) loss= 0.35566085577\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350499987602\n",
      "range:(9600, 9728) loss= 0.364096462727\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363833665848\n",
      "range:(9600, 9728) loss= 0.355246961117\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362759590149\n",
      "range:(9600, 9728) loss= 0.353444248438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 114\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351295590401\n",
      "range:(9600, 9728) loss= 0.350566864014\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.337358564138\n",
      "range:(9600, 9728) loss= 0.354345232248\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349459916353\n",
      "range:(9600, 9728) loss= 0.363264381886\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36431774497\n",
      "range:(9600, 9728) loss= 0.355174869299\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361288189888\n",
      "range:(9600, 9728) loss= 0.354020476341\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 115\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.353411734104\n",
      "range:(9600, 9728) loss= 0.349637210369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335331350565\n",
      "range:(9600, 9728) loss= 0.354843646288\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349977731705\n",
      "range:(9600, 9728) loss= 0.368648946285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.366655141115\n",
      "range:(9600, 9728) loss= 0.354198157787\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361975342035\n",
      "range:(9600, 9728) loss= 0.353176593781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 116\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35046055913\n",
      "range:(9600, 9728) loss= 0.350993037224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334702402353\n",
      "range:(9600, 9728) loss= 0.354449301958\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350578308105\n",
      "range:(9600, 9728) loss= 0.365842521191\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.365820109844\n",
      "range:(9600, 9728) loss= 0.355060070753\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.3632183671\n",
      "range:(9600, 9728) loss= 0.354666650295\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 117\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.353937387466\n",
      "range:(9600, 9728) loss= 0.352936416864\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.338233649731\n",
      "range:(9600, 9728) loss= 0.356685459614\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349977374077\n",
      "range:(9600, 9728) loss= 0.362789094448\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362947285175\n",
      "range:(9600, 9728) loss= 0.354763090611\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360983431339\n",
      "range:(9600, 9728) loss= 0.352202057838\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 118\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350243538618\n",
      "range:(9600, 9728) loss= 0.350141167641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334279179573\n",
      "range:(9600, 9728) loss= 0.355621635914\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350716531277\n",
      "range:(9600, 9728) loss= 0.363662719727\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363171219826\n",
      "range:(9600, 9728) loss= 0.354385972023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361186593771\n",
      "range:(9600, 9728) loss= 0.353115260601\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 119\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351681411266\n",
      "range:(9600, 9728) loss= 0.350518226624\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334640860558\n",
      "range:(9600, 9728) loss= 0.35576608777\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349973201752\n",
      "range:(9600, 9728) loss= 0.367043673992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.366285979748\n",
      "range:(9600, 9728) loss= 0.354832082987\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360100865364\n",
      "range:(9600, 9728) loss= 0.353271067142\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 120\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350746929646\n",
      "range:(9600, 9728) loss= 0.350566774607\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334375649691\n",
      "range:(9600, 9728) loss= 0.353416502476\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350008934736\n",
      "range:(9600, 9728) loss= 0.365181803703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36281567812\n",
      "range:(9600, 9728) loss= 0.354393601418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36011838913\n",
      "range:(9600, 9728) loss= 0.35255086422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 121\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350849568844\n",
      "range:(9600, 9728) loss= 0.350241661072\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334371566772\n",
      "range:(9600, 9728) loss= 0.355156362057\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349655210972\n",
      "range:(9600, 9728) loss= 0.364474654198\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362947642803\n",
      "range:(9600, 9728) loss= 0.355330705643\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360449790955\n",
      "range:(9600, 9728) loss= 0.351856052876\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 122\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351976543665\n",
      "range:(9600, 9728) loss= 0.350192427635\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335616528988\n",
      "range:(9600, 9728) loss= 0.353311240673\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349180459976\n",
      "range:(9600, 9728) loss= 0.363153159618\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363156765699\n",
      "range:(9600, 9728) loss= 0.354688763618\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360492169857\n",
      "range:(9600, 9728) loss= 0.353666007519\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 123\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.354151189327\n",
      "range:(9600, 9728) loss= 0.351862668991\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.338033467531\n",
      "range:(9600, 9728) loss= 0.354431003332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351010262966\n",
      "range:(9600, 9728) loss= 0.364035159349\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36450368166\n",
      "range:(9600, 9728) loss= 0.354241669178\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36263269186\n",
      "range:(9600, 9728) loss= 0.353484928608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 124\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352221250534\n",
      "range:(9600, 9728) loss= 0.351159989834\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334783613682\n",
      "range:(9600, 9728) loss= 0.354517191648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349749743938\n",
      "range:(9600, 9728) loss= 0.361661195755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361531436443\n",
      "range:(9600, 9728) loss= 0.354195475578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358704388142\n",
      "range:(9600, 9728) loss= 0.35233527422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 125\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350714206696\n",
      "range:(9600, 9728) loss= 0.349238634109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333618044853\n",
      "range:(9600, 9728) loss= 0.355760157108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351555466652\n",
      "range:(9600, 9728) loss= 0.362232387066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362050294876\n",
      "range:(9600, 9728) loss= 0.353676021099\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359054744244\n",
      "range:(9600, 9728) loss= 0.352012276649\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 126\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350652813911\n",
      "range:(9600, 9728) loss= 0.35007494688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.337326228619\n",
      "range:(9600, 9728) loss= 0.355911135674\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.3498480618\n",
      "range:(9600, 9728) loss= 0.364054501057\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362230092287\n",
      "range:(9600, 9728) loss= 0.353666335344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359006017447\n",
      "range:(9600, 9728) loss= 0.352886170149\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 127\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351738452911\n",
      "range:(9600, 9728) loss= 0.349737644196\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334697961807\n",
      "range:(9600, 9728) loss= 0.354291647673\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350423961878\n",
      "range:(9600, 9728) loss= 0.363504767418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362967222929\n",
      "range:(9600, 9728) loss= 0.35441082716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360044121742\n",
      "range:(9600, 9728) loss= 0.353217214346\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 128\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350678920746\n",
      "range:(9600, 9728) loss= 0.349837124348\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335429161787\n",
      "range:(9600, 9728) loss= 0.353707492352\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350191771984\n",
      "range:(9600, 9728) loss= 0.363619863987\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363651633263\n",
      "range:(9600, 9728) loss= 0.353152692318\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359110891819\n",
      "range:(9600, 9728) loss= 0.352849155664\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 129\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352979570627\n",
      "range:(9600, 9728) loss= 0.350927621126\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.337064564228\n",
      "range:(9600, 9728) loss= 0.355980187654\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351370513439\n",
      "range:(9600, 9728) loss= 0.361648857594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361296504736\n",
      "range:(9600, 9728) loss= 0.353324860334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35848402977\n",
      "range:(9600, 9728) loss= 0.350782632828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 130\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350726425648\n",
      "range:(9600, 9728) loss= 0.353562057018\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.340251803398\n",
      "range:(9600, 9728) loss= 0.356707245111\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351448982954\n",
      "range:(9600, 9728) loss= 0.361964464188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36222910881\n",
      "range:(9600, 9728) loss= 0.354617148638\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360896348953\n",
      "range:(9600, 9728) loss= 0.352744221687\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 131\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350054115057\n",
      "range:(9600, 9728) loss= 0.352924108505\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.337287604809\n",
      "range:(9600, 9728) loss= 0.354991018772\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351384609938\n",
      "range:(9600, 9728) loss= 0.361338436604\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361147999763\n",
      "range:(9600, 9728) loss= 0.354574143887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360294878483\n",
      "range:(9600, 9728) loss= 0.351448118687\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 132\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350064992905\n",
      "range:(9600, 9728) loss= 0.350211590528\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336050361395\n",
      "range:(9600, 9728) loss= 0.352846503258\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34982329607\n",
      "range:(9600, 9728) loss= 0.362987965345\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362382620573\n",
      "range:(9600, 9728) loss= 0.354225039482\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360653162003\n",
      "range:(9600, 9728) loss= 0.351589173079\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 133\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352992713451\n",
      "range:(9600, 9728) loss= 0.349234253168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333325862885\n",
      "range:(9600, 9728) loss= 0.354008615017\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348938137293\n",
      "range:(9600, 9728) loss= 0.361170649529\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361986786127\n",
      "range:(9600, 9728) loss= 0.354647517204\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358430504799\n",
      "range:(9600, 9728) loss= 0.352462947369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 134\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352655112743\n",
      "range:(9600, 9728) loss= 0.348752945662\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334072619677\n",
      "range:(9600, 9728) loss= 0.353762924671\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350213438272\n",
      "range:(9600, 9728) loss= 0.363264024258\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363793075085\n",
      "range:(9600, 9728) loss= 0.353452414274\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358813673258\n",
      "range:(9600, 9728) loss= 0.351260840893\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 135\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352230846882\n",
      "range:(9600, 9728) loss= 0.349431216717\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333907783031\n",
      "range:(9600, 9728) loss= 0.354391843081\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351252406836\n",
      "range:(9600, 9728) loss= 0.361498653889\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362156718969\n",
      "range:(9600, 9728) loss= 0.354317337275\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358675301075\n",
      "range:(9600, 9728) loss= 0.351640820503\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 136\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35230410099\n",
      "range:(9600, 9728) loss= 0.34834086895\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332760453224\n",
      "range:(9600, 9728) loss= 0.353266388178\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34844905138\n",
      "range:(9600, 9728) loss= 0.361401379108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360945045948\n",
      "range:(9600, 9728) loss= 0.353810936213\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358955800533\n",
      "range:(9600, 9728) loss= 0.35231384635\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 137\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351461142302\n",
      "range:(9600, 9728) loss= 0.352121829987\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.337198257446\n",
      "range:(9600, 9728) loss= 0.352306097746\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349920809269\n",
      "range:(9600, 9728) loss= 0.362968176603\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362815380096\n",
      "range:(9600, 9728) loss= 0.35398709774\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359938323498\n",
      "range:(9600, 9728) loss= 0.351641565561\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 138\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351735591888\n",
      "range:(9600, 9728) loss= 0.34995162487\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334298074245\n",
      "range:(9600, 9728) loss= 0.35503077507\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350019395351\n",
      "range:(9600, 9728) loss= 0.365563839674\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364906013012\n",
      "range:(9600, 9728) loss= 0.353954017162\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360242664814\n",
      "range:(9600, 9728) loss= 0.351095199585\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 139\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350465655327\n",
      "range:(9600, 9728) loss= 0.350014507771\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33307531476\n",
      "range:(9600, 9728) loss= 0.354238688946\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349689245224\n",
      "range:(9600, 9728) loss= 0.362726390362\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36274176836\n",
      "range:(9600, 9728) loss= 0.353271245956\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359861403704\n",
      "range:(9600, 9728) loss= 0.352604269981\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 140\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349723130465\n",
      "range:(9600, 9728) loss= 0.349462985992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334842145443\n",
      "range:(9600, 9728) loss= 0.356077849865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350071966648\n",
      "range:(9600, 9728) loss= 0.364526927471\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363156735897\n",
      "range:(9600, 9728) loss= 0.354349732399\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358924180269\n",
      "range:(9600, 9728) loss= 0.352891892195\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 141\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351540327072\n",
      "range:(9600, 9728) loss= 0.348744213581\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33362019062\n",
      "range:(9600, 9728) loss= 0.353059262037\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348628520966\n",
      "range:(9600, 9728) loss= 0.362160235643\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363310337067\n",
      "range:(9600, 9728) loss= 0.353276252747\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359067618847\n",
      "range:(9600, 9728) loss= 0.355430722237\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 142\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352806895971\n",
      "range:(9600, 9728) loss= 0.350922107697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335801899433\n",
      "range:(9600, 9728) loss= 0.353611111641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34878847003\n",
      "range:(9600, 9728) loss= 0.361420571804\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361361354589\n",
      "range:(9600, 9728) loss= 0.353701263666\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361077845097\n",
      "range:(9600, 9728) loss= 0.354370653629\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 143\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351493775845\n",
      "range:(9600, 9728) loss= 0.348703920841\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333981335163\n",
      "range:(9600, 9728) loss= 0.355333477259\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34976914525\n",
      "range:(9600, 9728) loss= 0.366707831621\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364792436361\n",
      "range:(9600, 9728) loss= 0.353573381901\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359693586826\n",
      "range:(9600, 9728) loss= 0.350895524025\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 144\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351634144783\n",
      "range:(9600, 9728) loss= 0.349651008844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336160898209\n",
      "range:(9600, 9728) loss= 0.352750897408\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348599374294\n",
      "range:(9600, 9728) loss= 0.361818879843\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361966431141\n",
      "range:(9600, 9728) loss= 0.354352623224\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362573862076\n",
      "range:(9600, 9728) loss= 0.35134780407\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 145\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352572977543\n",
      "range:(9600, 9728) loss= 0.350629508495\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334583342075\n",
      "range:(9600, 9728) loss= 0.353206753731\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349844574928\n",
      "range:(9600, 9728) loss= 0.3619389534\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362283557653\n",
      "range:(9600, 9728) loss= 0.353959083557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359071910381\n",
      "range:(9600, 9728) loss= 0.352537304163\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 146\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349780201912\n",
      "range:(9600, 9728) loss= 0.352197945118\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335999548435\n",
      "range:(9600, 9728) loss= 0.352531433105\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348458468914\n",
      "range:(9600, 9728) loss= 0.36124509573\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363222420216\n",
      "range:(9600, 9728) loss= 0.35299795866\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358044594526\n",
      "range:(9600, 9728) loss= 0.351325690746\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 147\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350912481546\n",
      "range:(9600, 9728) loss= 0.350059360266\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333958923817\n",
      "range:(9600, 9728) loss= 0.353409290314\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34993454814\n",
      "range:(9600, 9728) loss= 0.361835449934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360814094543\n",
      "range:(9600, 9728) loss= 0.353004395962\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359320819378\n",
      "range:(9600, 9728) loss= 0.35209351778\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 148\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350587517023\n",
      "range:(9600, 9728) loss= 0.348645567894\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332923859358\n",
      "range:(9600, 9728) loss= 0.352954685688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349591076374\n",
      "range:(9600, 9728) loss= 0.365521967411\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363526582718\n",
      "range:(9600, 9728) loss= 0.352550446987\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359327554703\n",
      "range:(9600, 9728) loss= 0.35153234005\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 149\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350056946278\n",
      "range:(9600, 9728) loss= 0.349264025688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33423191309\n",
      "range:(9600, 9728) loss= 0.353369474411\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349653065205\n",
      "range:(9600, 9728) loss= 0.362937867641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362323999405\n",
      "range:(9600, 9728) loss= 0.356525808573\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359316289425\n",
      "range:(9600, 9728) loss= 0.351911991835\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 150\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350832492113\n",
      "range:(9600, 9728) loss= 0.351961791515\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335576832294\n",
      "range:(9600, 9728) loss= 0.354930788279\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34923261404\n",
      "range:(9600, 9728) loss= 0.360613316298\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361235767603\n",
      "range:(9600, 9728) loss= 0.353875875473\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358795017004\n",
      "range:(9600, 9728) loss= 0.351402550936\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 151\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350321173668\n",
      "range:(9600, 9728) loss= 0.349417716265\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33401453495\n",
      "range:(9600, 9728) loss= 0.352030456066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348336130381\n",
      "range:(9600, 9728) loss= 0.361299902201\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360727936029\n",
      "range:(9600, 9728) loss= 0.354720652103\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36104169488\n",
      "range:(9600, 9728) loss= 0.351194798946\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 152\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350966334343\n",
      "range:(9600, 9728) loss= 0.349038422108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333661019802\n",
      "range:(9600, 9728) loss= 0.352336883545\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348623514175\n",
      "range:(9600, 9728) loss= 0.361803591251\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360609769821\n",
      "range:(9600, 9728) loss= 0.352239429951\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358024179935\n",
      "range:(9600, 9728) loss= 0.349982261658\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 153\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350680828094\n",
      "range:(9600, 9728) loss= 0.349579364061\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333313524723\n",
      "range:(9600, 9728) loss= 0.352654248476\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349209725857\n",
      "range:(9600, 9728) loss= 0.365933895111\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.365872383118\n",
      "range:(9600, 9728) loss= 0.352881789207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359477967024\n",
      "range:(9600, 9728) loss= 0.352863252163\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 154\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351832807064\n",
      "range:(9600, 9728) loss= 0.350145578384\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332982748747\n",
      "range:(9600, 9728) loss= 0.351584285498\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34892553091\n",
      "range:(9600, 9728) loss= 0.360711485147\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36049965024\n",
      "range:(9600, 9728) loss= 0.356096744537\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360869765282\n",
      "range:(9600, 9728) loss= 0.354228883982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 155\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351341575384\n",
      "range:(9600, 9728) loss= 0.349056184292\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333481132984\n",
      "range:(9600, 9728) loss= 0.351924538612\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34854310751\n",
      "range:(9600, 9728) loss= 0.361415207386\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360746920109\n",
      "range:(9600, 9728) loss= 0.352152884007\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359095603228\n",
      "range:(9600, 9728) loss= 0.350484251976\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 156\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350663095713\n",
      "range:(9600, 9728) loss= 0.348824083805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334184229374\n",
      "range:(9600, 9728) loss= 0.351630359888\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348530292511\n",
      "range:(9600, 9728) loss= 0.367570906878\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.365334272385\n",
      "range:(9600, 9728) loss= 0.353248178959\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357587277889\n",
      "range:(9600, 9728) loss= 0.350488096476\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 157\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351104259491\n",
      "range:(9600, 9728) loss= 0.350930243731\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336719363928\n",
      "range:(9600, 9728) loss= 0.353157401085\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348856806755\n",
      "range:(9600, 9728) loss= 0.360857307911\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360296785831\n",
      "range:(9600, 9728) loss= 0.353402495384\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358847975731\n",
      "range:(9600, 9728) loss= 0.353380322456\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 158\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350700080395\n",
      "range:(9600, 9728) loss= 0.349545061588\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334558337927\n",
      "range:(9600, 9728) loss= 0.35449308157\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349172353745\n",
      "range:(9600, 9728) loss= 0.36161506176\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361276626587\n",
      "range:(9600, 9728) loss= 0.354275643826\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.362329185009\n",
      "range:(9600, 9728) loss= 0.351380884647\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 159\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351255118847\n",
      "range:(9600, 9728) loss= 0.350191533566\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335380375385\n",
      "range:(9600, 9728) loss= 0.352577745914\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349292874336\n",
      "range:(9600, 9728) loss= 0.361299067736\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361088186502\n",
      "range:(9600, 9728) loss= 0.353268414736\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359298169613\n",
      "range:(9600, 9728) loss= 0.350493431091\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 160\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348854720592\n",
      "range:(9600, 9728) loss= 0.350952446461\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33568418026\n",
      "range:(9600, 9728) loss= 0.352338671684\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349579453468\n",
      "range:(9600, 9728) loss= 0.360997706652\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360705316067\n",
      "range:(9600, 9728) loss= 0.352311611176\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35886234045\n",
      "range:(9600, 9728) loss= 0.351770550013\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 161\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352019816637\n",
      "range:(9600, 9728) loss= 0.348836004734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333819419146\n",
      "range:(9600, 9728) loss= 0.352778702974\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348849236965\n",
      "range:(9600, 9728) loss= 0.365631759167\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361951828003\n",
      "range:(9600, 9728) loss= 0.353660106659\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358574956656\n",
      "range:(9600, 9728) loss= 0.350025087595\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 162\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349947035313\n",
      "range:(9600, 9728) loss= 0.350029885769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334103703499\n",
      "range:(9600, 9728) loss= 0.353294849396\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348350822926\n",
      "range:(9600, 9728) loss= 0.36173671484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36224013567\n",
      "range:(9600, 9728) loss= 0.352815032005\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356721252203\n",
      "range:(9600, 9728) loss= 0.353587508202\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 163\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350689649582\n",
      "range:(9600, 9728) loss= 0.349213689566\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332941651344\n",
      "range:(9600, 9728) loss= 0.352704972029\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347527503967\n",
      "range:(9600, 9728) loss= 0.360681355\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360453426838\n",
      "range:(9600, 9728) loss= 0.35685479641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361577212811\n",
      "range:(9600, 9728) loss= 0.352132976055\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 164\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349184781313\n",
      "range:(9600, 9728) loss= 0.347862839699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333196341991\n",
      "range:(9600, 9728) loss= 0.353911042213\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348253726959\n",
      "range:(9600, 9728) loss= 0.360871255398\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360644608736\n",
      "range:(9600, 9728) loss= 0.352900147438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358176469803\n",
      "range:(9600, 9728) loss= 0.350295513868\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 165\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350209414959\n",
      "range:(9600, 9728) loss= 0.349973320961\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333970010281\n",
      "range:(9600, 9728) loss= 0.354035437107\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348614513874\n",
      "range:(9600, 9728) loss= 0.359590381384\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359750151634\n",
      "range:(9600, 9728) loss= 0.352865874767\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358313113451\n",
      "range:(9600, 9728) loss= 0.351828694344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 166\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350539505482\n",
      "range:(9600, 9728) loss= 0.350097000599\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333419203758\n",
      "range:(9600, 9728) loss= 0.353497684002\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348970830441\n",
      "range:(9600, 9728) loss= 0.363701879978\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363221943378\n",
      "range:(9600, 9728) loss= 0.353078484535\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358958959579\n",
      "range:(9600, 9728) loss= 0.352041482925\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 167\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349687159061\n",
      "range:(9600, 9728) loss= 0.348603010178\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332938879728\n",
      "range:(9600, 9728) loss= 0.353488743305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34824898839\n",
      "range:(9600, 9728) loss= 0.36110830307\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360679924488\n",
      "range:(9600, 9728) loss= 0.354376107454\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358875393867\n",
      "range:(9600, 9728) loss= 0.349713385105\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 168\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350671052933\n",
      "range:(9600, 9728) loss= 0.348081171513\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33344745636\n",
      "range:(9600, 9728) loss= 0.352539181709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348008453846\n",
      "range:(9600, 9728) loss= 0.363938570023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362249016762\n",
      "range:(9600, 9728) loss= 0.35445535183\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359757095575\n",
      "range:(9600, 9728) loss= 0.35023021698\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 169\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349598020315\n",
      "range:(9600, 9728) loss= 0.348853737116\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333433389664\n",
      "range:(9600, 9728) loss= 0.351494282484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348371446133\n",
      "range:(9600, 9728) loss= 0.35975843668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.3599178195\n",
      "range:(9600, 9728) loss= 0.352964103222\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358687520027\n",
      "range:(9600, 9728) loss= 0.350097417831\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 170\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350195229053\n",
      "range:(9600, 9728) loss= 0.347685098648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332429260015\n",
      "range:(9600, 9728) loss= 0.354211837053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347557246685\n",
      "range:(9600, 9728) loss= 0.363903790712\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362243950367\n",
      "range:(9600, 9728) loss= 0.353333175182\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360325753689\n",
      "range:(9600, 9728) loss= 0.350769639015\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 171\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349515616894\n",
      "range:(9600, 9728) loss= 0.349597126245\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333858370781\n",
      "range:(9600, 9728) loss= 0.351266950369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34856402874\n",
      "range:(9600, 9728) loss= 0.360684067011\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360833346844\n",
      "range:(9600, 9728) loss= 0.357564508915\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360939919949\n",
      "range:(9600, 9728) loss= 0.352977216244\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 172\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351498186588\n",
      "range:(9600, 9728) loss= 0.349961161613\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334805935621\n",
      "range:(9600, 9728) loss= 0.352431118488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348470985889\n",
      "range:(9600, 9728) loss= 0.361136853695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361334085464\n",
      "range:(9600, 9728) loss= 0.354641824961\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360351800919\n",
      "range:(9600, 9728) loss= 0.35242331028\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 173\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351339936256\n",
      "range:(9600, 9728) loss= 0.34993493557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33564221859\n",
      "range:(9600, 9728) loss= 0.354218125343\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347147643566\n",
      "range:(9600, 9728) loss= 0.361802279949\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359571039677\n",
      "range:(9600, 9728) loss= 0.352851182222\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357931047678\n",
      "range:(9600, 9728) loss= 0.350057601929\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 174\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35109385848\n",
      "range:(9600, 9728) loss= 0.349984765053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3338136971\n",
      "range:(9600, 9728) loss= 0.354721486568\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34985730052\n",
      "range:(9600, 9728) loss= 0.359754621983\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359182834625\n",
      "range:(9600, 9728) loss= 0.352950543165\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360333859921\n",
      "range:(9600, 9728) loss= 0.348610579967\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 175\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349626988173\n",
      "range:(9600, 9728) loss= 0.35018825531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335909903049\n",
      "range:(9600, 9728) loss= 0.354923903942\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350510418415\n",
      "range:(9600, 9728) loss= 0.364370286465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361951649189\n",
      "range:(9600, 9728) loss= 0.35267496109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357705116272\n",
      "range:(9600, 9728) loss= 0.348635017872\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 176\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349605321884\n",
      "range:(9600, 9728) loss= 0.348562717438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332335829735\n",
      "range:(9600, 9728) loss= 0.352504998446\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347940593958\n",
      "range:(9600, 9728) loss= 0.360520452261\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360918223858\n",
      "range:(9600, 9728) loss= 0.351961344481\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35717985034\n",
      "range:(9600, 9728) loss= 0.348997473717\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 177\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349716037512\n",
      "range:(9600, 9728) loss= 0.349976629019\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333988308907\n",
      "range:(9600, 9728) loss= 0.354155242443\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.350243330002\n",
      "range:(9600, 9728) loss= 0.359620928764\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36036708951\n",
      "range:(9600, 9728) loss= 0.353173196316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358694076538\n",
      "range:(9600, 9728) loss= 0.352444022894\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 178\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351850986481\n",
      "range:(9600, 9728) loss= 0.349384665489\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33335852623\n",
      "range:(9600, 9728) loss= 0.352161347866\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349396169186\n",
      "range:(9600, 9728) loss= 0.360155463219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360007822514\n",
      "range:(9600, 9728) loss= 0.35191577673\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357479572296\n",
      "range:(9600, 9728) loss= 0.354791283607\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 179\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35016065836\n",
      "range:(9600, 9728) loss= 0.352456510067\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335006147623\n",
      "range:(9600, 9728) loss= 0.354767680168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349744319916\n",
      "range:(9600, 9728) loss= 0.359883368015\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359178364277\n",
      "range:(9600, 9728) loss= 0.35272705555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357814341784\n",
      "range:(9600, 9728) loss= 0.34974449873\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 180\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349858760834\n",
      "range:(9600, 9728) loss= 0.348624706268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333153784275\n",
      "range:(9600, 9728) loss= 0.353021740913\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348158180714\n",
      "range:(9600, 9728) loss= 0.360873699188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360026896\n",
      "range:(9600, 9728) loss= 0.35321611166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359518051147\n",
      "range:(9600, 9728) loss= 0.354911863804\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 181\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350599229336\n",
      "range:(9600, 9728) loss= 0.349784255028\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333643078804\n",
      "range:(9600, 9728) loss= 0.351951688528\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346908509731\n",
      "range:(9600, 9728) loss= 0.360364437103\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36085164547\n",
      "range:(9600, 9728) loss= 0.351734787226\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357412129641\n",
      "range:(9600, 9728) loss= 0.350103110075\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 182\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35008507967\n",
      "range:(9600, 9728) loss= 0.350642442703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332588732243\n",
      "range:(9600, 9728) loss= 0.353085696697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348808467388\n",
      "range:(9600, 9728) loss= 0.365332484245\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36222910881\n",
      "range:(9600, 9728) loss= 0.3532564044\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357782423496\n",
      "range:(9600, 9728) loss= 0.348832577467\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 183\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34994584322\n",
      "range:(9600, 9728) loss= 0.347807705402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332223951817\n",
      "range:(9600, 9728) loss= 0.353222757578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348264157772\n",
      "range:(9600, 9728) loss= 0.363062918186\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36135160923\n",
      "range:(9600, 9728) loss= 0.353292256594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359925836325\n",
      "range:(9600, 9728) loss= 0.350550800562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 184\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350631356239\n",
      "range:(9600, 9728) loss= 0.348921239376\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333161652088\n",
      "range:(9600, 9728) loss= 0.352650463581\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348310738802\n",
      "range:(9600, 9728) loss= 0.3606159091\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359506219625\n",
      "range:(9600, 9728) loss= 0.350952684879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357542872429\n",
      "range:(9600, 9728) loss= 0.349166095257\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 185\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349474877119\n",
      "range:(9600, 9728) loss= 0.349837005138\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333124756813\n",
      "range:(9600, 9728) loss= 0.353787183762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348348617554\n",
      "range:(9600, 9728) loss= 0.361074924469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360612154007\n",
      "range:(9600, 9728) loss= 0.35281586647\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35777130723\n",
      "range:(9600, 9728) loss= 0.349294126034\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 186\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349419534206\n",
      "range:(9600, 9728) loss= 0.349331885576\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333245903254\n",
      "range:(9600, 9728) loss= 0.351650774479\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349365383387\n",
      "range:(9600, 9728) loss= 0.362186193466\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36126357317\n",
      "range:(9600, 9728) loss= 0.35095334053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356036245823\n",
      "range:(9600, 9728) loss= 0.349871754646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 187\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349513977766\n",
      "range:(9600, 9728) loss= 0.348730266094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333310037851\n",
      "range:(9600, 9728) loss= 0.352082371712\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348490774632\n",
      "range:(9600, 9728) loss= 0.364458501339\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363041639328\n",
      "range:(9600, 9728) loss= 0.352202445269\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357293665409\n",
      "range:(9600, 9728) loss= 0.352035284042\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 188\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351822018623\n",
      "range:(9600, 9728) loss= 0.348681032658\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333943963051\n",
      "range:(9600, 9728) loss= 0.351793766022\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348065137863\n",
      "range:(9600, 9728) loss= 0.360679745674\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361405611038\n",
      "range:(9600, 9728) loss= 0.351643681526\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359536916018\n",
      "range:(9600, 9728) loss= 0.349734276533\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 189\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348488777876\n",
      "range:(9600, 9728) loss= 0.349907904863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3353715837\n",
      "range:(9600, 9728) loss= 0.352422595024\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349227875471\n",
      "range:(9600, 9728) loss= 0.362024724483\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36002933979\n",
      "range:(9600, 9728) loss= 0.355191320181\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.363865375519\n",
      "range:(9600, 9728) loss= 0.349986314774\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 190\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349478304386\n",
      "range:(9600, 9728) loss= 0.348993718624\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332392215729\n",
      "range:(9600, 9728) loss= 0.352609038353\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347554326057\n",
      "range:(9600, 9728) loss= 0.359877824783\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359566926956\n",
      "range:(9600, 9728) loss= 0.351273238659\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357408583164\n",
      "range:(9600, 9728) loss= 0.350911021233\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 191\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351199448109\n",
      "range:(9600, 9728) loss= 0.350745081902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334822237492\n",
      "range:(9600, 9728) loss= 0.35189050436\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346360981464\n",
      "range:(9600, 9728) loss= 0.364470332861\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362657219172\n",
      "range:(9600, 9728) loss= 0.353718549013\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360910356045\n",
      "range:(9600, 9728) loss= 0.349932551384\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 192\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349583148956\n",
      "range:(9600, 9728) loss= 0.354138433933\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.337397694588\n",
      "range:(9600, 9728) loss= 0.352103710175\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34825116396\n",
      "range:(9600, 9728) loss= 0.363968521357\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.363058269024\n",
      "range:(9600, 9728) loss= 0.351846665144\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358708828688\n",
      "range:(9600, 9728) loss= 0.349077522755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 193\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350278317928\n",
      "range:(9600, 9728) loss= 0.350011348724\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334555387497\n",
      "range:(9600, 9728) loss= 0.355629265308\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349446207285\n",
      "range:(9600, 9728) loss= 0.361769318581\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360799461603\n",
      "range:(9600, 9728) loss= 0.352103531361\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35682708025\n",
      "range:(9600, 9728) loss= 0.349624276161\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 194\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35038626194\n",
      "range:(9600, 9728) loss= 0.349243342876\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333214342594\n",
      "range:(9600, 9728) loss= 0.352237790823\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347948163748\n",
      "range:(9600, 9728) loss= 0.360940128565\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359879493713\n",
      "range:(9600, 9728) loss= 0.353634625673\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35879445076\n",
      "range:(9600, 9728) loss= 0.351517081261\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 195\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349269628525\n",
      "range:(9600, 9728) loss= 0.350274145603\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334930837154\n",
      "range:(9600, 9728) loss= 0.354120910168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348509848118\n",
      "range:(9600, 9728) loss= 0.360504120588\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358975827694\n",
      "range:(9600, 9728) loss= 0.350624382496\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35730573535\n",
      "range:(9600, 9728) loss= 0.35225301981\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 196\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.353882551193\n",
      "range:(9600, 9728) loss= 0.349605619907\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332574725151\n",
      "range:(9600, 9728) loss= 0.351149499416\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347223907709\n",
      "range:(9600, 9728) loss= 0.364428937435\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364455878735\n",
      "range:(9600, 9728) loss= 0.352179288864\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358769834042\n",
      "range:(9600, 9728) loss= 0.350218296051\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 197\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349449187517\n",
      "range:(9600, 9728) loss= 0.349402964115\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333885669708\n",
      "range:(9600, 9728) loss= 0.350553959608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346551656723\n",
      "range:(9600, 9728) loss= 0.36554980278\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364401787519\n",
      "range:(9600, 9728) loss= 0.353021323681\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359432160854\n",
      "range:(9600, 9728) loss= 0.351104438305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 198\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350802302361\n",
      "range:(9600, 9728) loss= 0.348601639271\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334115475416\n",
      "range:(9600, 9728) loss= 0.352879524231\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347673326731\n",
      "range:(9600, 9728) loss= 0.360911488533\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361122608185\n",
      "range:(9600, 9728) loss= 0.352633476257\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357509374619\n",
      "range:(9600, 9728) loss= 0.348780453205\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 199\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350004851818\n",
      "range:(9600, 9728) loss= 0.349303752184\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333221316338\n",
      "range:(9600, 9728) loss= 0.352909266949\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346936523914\n",
      "range:(9600, 9728) loss= 0.362031787634\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361899644136\n",
      "range:(9600, 9728) loss= 0.353785276413\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358959019184\n",
      "range:(9600, 9728) loss= 0.351545721292\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 200\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350594878197\n",
      "range:(9600, 9728) loss= 0.349408388138\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334735989571\n",
      "range:(9600, 9728) loss= 0.350709438324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347704589367\n",
      "range:(9600, 9728) loss= 0.360702335835\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360127747059\n",
      "range:(9600, 9728) loss= 0.351916491985\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358048170805\n",
      "range:(9600, 9728) loss= 0.349260032177\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 201\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351818561554\n",
      "range:(9600, 9728) loss= 0.350792765617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333198785782\n",
      "range:(9600, 9728) loss= 0.352559506893\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348443031311\n",
      "range:(9600, 9728) loss= 0.359845042229\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360158532858\n",
      "range:(9600, 9728) loss= 0.352774679661\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356856286526\n",
      "range:(9600, 9728) loss= 0.351327389479\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 202\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35096886754\n",
      "range:(9600, 9728) loss= 0.352024137974\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335216999054\n",
      "range:(9600, 9728) loss= 0.352567672729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347316741943\n",
      "range:(9600, 9728) loss= 0.360953629017\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359975755215\n",
      "range:(9600, 9728) loss= 0.350880593061\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.355760604143\n",
      "range:(9600, 9728) loss= 0.349000245333\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 203\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349878787994\n",
      "range:(9600, 9728) loss= 0.347934424877\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333301246166\n",
      "range:(9600, 9728) loss= 0.3523837924\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348643094301\n",
      "range:(9600, 9728) loss= 0.359877884388\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360004842281\n",
      "range:(9600, 9728) loss= 0.352482676506\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357895553112\n",
      "range:(9600, 9728) loss= 0.353182733059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 204\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350901603699\n",
      "range:(9600, 9728) loss= 0.350649654865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334025800228\n",
      "range:(9600, 9728) loss= 0.353947728872\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348741590977\n",
      "range:(9600, 9728) loss= 0.358440637589\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357955038548\n",
      "range:(9600, 9728) loss= 0.352936565876\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359513789415\n",
      "range:(9600, 9728) loss= 0.351015418768\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 205\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348948478699\n",
      "range:(9600, 9728) loss= 0.349167943001\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332686424255\n",
      "range:(9600, 9728) loss= 0.35012537241\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347042232752\n",
      "range:(9600, 9728) loss= 0.362200587988\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359691649675\n",
      "range:(9600, 9728) loss= 0.352511316538\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35670247674\n",
      "range:(9600, 9728) loss= 0.350489795208\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 206\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350449860096\n",
      "range:(9600, 9728) loss= 0.34960269928\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3342654109\n",
      "range:(9600, 9728) loss= 0.353799104691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347361981869\n",
      "range:(9600, 9728) loss= 0.363465845585\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361862719059\n",
      "range:(9600, 9728) loss= 0.353399097919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358997136354\n",
      "range:(9600, 9728) loss= 0.348295420408\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 207\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349157631397\n",
      "range:(9600, 9728) loss= 0.348925232887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333490908146\n",
      "range:(9600, 9728) loss= 0.351718306541\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348567545414\n",
      "range:(9600, 9728) loss= 0.360579669476\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361555218697\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-45a820a6a100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mminX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mminY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m75\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 0\n",
    "    for ep in range(100, 100 + no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(float(len(batch_images)) / min_batch_size)):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                if(index % 75 == 0):\n",
    "                    print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_1\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's visualize the representation of a random image and it's reconstructed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'computation_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a9b60d1283bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputation_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# load the weights from the model1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# instead of global variable initializer, restore the graph:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'computation_graph' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = computation_graph) as sess:\n",
    "    # load the weights from the model1\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # instead of global variable initializer, restore the graph:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    prediction = sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "    inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "    \n",
    "    random_image = batch_data[np.random.randint(len(batch_data))]\n",
    "    reconstructed_image = sess.run(prediction, feed_dict={inputs: np.array([random_image])})[0]\n",
    "    \n",
    "    # plot the two images with their titles:\n",
    "    plt.figure().suptitle(\"Original Image\")\n",
    "    plt.imshow(random_image, interpolation='none')\n",
    "    \n",
    "    plt.figure().suptitle(\"Reconstructed Image\")\n",
    "    plt.imshow(reconstructed_image, interpolation='none')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
