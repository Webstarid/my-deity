{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 200 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 3 # all kernels are 3x3\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 128 # we look at 64 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f6b800050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f6bd355d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA+A0R+Fpl0dhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f6b7c4a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f69dcd350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypefloat32\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[ 0.96470588  0.98431373  0.99607843]\n",
      " [ 0.95686275  0.97647059  0.98823529]\n",
      " [ 0.95294118  0.96862745  0.98039216]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD9CAYAAACcAsr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqb9cR3Y5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f6b7549d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2, tf.shape(a1), tf.shape(a2), tf.shape(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_, sha1, sha2, sha3 = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode(inp, sha1, sha2, sha3):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, sha3, \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, sha2,\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, sha1,\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.shape(tf_input),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_, sha1, sha2, sha3)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector, sha1, sha2, sha3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../Models/IDEA_1/Model_cifar_1/model_cifar_1-205\n",
      "epoch: 206\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350449860096\n",
      "range:(9600, 9728) loss= 0.34960269928\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3342654109\n",
      "range:(9600, 9728) loss= 0.353799104691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347361981869\n",
      "range:(9600, 9728) loss= 0.363465845585\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361862719059\n",
      "range:(9600, 9728) loss= 0.353399097919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358997136354\n",
      "range:(9600, 9728) loss= 0.348295420408\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 207\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349157631397\n",
      "range:(9600, 9728) loss= 0.348925232887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333490908146\n",
      "range:(9600, 9728) loss= 0.351718306541\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348567545414\n",
      "range:(9600, 9728) loss= 0.360579669476\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361555218697\n",
      "range:(9600, 9728) loss= 0.352793812752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361851781607\n",
      "range:(9600, 9728) loss= 0.34925276041\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 208\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350681871176\n",
      "range:(9600, 9728) loss= 0.347947895527\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332330048084\n",
      "range:(9600, 9728) loss= 0.352696657181\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348816633224\n",
      "range:(9600, 9728) loss= 0.360324084759\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359204828739\n",
      "range:(9600, 9728) loss= 0.353300035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357738494873\n",
      "range:(9600, 9728) loss= 0.351125299931\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 209\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350521206856\n",
      "range:(9600, 9728) loss= 0.349576115608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333509981632\n",
      "range:(9600, 9728) loss= 0.358094036579\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.351438432932\n",
      "range:(9600, 9728) loss= 0.367438256741\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.364623725414\n",
      "range:(9600, 9728) loss= 0.352497458458\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358446598053\n",
      "range:(9600, 9728) loss= 0.349643588066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 210\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349956572056\n",
      "range:(9600, 9728) loss= 0.349093049765\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334565937519\n",
      "range:(9600, 9728) loss= 0.352320253849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34808909893\n",
      "range:(9600, 9728) loss= 0.360795199871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360051035881\n",
      "range:(9600, 9728) loss= 0.351252257824\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357227087021\n",
      "range:(9600, 9728) loss= 0.349161535501\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 211\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349216550589\n",
      "range:(9600, 9728) loss= 0.347402185202\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332150638103\n",
      "range:(9600, 9728) loss= 0.350733160973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346224606037\n",
      "range:(9600, 9728) loss= 0.360610216856\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36051979661\n",
      "range:(9600, 9728) loss= 0.350876748562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358007907867\n",
      "range:(9600, 9728) loss= 0.350495100021\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 212\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348338901997\n",
      "range:(9600, 9728) loss= 0.348148524761\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333935797215\n",
      "range:(9600, 9728) loss= 0.353000938892\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348591566086\n",
      "range:(9600, 9728) loss= 0.359997391701\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361609041691\n",
      "range:(9600, 9728) loss= 0.353926360607\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360335856676\n",
      "range:(9600, 9728) loss= 0.349302232265\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 213\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347960710526\n",
      "range:(9600, 9728) loss= 0.348368883133\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333289325237\n",
      "range:(9600, 9728) loss= 0.355008482933\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348280876875\n",
      "range:(9600, 9728) loss= 0.36075296998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360317647457\n",
      "range:(9600, 9728) loss= 0.351770311594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360432893038\n",
      "range:(9600, 9728) loss= 0.349487900734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 214\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350090414286\n",
      "range:(9600, 9728) loss= 0.349759757519\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333829075098\n",
      "range:(9600, 9728) loss= 0.353508889675\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346909254789\n",
      "range:(9600, 9728) loss= 0.361582696438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360818564892\n",
      "range:(9600, 9728) loss= 0.353230118752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360754966736\n",
      "range:(9600, 9728) loss= 0.349558055401\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 215\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348819971085\n",
      "range:(9600, 9728) loss= 0.35013127327\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333628356457\n",
      "range:(9600, 9728) loss= 0.351641565561\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346429407597\n",
      "range:(9600, 9728) loss= 0.363316357136\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362078785896\n",
      "range:(9600, 9728) loss= 0.352085918188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358236074448\n",
      "range:(9600, 9728) loss= 0.350647270679\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 216\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349499493837\n",
      "range:(9600, 9728) loss= 0.348559111357\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3342628479\n",
      "range:(9600, 9728) loss= 0.351517617702\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346903175116\n",
      "range:(9600, 9728) loss= 0.361556708813\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361856341362\n",
      "range:(9600, 9728) loss= 0.350938498974\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357369363308\n",
      "range:(9600, 9728) loss= 0.348776102066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 217\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349396377802\n",
      "range:(9600, 9728) loss= 0.348878383636\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334177136421\n",
      "range:(9600, 9728) loss= 0.352831661701\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346970379353\n",
      "range:(9600, 9728) loss= 0.359713822603\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358971059322\n",
      "range:(9600, 9728) loss= 0.351392507553\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357525229454\n",
      "range:(9600, 9728) loss= 0.349204421043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 218\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349190980196\n",
      "range:(9600, 9728) loss= 0.348326325417\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33275577426\n",
      "range:(9600, 9728) loss= 0.352315247059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347951382399\n",
      "range:(9600, 9728) loss= 0.360827565193\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359787583351\n",
      "range:(9600, 9728) loss= 0.352503240108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361942410469\n",
      "range:(9600, 9728) loss= 0.352724522352\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 219\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.35109397769\n",
      "range:(9600, 9728) loss= 0.348693609238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333715558052\n",
      "range:(9600, 9728) loss= 0.353673666716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348034352064\n",
      "range:(9600, 9728) loss= 0.363875508308\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.361690759659\n",
      "range:(9600, 9728) loss= 0.352160871029\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357354104519\n",
      "range:(9600, 9728) loss= 0.349861085415\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 220\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349492371082\n",
      "range:(9600, 9728) loss= 0.350655853748\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336447060108\n",
      "range:(9600, 9728) loss= 0.354740381241\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348152726889\n",
      "range:(9600, 9728) loss= 0.361170351505\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360524445772\n",
      "range:(9600, 9728) loss= 0.351776391268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357456684113\n",
      "range:(9600, 9728) loss= 0.349509835243\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 221\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349379301071\n",
      "range:(9600, 9728) loss= 0.34922504425\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332990884781\n",
      "range:(9600, 9728) loss= 0.350250631571\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347269892693\n",
      "range:(9600, 9728) loss= 0.35961291194\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358970582485\n",
      "range:(9600, 9728) loss= 0.351491987705\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357636392117\n",
      "range:(9600, 9728) loss= 0.351344168186\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 222\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350125730038\n",
      "range:(9600, 9728) loss= 0.34816545248\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332784444094\n",
      "range:(9600, 9728) loss= 0.351447582245\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348397195339\n",
      "range:(9600, 9728) loss= 0.359211206436\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358527183533\n",
      "range:(9600, 9728) loss= 0.353697896004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359525620937\n",
      "range:(9600, 9728) loss= 0.351337730885\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 223\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350593328476\n",
      "range:(9600, 9728) loss= 0.351081907749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334663242102\n",
      "range:(9600, 9728) loss= 0.3511518538\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347049117088\n",
      "range:(9600, 9728) loss= 0.359439194202\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357832968235\n",
      "range:(9600, 9728) loss= 0.351825743914\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357024490833\n",
      "range:(9600, 9728) loss= 0.34940135479\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 224\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.347570568323\n",
      "range:(9600, 9728) loss= 0.348356425762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333003818989\n",
      "range:(9600, 9728) loss= 0.350338160992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346521854401\n",
      "range:(9600, 9728) loss= 0.362543106079\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.3600936234\n",
      "range:(9600, 9728) loss= 0.351587057114\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358510971069\n",
      "range:(9600, 9728) loss= 0.350633323193\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 225\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348523676395\n",
      "range:(9600, 9728) loss= 0.349883973598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.3329834342\n",
      "range:(9600, 9728) loss= 0.351459354162\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.3470236063\n",
      "range:(9600, 9728) loss= 0.358139157295\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357525110245\n",
      "range:(9600, 9728) loss= 0.353311747313\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359199851751\n",
      "range:(9600, 9728) loss= 0.348258733749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 226\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350521534681\n",
      "range:(9600, 9728) loss= 0.349382281303\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335572957993\n",
      "range:(9600, 9728) loss= 0.351790994406\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348537147045\n",
      "range:(9600, 9728) loss= 0.362353622913\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.362104088068\n",
      "range:(9600, 9728) loss= 0.352991878986\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35946893692\n",
      "range:(9600, 9728) loss= 0.350563794374\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 227\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351310968399\n",
      "range:(9600, 9728) loss= 0.349063009024\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333190798759\n",
      "range:(9600, 9728) loss= 0.35096424818\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346627980471\n",
      "range:(9600, 9728) loss= 0.362207710743\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359787970781\n",
      "range:(9600, 9728) loss= 0.351959466934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357857525349\n",
      "range:(9600, 9728) loss= 0.347954690456\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 228\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348149210215\n",
      "range:(9600, 9728) loss= 0.347903072834\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331874191761\n",
      "range:(9600, 9728) loss= 0.353374063969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347968935966\n",
      "range:(9600, 9728) loss= 0.359107106924\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359210073948\n",
      "range:(9600, 9728) loss= 0.351977169514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356683552265\n",
      "range:(9600, 9728) loss= 0.350727260113\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 229\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.352470248938\n",
      "range:(9600, 9728) loss= 0.348268568516\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333364367485\n",
      "range:(9600, 9728) loss= 0.350494027138\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346757501364\n",
      "range:(9600, 9728) loss= 0.359752804041\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359027206898\n",
      "range:(9600, 9728) loss= 0.351252377033\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357613414526\n",
      "range:(9600, 9728) loss= 0.348567426205\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 230\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348771959543\n",
      "range:(9600, 9728) loss= 0.348030030727\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332973331213\n",
      "range:(9600, 9728) loss= 0.351685672998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346996426582\n",
      "range:(9600, 9728) loss= 0.359237253666\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359830468893\n",
      "range:(9600, 9728) loss= 0.353400230408\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358116179705\n",
      "range:(9600, 9728) loss= 0.351305365562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 231\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350231975317\n",
      "range:(9600, 9728) loss= 0.34805637598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333024680614\n",
      "range:(9600, 9728) loss= 0.3538223207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349020719528\n",
      "range:(9600, 9728) loss= 0.361011326313\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360512077808\n",
      "range:(9600, 9728) loss= 0.356294482946\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360757052898\n",
      "range:(9600, 9728) loss= 0.349253863096\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 232\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351506471634\n",
      "range:(9600, 9728) loss= 0.347999066114\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334182143211\n",
      "range:(9600, 9728) loss= 0.350593209267\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34543338418\n",
      "range:(9600, 9728) loss= 0.359716117382\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358940899372\n",
      "range:(9600, 9728) loss= 0.351229518652\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357610225677\n",
      "range:(9600, 9728) loss= 0.349468529224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 233\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34818893671\n",
      "range:(9600, 9728) loss= 0.353354811668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.337863326073\n",
      "range:(9600, 9728) loss= 0.351789653301\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346637010574\n",
      "range:(9600, 9728) loss= 0.362125456333\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359835386276\n",
      "range:(9600, 9728) loss= 0.351003825665\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356948703527\n",
      "range:(9600, 9728) loss= 0.351665794849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 234\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351434022188\n",
      "range:(9600, 9728) loss= 0.347184509039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331328332424\n",
      "range:(9600, 9728) loss= 0.353883802891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346572697163\n",
      "range:(9600, 9728) loss= 0.359587192535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360749840736\n",
      "range:(9600, 9728) loss= 0.351057827473\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356655061245\n",
      "range:(9600, 9728) loss= 0.349246263504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 235\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.34920156002\n",
      "range:(9600, 9728) loss= 0.348581194878\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334841936827\n",
      "range:(9600, 9728) loss= 0.349924862385\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346744507551\n",
      "range:(9600, 9728) loss= 0.358579307795\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358583807945\n",
      "range:(9600, 9728) loss= 0.352201163769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357064783573\n",
      "range:(9600, 9728) loss= 0.348535507917\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 236\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349722504616\n",
      "range:(9600, 9728) loss= 0.350139439106\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334894418716\n",
      "range:(9600, 9728) loss= 0.352319151163\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347517043352\n",
      "range:(9600, 9728) loss= 0.358875215054\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358568519354\n",
      "range:(9600, 9728) loss= 0.353720486164\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358789384365\n",
      "range:(9600, 9728) loss= 0.348859429359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 237\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350120574236\n",
      "range:(9600, 9728) loss= 0.349010944366\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.336510747671\n",
      "range:(9600, 9728) loss= 0.352619320154\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.34764495492\n",
      "range:(9600, 9728) loss= 0.358808100224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.357812613249\n",
      "range:(9600, 9728) loss= 0.35103648901\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.355999499559\n",
      "range:(9600, 9728) loss= 0.34977132082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 238\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349904477596\n",
      "range:(9600, 9728) loss= 0.347281306982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331372797489\n",
      "range:(9600, 9728) loss= 0.355508267879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.349396109581\n",
      "range:(9600, 9728) loss= 0.36428335309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36063605547\n",
      "range:(9600, 9728) loss= 0.355307042599\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.361433029175\n",
      "range:(9600, 9728) loss= 0.349447757006\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 239\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348857820034\n",
      "range:(9600, 9728) loss= 0.349846422672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334222942591\n",
      "range:(9600, 9728) loss= 0.352986723185\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347634673119\n",
      "range:(9600, 9728) loss= 0.359929889441\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358759999275\n",
      "range:(9600, 9728) loss= 0.351354509592\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35626077652\n",
      "range:(9600, 9728) loss= 0.35149744153\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 240\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.351238310337\n",
      "range:(9600, 9728) loss= 0.348953008652\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334276795387\n",
      "range:(9600, 9728) loss= 0.35199213028\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.348157733679\n",
      "range:(9600, 9728) loss= 0.359178125858\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358441710472\n",
      "range:(9600, 9728) loss= 0.35122320056\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357551157475\n",
      "range:(9600, 9728) loss= 0.349544525146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 241\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348365426064\n",
      "range:(9600, 9728) loss= 0.348875522614\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333141237497\n",
      "range:(9600, 9728) loss= 0.352760732174\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347747385502\n",
      "range:(9600, 9728) loss= 0.357989042997\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358650535345\n",
      "range:(9600, 9728) loss= 0.351032853127\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356008887291\n",
      "range:(9600, 9728) loss= 0.350119411945\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 242\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349716484547\n",
      "range:(9600, 9728) loss= 0.349637359381\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332285225391\n",
      "range:(9600, 9728) loss= 0.351797670126\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347689300776\n",
      "range:(9600, 9728) loss= 0.359365612268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360383093357\n",
      "range:(9600, 9728) loss= 0.354415535927\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358942329884\n",
      "range:(9600, 9728) loss= 0.35034352541\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 243\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348545491695\n",
      "range:(9600, 9728) loss= 0.349189519882\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331227481365\n",
      "range:(9600, 9728) loss= 0.350136637688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346057921648\n",
      "range:(9600, 9728) loss= 0.359561800957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358822077513\n",
      "range:(9600, 9728) loss= 0.35004684329\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356031477451\n",
      "range:(9600, 9728) loss= 0.352172315121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 244\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349459260702\n",
      "range:(9600, 9728) loss= 0.348115235567\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332289099693\n",
      "range:(9600, 9728) loss= 0.351081252098\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347062945366\n",
      "range:(9600, 9728) loss= 0.359659820795\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359722137451\n",
      "range:(9600, 9728) loss= 0.356653898954\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.36158144474\n",
      "range:(9600, 9728) loss= 0.350812792778\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 245\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350218683481\n",
      "range:(9600, 9728) loss= 0.34716925025\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331294596195\n",
      "range:(9600, 9728) loss= 0.352693319321\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346062064171\n",
      "range:(9600, 9728) loss= 0.361934125423\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359254181385\n",
      "range:(9600, 9728) loss= 0.350874185562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356119185686\n",
      "range:(9600, 9728) loss= 0.34866091609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 246\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349748134613\n",
      "range:(9600, 9728) loss= 0.348556876183\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33155721426\n",
      "range:(9600, 9728) loss= 0.352569520473\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347008734941\n",
      "range:(9600, 9728) loss= 0.359162628651\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358353614807\n",
      "range:(9600, 9728) loss= 0.353408455849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359170168638\n",
      "range:(9600, 9728) loss= 0.348551809788\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 247\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350443959236\n",
      "range:(9600, 9728) loss= 0.349462449551\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333390444517\n",
      "range:(9600, 9728) loss= 0.35209813714\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347663670778\n",
      "range:(9600, 9728) loss= 0.359663844109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359347373247\n",
      "range:(9600, 9728) loss= 0.350666105747\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356224179268\n",
      "range:(9600, 9728) loss= 0.348652899265\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 248\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349493741989\n",
      "range:(9600, 9728) loss= 0.349940657616\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.33404135704\n",
      "range:(9600, 9728) loss= 0.352269768715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.345468640327\n",
      "range:(9600, 9728) loss= 0.361449897289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.3636302948\n",
      "range:(9600, 9728) loss= 0.351955890656\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.3567070961\n",
      "range:(9600, 9728) loss= 0.348041802645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 249\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348813772202\n",
      "range:(9600, 9728) loss= 0.347076058388\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331836998463\n",
      "range:(9600, 9728) loss= 0.351071536541\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347744941711\n",
      "range:(9600, 9728) loss= 0.360606580973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359375119209\n",
      "range:(9600, 9728) loss= 0.354701638222\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.359156638384\n",
      "range:(9600, 9728) loss= 0.350277215242\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 250\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348339915276\n",
      "range:(9600, 9728) loss= 0.34936991334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334248542786\n",
      "range:(9600, 9728) loss= 0.350962609053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346646487713\n",
      "range:(9600, 9728) loss= 0.359596192837\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360164225101\n",
      "range:(9600, 9728) loss= 0.350793749094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.35645955801\n",
      "range:(9600, 9728) loss= 0.347908258438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 251\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348655223846\n",
      "range:(9600, 9728) loss= 0.348240733147\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331839948893\n",
      "range:(9600, 9728) loss= 0.350073039532\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.345395803452\n",
      "range:(9600, 9728) loss= 0.358298242092\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358052670956\n",
      "range:(9600, 9728) loss= 0.353568643332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356435477734\n",
      "range:(9600, 9728) loss= 0.348904311657\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 252\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349181711674\n",
      "range:(9600, 9728) loss= 0.347854584455\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331256628036\n",
      "range:(9600, 9728) loss= 0.349884599447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346105396748\n",
      "range:(9600, 9728) loss= 0.361868679523\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359643518925\n",
      "range:(9600, 9728) loss= 0.351809412241\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357225596905\n",
      "range:(9600, 9728) loss= 0.347561150789\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 253\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348159074783\n",
      "range:(9600, 9728) loss= 0.346802651882\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331314861774\n",
      "range:(9600, 9728) loss= 0.34914162755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346480607986\n",
      "range:(9600, 9728) loss= 0.358330965042\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358283281326\n",
      "range:(9600, 9728) loss= 0.352950632572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.358334839344\n",
      "range:(9600, 9728) loss= 0.351342499256\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 254\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349154919386\n",
      "range:(9600, 9728) loss= 0.348350048065\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335029542446\n",
      "range:(9600, 9728) loss= 0.351672202349\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347489416599\n",
      "range:(9600, 9728) loss= 0.359855800867\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359482795\n",
      "range:(9600, 9728) loss= 0.350614786148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356318593025\n",
      "range:(9600, 9728) loss= 0.347228288651\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 255\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348574280739\n",
      "range:(9600, 9728) loss= 0.349073052406\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334073096514\n",
      "range:(9600, 9728) loss= 0.350900948048\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.347268640995\n",
      "range:(9600, 9728) loss= 0.363010525703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.36000585556\n",
      "range:(9600, 9728) loss= 0.354326665401\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.360684931278\n",
      "range:(9600, 9728) loss= 0.347528278828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 256\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348386883736\n",
      "range:(9600, 9728) loss= 0.349854797125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335887551308\n",
      "range:(9600, 9728) loss= 0.35117661953\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.345163226128\n",
      "range:(9600, 9728) loss= 0.359745740891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358782947063\n",
      "range:(9600, 9728) loss= 0.350506305695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356784045696\n",
      "range:(9600, 9728) loss= 0.34862396121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 257\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349159836769\n",
      "range:(9600, 9728) loss= 0.34959924221\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334235221148\n",
      "range:(9600, 9728) loss= 0.350062072277\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346609741449\n",
      "range:(9600, 9728) loss= 0.359793484211\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359672427177\n",
      "range:(9600, 9728) loss= 0.350981414318\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356988996267\n",
      "range:(9600, 9728) loss= 0.34910094738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 258\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348671793938\n",
      "range:(9600, 9728) loss= 0.34801068902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.332705378532\n",
      "range:(9600, 9728) loss= 0.350950956345\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.345570027828\n",
      "range:(9600, 9728) loss= 0.360456049442\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.360629379749\n",
      "range:(9600, 9728) loss= 0.351249188185\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.357472956181\n",
      "range:(9600, 9728) loss= 0.348685920238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 259\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.349169492722\n",
      "range:(9600, 9728) loss= 0.34955728054\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.334374189377\n",
      "range:(9600, 9728) loss= 0.350460410118\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346594661474\n",
      "range:(9600, 9728) loss= 0.359620958567\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358374387026\n",
      "range:(9600, 9728) loss= 0.351584255695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356504619122\n",
      "range:(9600, 9728) loss= 0.350398629904\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 260\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350147068501\n",
      "range:(9600, 9728) loss= 0.347314566374\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.331996381283\n",
      "range:(9600, 9728) loss= 0.350943058729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.345869004726\n",
      "range:(9600, 9728) loss= 0.358161091805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.358677178621\n",
      "range:(9600, 9728) loss= 0.350291192532\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.355745941401\n",
      "range:(9600, 9728) loss= 0.348581016064\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 261\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.348243892193\n",
      "range:(9600, 9728) loss= 0.349671423435\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.335283815861\n",
      "range:(9600, 9728) loss= 0.351842105389\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.345308482647\n",
      "range:(9600, 9728) loss= 0.358509719372\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359361886978\n",
      "range:(9600, 9728) loss= 0.351606965065\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.356559157372\n",
      "range:(9600, 9728) loss= 0.349375128746\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 262\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 128) loss= 0.350107401609\n",
      "range:(9600, 9728) loss= 0.348236262798\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 128) loss= 0.333157867193\n",
      "range:(9600, 9728) loss= 0.351807594299\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 128) loss= 0.346504032612\n",
      "range:(9600, 9728) loss= 0.359490483999\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 128) loss= 0.359884589911\n",
      "range:(9600, 9728) loss= 0.351747334003\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 128) loss= 0.355910122395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-2f0e5eebbcdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mminX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mminY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mminX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mminY\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m75\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 0\n",
    "    for ep in range(205, 100 + no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(float(len(batch_images)) / min_batch_size)):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                if(index % 75 == 0):\n",
    "                    print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_1\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's visualize the representation of a random image and it's reconstructed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph = computation_graph) as sess:\n",
    "    # load the weights from the model1\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # instead of global variable initializer, restore the graph:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    prediction = sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "    inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "    \n",
    "    random_image = batch_data[np.random.randint(len(batch_data))]\n",
    "    reconstructed_image = sess.run(prediction, feed_dict={inputs: np.array([random_image])})[0]\n",
    "    \n",
    "    # plot the two images with their titles:\n",
    "    plt.figure().suptitle(\"Original Image\")\n",
    "    plt.imshow(random_image, interpolation='none')\n",
    "    \n",
    "    plt.figure().suptitle(\"Reconstructed Image\")\n",
    "    plt.imshow(reconstructed_image, interpolation='none')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
