{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 500 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 5 # all kernels are 5x5\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 5000 # we look at 5000 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ6O48TtORvn/+93jeEK1F1IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc465b7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc8fec41d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEVCAYAAAAPaTtOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyTBuABwnw7gBcJwM4wbAcTLM/wMnotFgRToDiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc4a073dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv+OklP8PJC/ttLaaVQ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc49e64790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypefloat32\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[ 0.85882354  0.8509804   0.82352942]\n",
      " [ 0.84705883  0.84705883  0.81960785]\n",
      " [ 0.8392157   0.83529413  0.82352942]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHk1JREFUeJztnWusXNd13//rnHnd94sUSVGyqFfQKkEsG4TgIkbgJkig\nGAFkA4FhFzD0wYiCIgZqIP0guEDsAv3gFLENfyhc0JUQpXX9aGzDQmG0cQUDQr7Iph1ZlqXalhXJ\nEkmR4uV93zsz57H6YYYFRe//vkNe3rmS9v8HEJy71+yz9+xz1pyZ/Z+1lrk7hBDpkR30BIQQB4Oc\nX4hEkfMLkShyfiESRc4vRKLI+YVIFDm/EIki5xciUeT8QiRKYy+dzex+AF8AkAP4L+7+mdjzF2fn\n/fhNNwdtsd8Zel1fz9z48WJjOR+rLqtwe1XGZsIteU5tWXad78vsF5t8GojeAyLrGFvImp4z3on3\nAWK/RI2+NLYckddlWcRmfK0sds5i60jGM4++siBnXz+L1Y2VkTpet/ObWQ7gPwH4AwCvAviBmT3u\n7s+xPsdvuhnf+uv/FrTFTnyv1w/PIXLx5S3+0orIWP2ix22ry8H2rZU12qeOXBCtuRlq67SmqC0H\nf9OovQi2R6/LvM2PZx1uq8JvhgDQ64bXsS7D8wOAbm+L2sqC97MqciFU4XPdarVol2abr0erM0lt\njTZfK7Sb1JSR8fKa9wFZ+4/+5b/ifa4ed+Rn/jr3AXjB3V909z6ArwJ4YA/HE0KMkb04/3EAr1zx\n96vDNiHEW4B93/Azs4fM7LSZnb60vrLfwwkhRmQvzn8GwK1X/H3LsO0NuPspdz/p7icXZxf2MJwQ\n4kayF+f/AYC7zex2M2sB+DCAx2/MtIQQ+8117/a7e2lmHwfwvzGQ+h5195/u0gdVEZPFSD+yY1uS\ndgDo9XaorazC6gEA9LY3qa27Fv7aUuV8V7azdJja2rNL1AYiKwJAf4fvijvZFc9r/pqRc1vdjO2k\nR0y98Hnejsx9p3id2ho5v1TrPle2MoTPTR4RCLOILNevI+sRkeayWDeyc1+VvFNGJOlrSc6zJ53f\n3b8D4Dt7OYYQ4mDQL/yESBQ5vxCJIucXIlHk/EIkipxfiETZ027/NeM8Mi4WuceimxoZD3Dp97ap\nrdxep7aqy/s1WuGgjtbsHO3TnJ2ntqzBg0v6JQ8wisYQEqmv2tmgfTY3+C8vqyZf4840f911Fg5y\n6YOv78VLL1PbzCRfx8kWl0wzD1/iEZUYDYtEW0bOGTIu+dYRCc7L8BnNPXI8Jjleg9SnO78QiSLn\nFyJR5PxCJIqcX4hEkfMLkShj3e33ukZJ0juxPGYAYGwXNbazGVEPqoIHsmSRvHrN6cWwYXKa9rEs\nssQVT00VC4Aqi4gyQoJBMo/tYHNbt89VgtVz4bRmALDO0m61+Xle37zA5xFJr3ZkgafWapPAnlja\nxbzkabzyiExgsfPpkSgocgvO8kgeSjKWRzNUjjSsEOLtjpxfiESR8wuRKHJ+IRJFzi9Eosj5hUiU\n8Qb2AAAJSPA2n4qTAB7vRaSVSJ6+MiIR5h1eKScjVV68jlRHIrnsAKC2SBWaiNSXRyrUeCMsbfVq\nvr5lJECnnXObb/AAqeUzPw+2Xzx/lvaZnONjYZJLbBd756htgeQgbFVcHkSX3xNjxYE6zoN+8jaX\nU41c32Wfr2/GpNQ6IilefYyRnymEeFsh5xciUeT8QiSKnF+IRJHzC5Eocn4hEmVPUp+ZvQRgA4PC\nTaW7n4x2yDJgMpzbDRGpDzQyLiJ5bfH3tUYWidoybmPLxXIMAkCv36W2n734DLVlE2SdACzOz1Jb\nfzMse62e+yc+lnNZceoQLzfWmOeFV2d7YSntwq/WaJ+1Ps/vZ61IhFvGJdNePyx9LUwcoX1m8ohk\nV/Lz0u9yeblj/JhehOdYdPm1w442tnJdQ/6lu1+8AccRQowRfewXIlH26vwO4O/N7Idm9tCNmJAQ\nYjzs9WP/e939jJndBOC7ZvZ/3f3JK58wfFN4CACOHeLfs4QQ42VPd353PzP8/wKAbwG4L/CcU+5+\n0t1PLszwDSIhxHi5buc3sykzm7n8GMAfAnj2Rk1MCLG/7OVj/xEA3zKzy8f57+7+v2IdLMuQT5Bo\nqkhZqIJEuJWRLIyx2Kb2FJfKLOdSTk2kRcv5PC4u/4razr/+C2q77TfvoraVLi+vtU2kPmvwRJw0\nQgxAv+bS58W1HWrbWA4LQL0+T8S5U3GpL9vk52x6gc9xbYMkBY1KYlxWbOTcZYyUBgOAXsVfd27h\n8TyStJTOPpK49mqu2/nd/UUA77ze/kKIg0VSnxCJIucXIlHk/EIkipxfiESR8wuRKGNP4JkZkfQ8\n8j5EIrPqbS6F0HEAtGbnqa1XR5I39laD7avLvMbc5tYr1Hb7XbzGH4oz1LR8lsdRzS8dDbYfuuNu\n2qfX49Fom+tcItx4jdfqKz2cSHT2jtton8mIQLu6FUloGrHVRFpcKV7lx4vUjYw5zHSbJ3+NCXBN\nMl5W8tfVJfJsHStCePXxR36mEOJthZxfiESR8wuRKHJ+IRJFzi9Eoox3t9+MBvDUJQ8uaZASX90+\nDyypM75zHIvpKCLBJa9dfCHYvrbD89JNH+Y7+s4FCdQVf1+ePsx3lSfml4LtvYyXp+pFlJG8ydfx\nxDv4MTc2w2uyU/Fz5s73xPPlsNICAHXB1YqKBOkUPV4Ka6fH1ZTV2LZ9xVWkqYzn8Kur8LXfW79E\n+2yshG39Hr9+r0Z3fiESRc4vRKLI+YVIFDm/EIki5xciUeT8QiTKWKU+N6BshHW2uh+Ra4qwPFQY\nD2LIcp7XLVZC6+Ly89S2XYbz400f4SnJZ48eo7aq4q+5jOSDy+b462Z537o7m7RPscPXY2uNy015\nI7L+JFhlLpLBuRsp15WDS8EekUXXdsJS5aVXeB7EtdWz1Nbk1cuwssEDzZDz191fCwdIrZznwV0T\nEzPB9muo1qU7vxCpIucXIlHk/EIkipxfiESR8wuRKHJ+IRJlV6nPzB4F8McALrj7bw3bFgF8DcAJ\nAC8B+JC7c+3kMu6o+yT3WCxfGYlUqsGj0TLn0XSbGzxqq3Kec+/Q4bB82G/wUK9mpLxTq8FLgxUR\nObKsuG2HrFUj8jafdbgsWkzxdewXPDKuPRmO+OtXfK22e1zOa0by48VKaM1NhNvrLS7PvvLSj6it\n0+Hy7EyT5yd8fZ3LgKtnw9Li4lw4HyMA3HTiN4Ptje9/j/a5mlHu/H8D4P6r2h4G8IS73w3gieHf\nQoi3ELs6v7s/CeDqX3o8AOCx4ePHAHzgBs9LCLHPXO93/iPufvnnbq9hULFXCPEWYs8bfu7uiFQM\nNrOHzOy0mZ1eWdt9W0AIMR6u1/nPm9kxABj+T6tWuPspdz/p7icX5vjvm4UQ4+V6nf9xAA8OHz8I\n4Ns3ZjpCiHExitT3FQDvA3DIzF4F8CkAnwHwdTP7GICXAXxolMHcHVURlvSqSMkoL8PfKpoZn37Z\n5WWmeuvnqW17i0d0dXfCMlXZOkT7tNs8yWWjSXQoAOuXuBy5vc2j31ZJea3pmVnap9nkc5ydnqO2\ntY1IVGIRXqtmg8uKec3PZxVJMtol8jEAtPPwtTM3z9ejOHwLtW2u8BJlUzfx89IFt00cCSf+PHzk\nTtqnNRsOL7SI7Hk1uz7T3T9CTL8/8ihCiDcd+oWfEIki5xciUeT8QiSKnF+IRJHzC5Eo463V5w6Q\nqK66jNTWI0pO2eVJKbc2X6O2/gZPSlltcdloazUsEZ7b+Cfa58wrv6K2paWbqG1iiktRnQkeaTcz\n2Qy2b6zzWneTHZ71cToiVTbrcJJOADCSwLPc5BGJOV96ZJEIyDznMqCRyMNun0vBE+1wvUMAaPT5\nemxt8GNigtfqm5k5Hmyv27yPNcPra/yU/Bq68wuRKHJ+IRJFzi9Eosj5hUgUOb8QiSLnFyJRxiz1\nAVVJIuN6XALq74QjqTaXeQTezkq4rh4A9Da4RFgWXPbqLIXro91+lCeXvNRdo7bnf8plwIl5Hk13\nz2/fQ21ZFn4/rwuuo21GogS9xeWm7a1wDUUA6BBpy3neFwBhmRIAyh4/Z3nO72F5g8yj5uvRr7it\nNcHPdW+Dr0e7yV2t9rA+V0S8s8rDfkQOFUR3fiESRc4vRKLI+YVIFDm/EIki5xciUca62+/uKLvh\nvG91ZMe5v/56sH3n0hnaZ3uZ5+nbIXnuAKBXRXaj++EAkqnD/D308AIPEllv89JVP/s5Lxt29izP\n79cmeQGnOry0Qh7ZIZ6d4pfI6xe4klEUYfVmYpqrB4tHb6a26Zmw0gLEcyFSImXDaue5CbcLHoBW\n7PB+i/PhnHsAUPbDx4xdivW1RPAQdOcXIlHk/EIkipxfiESR8wuRKHJ+IRJFzi9EooxSrutRAH8M\n4IK7/9aw7dMA/hTAZQ3uk+7+nd2O5XWNohcOfqgiUl+9uRVu3wrnZwOAYpNXBN7Z5BIVGjw/Xqc+\nFmx3HnOCrUgZskMNXuZr6i4uEa7u8Hx8r10I5y6sMp4Db2U9vL6DjtdXXDUjefXqiktlZY/P40Lk\nfC4uLFJbXYclve4Gv97KHg/scef3y8q4NrceWeOpxXC5LudLhYpE8MTCpq5mlDv/3wC4P9D+eXe/\nd/hvV8cXQry52NX53f1JADzdrRDiLclevvN/3MyeMbNHzez6PhsKIQ6M63X+LwK4E8C9AM4B+Cx7\nopk9ZGanzez0auy7thBirFyX87v7eXev3L0G8CUA90Wee8rdT7r7yflIrXchxHi5Luc3syu3vT8I\n4NkbMx0hxLgYRer7CoD3AThkZq8C+BSA95nZvRgoCy8B+LNRBnOvUZFor6Ls0X5VXQbb60iEVbfL\nj9crw8cDgGabR0vVIGWh6khZpYLnpaN1yAB0IhF/tx3iEXoNhI+5VXDJsRGpk1X0eb+JCR5p1y/C\n69ju8PUttnkex5hEuHzuArW5h8Uv73FRrI5IdpciZc/axqMLLSItNjvhNfa5WDk0dl2NHu23q/O7\n+0cCzY+MPIIQ4k2JfuEnRKLI+YVIFDm/EIki5xciUeT8QiTKmMt1OSoiORVEzgOAiqgXRc1ljW7B\npbKsNUltiJSnKiwsH3oVeQ+tuc1yLg0Vm1yqvHSRh1oskwjIQ8cj8uACj2RcXuHSVqczS20VSZC5\nucFf13SHl8IqSZk3ACgi0m1FJMK1ZR4l2Jrk18DsUjgCDwCW5sJRnwDQLHlUJWrihpEQPctJn2tI\n7Kk7vxCJIucXIlHk/EIkipxfiESR8wuRKHJ+IRJlzLX6alTdsBTlfR5ZVpHovVj0lTX4S2s2IpF2\nWSRCz4nc5FxqKmNSn/M51pHorCxrU1srC8uH1uNznJzg83iZSIcA4M7lMlZbb3WZS4eZ87WP5LKM\nJknNmuFIzK2M95md44mpFiIRlZ0JLplaj69xuUWukch1VdPsnqOn8NSdX4hEkfMLkShyfiESRc4v\nRKLI+YVIlPHu9tcViu1wia1ih+8qV0VYCahrvrPZbPCd40aLB1mURvL0AXAyXl1ypSK2a19XEYUj\nUquJ5UEcGslgfHe70+S71L3tcHk1ANhYD5cGA4DOxkZ4GpF9+xJ8rCmiHgDA9MJhauuX4df9G/fw\nPq0GP2frKzyoaicSBNWqeTDZZDOsIFjFd/vpeSY5C0Pozi9Eosj5hUgUOb8QiSLnFyJR5PxCJIqc\nX4hEGaVc160A/hbAEQyiBk65+xfMbBHA1wCcwKBk14c8FumBgdRXbYflkLLL5auyx/K+cVmu0eDB\nL3nOA3sq58dkMopHSklVERmwS2QoAPBI0FK3v0ltvW5YYms2uLw52eGyaF1wuWllNSzbAsDhifB4\nR99xM+0zM7NEbTuR8muHlg5RW7sZvsSrikvLy+fPUVsZya1okWun7vFrZHo+nBcwj1xXiOS8HJVR\n7vwlgL9w93sAvAfAn5vZPQAeBvCEu98N4Inh30KItwi7Or+7n3P3Hw0fbwB4HsBxAA8AeGz4tMcA\nfGC/JimEuPFc03d+MzsB4F0AngJwxN0vfz56DYOvBUKItwgjO7+ZTQP4BoBPuPsbvuz5oA5y8Euq\nmT1kZqfN7PT6Nv+eJYQYLyM5v5k1MXD8L7v7N4fN583s2NB+DECwSLq7n3L3k+5+cnaSF2UQQoyX\nXZ3fzAzAIwCed/fPXWF6HMCDw8cPAvj2jZ+eEGK/GCWq73cAfBTAT8zs6WHbJwF8BsDXzexjAF4G\n8KHdDlSXBTYvBT8goCq4tMUC3Nx59JVF3teqko9VRaKijORHKyMRc/2CR6pt74RlOQAoIlF9Nbh8\nODMVlu3aLb5WvS0uHU61uUTYPDpHbXfc88+D7Y1JLrNurvOvhbOTvLTZwhSXdVlZrvNnXqJ9YgkD\nD88f57alW6nt/C8j4xGZO4tFfRakdBzLMxlgV+d3938AaFzq7488khDiTYV+4SdEosj5hUgUOb8Q\niSLnFyJR5PxCJMpYE3iW/R4uvvxC0Nbp8CSSzVY4+WFBo/2AfskljxI8Iqpfc53HSYRbN5JQc6vk\nUl+dReS8nEuOs/N8rWZniSQWSXa6vsYlx2bOL5GbT7yD2trtcL/NDR4JON2MRFt2uRz5y+d5ItGi\nH5ZhWzm/7y0t3EJtRw79BrV1Olz63J6NyLqrF4Pt3o9cH6y8XeQ8X43u/EIkipxfiESR8wuRKHJ+\nIRJFzi9Eosj5hUiU8Up9RYFL58KyzML8Iu83EZbSepEApsr4S6ub/D2vrnkkFbKwjJJFart5l8uK\nOc+bidZE5NS0+Avf2Amv1cY6l8peP79MbVmHR9NNdrg019sKS1tVJIKwiNyKujtc1i0ikliWhec4\ns3SU9jl0093UNjPJE5BG1EPMznMZ8AKp/1f3+bVTbofPs9ejR/Xpzi9Eosj5hUgUOb8QiSLnFyJR\n5PxCJMpYd/stb6K1SHZLje9SFiRYocx47rapWV7CqRHJ+bYdKYVVeXiHNS/CJcgAoB3NM8h3qbe2\nt6lte40Hx6yQPHgrG/x4zYzLDq3IjvPGJf66m63wMXura7TPThmZY4Nnfu40wuWuAOD4sbuC7YtH\n76R9JidvoraJZjjIDACqyPxn5riatdw8G2zv9fjxWpEcj6OiO78QiSLnFyJR5PxCJIqcX4hEkfML\nkShyfiESZVepz8xuBfC3GJTgdgCn3P0LZvZpAH8K4PXhUz/p7t+JDtZsYfFouKTR1vlXab88D0tz\nrWkun7RmDvPjTfBglfl5LvPAw9LL+lZYqgGAnZ2ItLXFg4h6vZza1lbCOd8A4MLFsAxYOpfsJhb5\netRcqcTZc+eorU2kvkYkxVyrzXMTLi7wUlhHD4flvEG/cD6+iTl+nlnOSADII+XcSuPyW9Xkry1v\nhtffI8drNcMnJoucr6sZRecvAfyFu//IzGYA/NDMvju0fd7d/3r04YQQbxZGqdV3DsC54eMNM3se\nAK9WKIR4S3BN3/nN7ASAdwF4atj0cTN7xsweNbOFGzw3IcQ+MrLzm9k0gG8A+IS7rwP4IoA7AdyL\nwSeDz5J+D5nZaTM7vdnj+e2FEONlJOc3syYGjv9ld/8mALj7eXevfFAQ/EsA7gv1dfdT7n7S3U9O\nR2q9CyHGy67Ob2YG4BEAz7v7565oP3bF0z4I4NkbPz0hxH4xym7/7wD4KICfmNnTw7ZPAviImd2L\ngfz3EoA/2+1AWZ5jajoseaxd4NLWxEw4aquzeIT2mZzhtrzDI8RyIqEAQLEVzrWWR5K3zR86QW1b\nWzwH3vY6L+/UnuSyaFE/F2xfvvgyP16bR/U15njuudK47MWqjS1M8a2hufnbqO2mQ7w02MI8z6vX\nas4E25ttvvYWKaNmkXJYeYOvY7PFpb72ZNhW1fwa4BGho5frGmW3/x8AhDwiqukLId7c6Bd+QiSK\nnF+IRJHzC5Eocn4hEkXOL0SijDWBZ5Y30VkIl0nqLKzQflOL4WSc7UUemdWZnOXzaPEfG1nOJcfG\nRDjaa8p46ScPCiXDflz9QbnE5abZpSVqs1a439Q8l7Y6MzyKrU/KXQHAwhSXtnKSj3U6khzzyPF3\n8rHmeZRmI5IkNSfrnzlPGOslt1UVt2XGr52sGZEBJ8Ny5FYkQWrd7wfbPRJ1+GtzGvmZQoi3FXJ+\nIRJFzi9Eosj5hUgUOb8QiSLnFyJRxiv1ZQ1MTYdlu4kpHoXXbpGovgbXyhoWkVYiNf7yJrd5Myx7\nec3ln7Ifrp0HAM18h9qKkss8veICtc0uhGXMfsnlvNUVnmR0ZobLihM1l7Za7XA04OHDd9A+s7OR\npKsRyTGreHJSL3rh9ppf+nnGX1dMuvU6Uj8vEgHZ7oTP2XZEtasKkvxVUp8QYjfk/EIkipxfiESR\n8wuRKHJ+IRJFzi9EooxV6kMO2Ax5v5nmkXbdLNyn1eCyS0SFQpVxOYQMBQBokOSNVUTiaeZ8ia3N\nZcWsx+c4PcWTak5OhCPEuhvhKDAAWF7ltf/aDV5rYXImlowzXCNvboZHQGYxmarg87eI1Grk3Fik\npl0sIWuMKjL/WBThBEmgulJHJMxKUX1CiOtEzi9Eosj5hUgUOb8QiSLnFyJRdt3tN7MOgCcBtIfP\n/zt3/5SZ3Q7gqwCWAPwQwEfdnW/JDo6FRjM85GSLb79mHt71bBrfZW+A75TmEVsD4UCQQb9w+SSL\nbB3XxgNSAB58NDdznNomZ3npqp0ivCY3F7xE2eoZvtvfaPDd46VIubSpuXAAV6fN5xEuDDUgFrzT\niOykZ6x8VSyHX0WCZhDfTa8rfj3Wkdx/OQn6ic2j390O94m8rqsZ5c7fA/B77v5ODMpx329m7wHw\nVwA+7+53AVgB8LGRRxVCHDi7Or8P2Bz+2Rz+cwC/B+Dvhu2PAfjAvsxQCLEvjPSd38zyYYXeCwC+\nC+CXAFbd///n8VcB8M+pQog3HSM5v7tX7n4vgFsA3Afgn406gJk9ZGanzez02hYvOSyEGC/XtNvv\n7qsAvgfgXwCYN7PLu3e3ADhD+pxy95PufnJuKvzTUyHE+NnV+c3ssJnNDx9PAPgDAM9j8CbwJ8On\nPQjg2/s1SSHEjWeUwJ5jAB4zsxyDN4uvu/v/NLPnAHzVzP4DgH8E8Miug9kOFprPBW3TSy/Tfl6E\no3TydlhOAoBmZ4LaLOcSmzd4vx7CuQTrjMtXZc3HymoezFRHyjtVFnnPzsNq6+QSL1929HaeV2/9\nEs8XiJyvVYPkwbOIHIZIIEssMKaRRaJ0iDRXRWS0kgTNAHFZtyz5/KuKS4QVGa+OBIz1tsO5IWP5\nJK9mV+d392cAvCvQ/iIG3/+FEG9B9As/IRJFzi9Eosj5hUgUOb8QiSLnFyJR7Fpyfu15MLPXAVzW\n9A4B4OFk40PzeCOaxxt5q83jNnfndc+uYKzO/4aBzU67+8kDGVzz0Dw0D33sFyJV5PxCJMpBOv+p\nAxz7SjSPN6J5vJG37TwO7Du/EOJg0cd+IRLlQJzfzO43s5+Z2Qtm9vBBzGE4j5fM7Cdm9rSZnR7j\nuI+a2QUze/aKtkUz+66Z/WL4P6+Ftb/z+LSZnRmuydNm9v4xzONWM/uemT1nZj81s38zbB/rmkTm\nMdY1MbOOmX3fzH48nMe/H7bfbmZPDf3ma2bGQz9Hwd3H+g9AjkEasDswSF/7YwD3jHsew7m8BODQ\nAYz7uwDeDeDZK9r+I4CHh48fBvBXBzSPTwP4t2Nej2MA3j18PAPg5wDuGfeaROYx1jXBII3x9PBx\nE8BTAN4D4OsAPjxs/88A/vVexjmIO/99AF5w9xd9kOr7qwAeOIB5HBju/iSAS1c1P4BBIlRgTAlR\nyTzGjrufc/cfDR9vYJAs5jjGvCaReYwVH7DvSXMPwvmPA3jlir8PMvmnA/h7M/uhmT10QHO4zBF3\nPzd8/BoAnhR///m4mT0z/Fqw718/rsTMTmCQP+IpHOCaXDUPYMxrMo6kualv+L3X3d8N4I8A/LmZ\n/e5BTwgYvPMDrNrEvvNFAHdiUKPhHIDPjmtgM5sG8A0An3D39Stt41yTwDzGvia+h6S5o3IQzn8G\nwK1X/E2Tf+437n5m+P8FAN/CwWYmOm9mxwBg+H8kf9b+4e7nhxdeDeBLGNOamFkTA4f7srt/c9g8\n9jUJzeOg1mQ49jUnzR2Vg3D+HwC4e7hz2QLwYQCPj3sSZjZlZjOXHwP4QwDPxnvtK49jkAgVOMCE\nqJedbcgHMYY1sUFivEcAPO/un7vCNNY1YfMY95qMLWnuuHYwr9rNfD8GO6m/BPDvDmgOd2CgNPwY\nwE/HOQ8AX8Hg42OBwXe3j2FQ8/AJAL8A8H8ALB7QPP4rgJ8AeAYD5zs2hnm8F4OP9M8AeHr47/3j\nXpPIPMa6JgB+G4OkuM9g8Ebzl1dcs98H8AKA/wGgvZdx9As/IRIl9Q0/IZJFzi9Eosj5hUgUOb8Q\niSLnFyJR5PxCJIqcX4hEkfMLkSj/D5ADwQ+YmuEYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc49def550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2, tf.shape(a1), tf.shape(a2), tf.shape(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_, sha1, sha2, sha3 = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(inp, sha1, sha2, sha3):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, sha3, \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, sha2,\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, sha1,\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.shape(tf_input),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_, sha1, sha2, sha3)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector, sha1, sha2, sha3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../Models/IDEA_1/Model_cifar_2/model_cifar_2-600\n",
      "epoch: 601\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 145.202911377\n",
      "range:(5000, 10000) loss= 138.055938721\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 135.607254028\n",
      "range:(5000, 10000) loss= 110.583679199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 102.144805908\n",
      "range:(5000, 10000) loss= 91.3598327637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 111.607070923\n",
      "range:(5000, 10000) loss= 102.20539856\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 105.816116333\n",
      "range:(5000, 10000) loss= 116.615951538\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 602\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 98.8528823853\n",
      "range:(5000, 10000) loss= 103.224380493\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 118.666465759\n",
      "range:(5000, 10000) loss= 103.649078369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 94.0255813599\n",
      "range:(5000, 10000) loss= 115.337509155\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 127.210479736\n",
      "range:(5000, 10000) loss= 89.9520874023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 102.626281738\n",
      "range:(5000, 10000) loss= 91.766242981\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 603\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 79.5123901367\n",
      "range:(5000, 10000) loss= 109.247161865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 134.223175049\n",
      "range:(5000, 10000) loss= 91.6471633911\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 127.944511414\n",
      "range:(5000, 10000) loss= 161.521133423\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 151.137802124\n",
      "range:(5000, 10000) loss= 97.9039001465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 136.90687561\n",
      "range:(5000, 10000) loss= 120.600357056\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 604\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 110.873657227\n",
      "range:(5000, 10000) loss= 110.434120178\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 100.422973633\n",
      "range:(5000, 10000) loss= 83.7958908081\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 77.2566986084\n",
      "range:(5000, 10000) loss= 88.7044448853\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 83.5966339111\n",
      "range:(5000, 10000) loss= 80.6829833984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 65.0530395508\n",
      "range:(5000, 10000) loss= 74.9561920166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 605\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 70.23777771\n",
      "range:(5000, 10000) loss= 56.5864715576\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 108.012008667\n",
      "range:(5000, 10000) loss= 90.9366607666\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 138.633270264\n",
      "range:(5000, 10000) loss= 113.088401794\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 153.98248291\n",
      "range:(5000, 10000) loss= 185.648376465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 164.233917236\n",
      "range:(5000, 10000) loss= 141.088943481\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 606\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 206.4140625\n",
      "range:(5000, 10000) loss= 151.63142395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 143.885192871\n",
      "range:(5000, 10000) loss= 142.978317261\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 154.897323608\n",
      "range:(5000, 10000) loss= 160.215255737\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 130.512680054\n",
      "range:(5000, 10000) loss= 158.497436523\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 144.585525513\n",
      "range:(5000, 10000) loss= 127.612533569\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 607\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 111.491264343\n",
      "range:(5000, 10000) loss= 112.735359192\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 79.2690429688\n",
      "range:(5000, 10000) loss= 69.0717926025\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 81.1454925537\n",
      "range:(5000, 10000) loss= 154.167541504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 148.905197144\n",
      "range:(5000, 10000) loss= 134.211074829\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 138.263702393\n",
      "range:(5000, 10000) loss= 110.830734253\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 608\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 130.94909668\n",
      "range:(5000, 10000) loss= 100.038009644\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 134.83694458\n",
      "range:(5000, 10000) loss= 95.7664260864\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 97.1055297852\n",
      "range:(5000, 10000) loss= 75.2562103271\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 95.7189712524\n",
      "range:(5000, 10000) loss= 130.992507935\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 87.1145172119\n",
      "range:(5000, 10000) loss= 100.435874939\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 609\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 92.7478179932\n",
      "range:(5000, 10000) loss= 83.3688735962\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 85.9897537231\n",
      "range:(5000, 10000) loss= 75.6122055054\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 66.4296188354\n",
      "range:(5000, 10000) loss= 63.2812805176\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 65.2203826904\n",
      "range:(5000, 10000) loss= 66.9682312012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 63.5657539368\n",
      "range:(5000, 10000) loss= 56.9005317688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 610\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 57.6095466614\n",
      "range:(5000, 10000) loss= 63.0784606934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 68.4360275269\n",
      "range:(5000, 10000) loss= 88.3224563599\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 66.296295166\n",
      "range:(5000, 10000) loss= 96.5268707275\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 83.4277954102\n",
      "range:(5000, 10000) loss= 85.378288269\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 151.71244812\n",
      "range:(5000, 10000) loss= 122.644355774\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 611\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 164.748519897\n",
      "range:(5000, 10000) loss= 153.547775269\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 157.870391846\n",
      "range:(5000, 10000) loss= 155.275283813\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 119.634597778\n",
      "range:(5000, 10000) loss= 120.074623108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 116.183906555\n",
      "range:(5000, 10000) loss= 118.026603699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 108.559593201\n",
      "range:(5000, 10000) loss= 94.2579040527\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 612\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 88.1034317017\n",
      "range:(5000, 10000) loss= 90.6199264526\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 77.0299072266\n",
      "range:(5000, 10000) loss= 63.3817520142\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 68.6425552368\n",
      "range:(5000, 10000) loss= 51.8092460632\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 63.498840332\n",
      "range:(5000, 10000) loss= 72.4331512451\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 115.332015991\n",
      "range:(5000, 10000) loss= 80.2250061035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 613\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 99.7339172363\n",
      "range:(5000, 10000) loss= 97.4981460571\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 91.4078979492\n",
      "range:(5000, 10000) loss= 89.4313278198\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 69.4048919678\n",
      "range:(5000, 10000) loss= 74.6894607544\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 69.4030838013\n",
      "range:(5000, 10000) loss= 63.2400932312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 63.4710769653\n",
      "range:(5000, 10000) loss= 63.7667160034\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 614\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 61.4691772461\n",
      "range:(5000, 10000) loss= 42.4954109192\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 51.1391792297\n",
      "range:(5000, 10000) loss= 38.115447998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 53.0127182007\n",
      "range:(5000, 10000) loss= 57.5135116577\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 50.8842430115\n",
      "range:(5000, 10000) loss= 69.4990921021\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 50.7966499329\n",
      "range:(5000, 10000) loss= 58.2868843079\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 615\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 55.579120636\n",
      "range:(5000, 10000) loss= 57.294380188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 49.6503372192\n",
      "range:(5000, 10000) loss= 57.396156311\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 90.934425354\n",
      "range:(5000, 10000) loss= 58.7723655701\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 71.8916168213\n",
      "range:(5000, 10000) loss= 59.4284591675\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 52.4462890625\n",
      "range:(5000, 10000) loss= 69.8124237061\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 616\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 61.4860649109\n",
      "range:(5000, 10000) loss= 55.6631011963\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 63.0799751282\n",
      "range:(5000, 10000) loss= 61.6687545776\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 54.9166717529\n",
      "range:(5000, 10000) loss= 58.1228179932\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 47.2297134399\n",
      "range:(5000, 10000) loss= 45.3687591553\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 48.46043396\n",
      "range:(5000, 10000) loss= 40.7229576111\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 617\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 45.9200973511\n",
      "range:(5000, 10000) loss= 47.7506332397\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 38.7023544312\n",
      "range:(5000, 10000) loss= 44.3846321106\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 41.8429908752\n",
      "range:(5000, 10000) loss= 40.4564590454\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 37.3114013672\n",
      "range:(5000, 10000) loss= 37.6597175598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 34.383190155\n",
      "range:(5000, 10000) loss= 37.5162200928\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 618\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 37.524723053\n",
      "range:(5000, 10000) loss= 63.9408073425\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 50.4856491089\n",
      "range:(5000, 10000) loss= 48.6982154846\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 43.6518707275\n",
      "range:(5000, 10000) loss= 51.6275749207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 51.4979400635\n",
      "range:(5000, 10000) loss= 49.781917572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 45.4463310242\n",
      "range:(5000, 10000) loss= 48.2957077026\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 619\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 44.2125015259\n",
      "range:(5000, 10000) loss= 52.1559295654\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 36.0241394043\n",
      "range:(5000, 10000) loss= 51.2640647888\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 44.4743309021\n",
      "range:(5000, 10000) loss= 36.5012283325\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 44.3852310181\n",
      "range:(5000, 10000) loss= 36.9145622253\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 38.2784843445\n",
      "range:(5000, 10000) loss= 41.1364250183\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 620\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 36.8124542236\n",
      "range:(5000, 10000) loss= 37.5150299072\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 36.6821403503\n",
      "range:(5000, 10000) loss= 39.6911964417\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 37.8166465759\n",
      "range:(5000, 10000) loss= 37.9878616333\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 40.1938285828\n",
      "range:(5000, 10000) loss= 35.8458442688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 33.6918144226\n",
      "range:(5000, 10000) loss= 30.8833389282\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 621\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 42.0601272583\n",
      "range:(5000, 10000) loss= 39.0176315308\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 34.7197570801\n",
      "range:(5000, 10000) loss= 36.1639785767\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 36.0354804993\n",
      "range:(5000, 10000) loss= 36.770690918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 33.6349182129\n",
      "range:(5000, 10000) loss= 38.8411750793\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 36.0927886963\n",
      "range:(5000, 10000) loss= 30.7069530487\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 622\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 39.1255874634\n",
      "range:(5000, 10000) loss= 29.8576793671\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 34.3498458862\n",
      "range:(5000, 10000) loss= 27.9941768646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 29.484462738\n",
      "range:(5000, 10000) loss= 33.5156135559\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 29.1244239807\n",
      "range:(5000, 10000) loss= 30.9858512878\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 30.9260368347\n",
      "range:(5000, 10000) loss= 30.0281295776\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 623\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 30.2712650299\n",
      "range:(5000, 10000) loss= 30.6359119415\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 36.3543167114\n",
      "range:(5000, 10000) loss= 29.9746074677\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 32.0565872192\n",
      "range:(5000, 10000) loss= 32.7606506348\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 29.2778205872\n",
      "range:(5000, 10000) loss= 33.3315658569\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 29.2132205963\n",
      "range:(5000, 10000) loss= 30.0074462891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 624\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 31.2360725403\n",
      "range:(5000, 10000) loss= 30.7249011993\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 30.5936126709\n",
      "range:(5000, 10000) loss= 34.1464309692\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 33.2152099609\n",
      "range:(5000, 10000) loss= 31.7463703156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 35.2990493774\n",
      "range:(5000, 10000) loss= 31.0565643311\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 27.0701599121\n",
      "range:(5000, 10000) loss= 31.0077209473\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 625\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 28.5509777069\n",
      "range:(5000, 10000) loss= 27.7839851379\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 26.2534580231\n",
      "range:(5000, 10000) loss= 26.1117534637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 27.0025863647\n",
      "range:(5000, 10000) loss= 26.5936431885\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 28.1828994751\n",
      "range:(5000, 10000) loss= 25.3967094421\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 28.2517223358\n",
      "range:(5000, 10000) loss= 26.733171463\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 626\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 25.0045413971\n",
      "range:(5000, 10000) loss= 26.8644485474\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 25.7058525085\n",
      "range:(5000, 10000) loss= 25.6879024506\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 25.3991336823\n",
      "range:(5000, 10000) loss= 25.4466381073\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 24.4884147644\n",
      "range:(5000, 10000) loss= 24.3576755524\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 26.1429271698\n",
      "range:(5000, 10000) loss= 22.5640487671\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 627\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 25.9955329895\n",
      "range:(5000, 10000) loss= 23.6212158203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 29.0292663574\n",
      "range:(5000, 10000) loss= 23.7847671509\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 25.6809024811\n",
      "range:(5000, 10000) loss= 22.9018135071\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 29.1794204712\n",
      "range:(5000, 10000) loss= 23.2930908203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 26.9125061035\n",
      "range:(5000, 10000) loss= 26.6267662048\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 628\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 24.2389850616\n",
      "range:(5000, 10000) loss= 31.2331008911\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 25.8705978394\n",
      "range:(5000, 10000) loss= 24.1773147583\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 27.8382701874\n",
      "range:(5000, 10000) loss= 23.9278945923\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 24.9344768524\n",
      "range:(5000, 10000) loss= 25.8724708557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 25.3304729462\n",
      "range:(5000, 10000) loss= 27.8869571686\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 629\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 26.8588581085\n",
      "range:(5000, 10000) loss= 27.2258548737\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 26.4515762329\n",
      "range:(5000, 10000) loss= 22.9480133057\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 26.3634548187\n",
      "range:(5000, 10000) loss= 23.6554927826\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 26.392698288\n",
      "range:(5000, 10000) loss= 28.0730991364\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 28.6414928436\n",
      "range:(5000, 10000) loss= 24.1340923309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 630\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 25.8293323517\n",
      "range:(5000, 10000) loss= 28.3209648132\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 24.071395874\n",
      "range:(5000, 10000) loss= 25.1617736816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 24.9659786224\n",
      "range:(5000, 10000) loss= 24.1263256073\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 26.0838375092\n",
      "range:(5000, 10000) loss= 25.3612365723\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 24.1524620056\n",
      "range:(5000, 10000) loss= 24.4325122833\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 631\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 22.0223026276\n",
      "range:(5000, 10000) loss= 23.4971046448\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 23.1309223175\n",
      "range:(5000, 10000) loss= 22.2856693268\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 22.3661289215\n",
      "range:(5000, 10000) loss= 22.678194046\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 21.5521125793\n",
      "range:(5000, 10000) loss= 21.6664962769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 20.8559799194\n",
      "range:(5000, 10000) loss= 19.9771080017\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 632\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 19.5812797546\n",
      "range:(5000, 10000) loss= 20.6081790924\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 20.8595314026\n",
      "range:(5000, 10000) loss= 20.3290328979\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 21.8033275604\n",
      "range:(5000, 10000) loss= 21.053024292\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 19.9172534943\n",
      "range:(5000, 10000) loss= 20.0002326965\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 19.3133792877\n",
      "range:(5000, 10000) loss= 20.0192394257\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 633\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 21.2533035278\n",
      "range:(5000, 10000) loss= 21.089427948\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 20.4625492096\n",
      "range:(5000, 10000) loss= 20.0715961456\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 21.3418693542\n",
      "range:(5000, 10000) loss= 23.7613143921\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 19.9572105408\n",
      "range:(5000, 10000) loss= 21.3639602661\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 21.8761138916\n",
      "range:(5000, 10000) loss= 21.302154541\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 634\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 21.3945198059\n",
      "range:(5000, 10000) loss= 22.6625232697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 22.3149833679\n",
      "range:(5000, 10000) loss= 22.8266925812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 21.9292755127\n",
      "range:(5000, 10000) loss= 23.3877315521\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 20.8650741577\n",
      "range:(5000, 10000) loss= 25.7044448853\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 22.8813781738\n",
      "range:(5000, 10000) loss= 25.6317310333\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 635\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 20.7292366028\n",
      "range:(5000, 10000) loss= 22.1487369537\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 22.3406829834\n",
      "range:(5000, 10000) loss= 20.9018173218\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 21.5661201477\n",
      "range:(5000, 10000) loss= 20.1995449066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 22.7213821411\n",
      "range:(5000, 10000) loss= 20.0921878815\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 24.5732402802\n",
      "range:(5000, 10000) loss= 21.8922691345\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 636\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 22.7919483185\n",
      "range:(5000, 10000) loss= 20.3111801147\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 20.5040283203\n",
      "range:(5000, 10000) loss= 21.8039569855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 22.10039711\n",
      "range:(5000, 10000) loss= 18.7339191437\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 22.8926792145\n",
      "range:(5000, 10000) loss= 22.5465946198\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 20.979850769\n",
      "range:(5000, 10000) loss= 21.451467514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 637\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 21.9218444824\n",
      "range:(5000, 10000) loss= 20.3565635681\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 21.0309848785\n",
      "range:(5000, 10000) loss= 19.7392024994\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 20.1546707153\n",
      "range:(5000, 10000) loss= 19.3227748871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 18.884935379\n",
      "range:(5000, 10000) loss= 18.9011688232\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 19.3266601562\n",
      "range:(5000, 10000) loss= 19.7883033752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 638\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 22.3954105377\n",
      "range:(5000, 10000) loss= 19.1928253174\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 20.7826366425\n",
      "range:(5000, 10000) loss= 19.8523101807\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 18.7792606354\n",
      "range:(5000, 10000) loss= 19.851890564\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 18.925994873\n",
      "range:(5000, 10000) loss= 20.3625831604\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 19.6061782837\n",
      "range:(5000, 10000) loss= 18.8551177979\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 639\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 17.692533493\n",
      "range:(5000, 10000) loss= 18.4810199738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 18.6307601929\n",
      "range:(5000, 10000) loss= 16.9503250122\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 18.5828208923\n",
      "range:(5000, 10000) loss= 17.5106506348\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 18.8949375153\n",
      "range:(5000, 10000) loss= 17.7270793915\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 18.3166694641\n",
      "range:(5000, 10000) loss= 17.8666057587\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 640\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 19.4103965759\n",
      "range:(5000, 10000) loss= 17.6757030487\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 17.36236763\n",
      "range:(5000, 10000) loss= 18.4774169922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 16.9515800476\n",
      "range:(5000, 10000) loss= 16.4016799927\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 15.7986469269\n",
      "range:(5000, 10000) loss= 15.7650032043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 16.2271099091\n",
      "range:(5000, 10000) loss= 15.4980669022\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 641\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 16.4343299866\n",
      "range:(5000, 10000) loss= 16.8178672791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 16.0977649689\n",
      "range:(5000, 10000) loss= 15.16801548\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 15.9133253098\n",
      "range:(5000, 10000) loss= 14.6718006134\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 17.0381889343\n",
      "range:(5000, 10000) loss= 15.1740179062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 17.9084339142\n",
      "range:(5000, 10000) loss= 17.3456420898\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 642\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 16.6690731049\n",
      "range:(5000, 10000) loss= 17.3063926697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 19.08152771\n",
      "range:(5000, 10000) loss= 17.9761314392\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 19.8151111603\n",
      "range:(5000, 10000) loss= 17.256362915\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 19.0026302338\n",
      "range:(5000, 10000) loss= 18.2118663788\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 18.6518859863\n",
      "range:(5000, 10000) loss= 17.5107593536\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 643\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 18.3942947388\n",
      "range:(5000, 10000) loss= 18.9600524902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 17.7805366516\n",
      "range:(5000, 10000) loss= 18.6213951111\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 18.737651825\n",
      "range:(5000, 10000) loss= 17.8519649506\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 20.2227840424\n",
      "range:(5000, 10000) loss= 17.2921524048\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 18.9131412506\n",
      "range:(5000, 10000) loss= 17.2780723572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 644\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 17.5734176636\n",
      "range:(5000, 10000) loss= 17.8457527161\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 18.1259765625\n",
      "range:(5000, 10000) loss= 21.5692844391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 20.4294471741\n",
      "range:(5000, 10000) loss= 20.7228488922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 18.7519378662\n",
      "range:(5000, 10000) loss= 21.2117958069\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 18.9064559937\n",
      "range:(5000, 10000) loss= 17.2035808563\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 645\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 20.2684192657\n",
      "range:(5000, 10000) loss= 17.1919155121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 17.0248508453\n",
      "range:(5000, 10000) loss= 18.8612957001\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 22.1107025146\n",
      "range:(5000, 10000) loss= 19.7575817108\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 18.3210639954\n",
      "range:(5000, 10000) loss= 17.8913059235\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 17.9920406342\n",
      "range:(5000, 10000) loss= 20.0583496094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 646\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 25.2234725952\n",
      "range:(5000, 10000) loss= 18.734703064\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 21.387342453\n",
      "range:(5000, 10000) loss= 18.9778823853\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 18.1441841125\n",
      "range:(5000, 10000) loss= 20.335817337\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 18.9167041779\n",
      "range:(5000, 10000) loss= 18.2511978149\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 17.4776039124\n",
      "range:(5000, 10000) loss= 17.9800815582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 647\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 20.3964653015\n",
      "range:(5000, 10000) loss= 20.1344642639\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 19.2828426361\n",
      "range:(5000, 10000) loss= 20.7404918671\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 20.2691802979\n",
      "range:(5000, 10000) loss= 23.314283371\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 18.291841507\n",
      "range:(5000, 10000) loss= 20.8275680542\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 21.6662120819\n",
      "range:(5000, 10000) loss= 25.4771499634\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 648\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 26.8756523132\n",
      "range:(5000, 10000) loss= 21.6037559509\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 28.3316001892\n",
      "range:(5000, 10000) loss= 28.1624126434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 63.5028190613\n",
      "range:(5000, 10000) loss= 37.472442627\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 78.0302429199\n",
      "range:(5000, 10000) loss= 36.4416999817\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 67.2860183716\n",
      "range:(5000, 10000) loss= 72.9226531982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 649\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 79.3455200195\n",
      "range:(5000, 10000) loss= 66.4517440796\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 78.3287582397\n",
      "range:(5000, 10000) loss= 86.4882736206\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 87.3455352783\n",
      "range:(5000, 10000) loss= 97.8727645874\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 76.4948120117\n",
      "range:(5000, 10000) loss= 81.8336029053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 80.6416397095\n",
      "range:(5000, 10000) loss= 121.882102966\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 650\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 114.788925171\n",
      "range:(5000, 10000) loss= 108.706672668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 243.637512207\n",
      "range:(5000, 10000) loss= 147.411514282\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 208.103134155\n",
      "range:(5000, 10000) loss= 263.575500488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 306.567657471\n",
      "range:(5000, 10000) loss= 374.91394043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 482.691314697\n",
      "range:(5000, 10000) loss= 589.825256348\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 651\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 727.126037598\n",
      "range:(5000, 10000) loss= 876.21081543\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 883.032531738\n",
      "range:(5000, 10000) loss= 773.587463379\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 751.163208008\n",
      "range:(5000, 10000) loss= 663.527526855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 580.438903809\n",
      "range:(5000, 10000) loss= 763.534851074\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 839.984008789\n",
      "range:(5000, 10000) loss= 958.260559082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 652\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1078.52307129\n",
      "range:(5000, 10000) loss= 960.463928223\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1043.66955566\n",
      "range:(5000, 10000) loss= 936.951599121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 816.982971191\n",
      "range:(5000, 10000) loss= 881.659851074\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 738.312744141\n",
      "range:(5000, 10000) loss= 568.135070801\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 424.809631348\n",
      "range:(5000, 10000) loss= 621.77947998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 653\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 524.876586914\n",
      "range:(5000, 10000) loss= 667.932922363\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 684.928710938\n",
      "range:(5000, 10000) loss= 827.751708984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 772.670837402\n",
      "range:(5000, 10000) loss= 704.969665527\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 688.829162598\n",
      "range:(5000, 10000) loss= 539.934143066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 397.200958252\n",
      "range:(5000, 10000) loss= 190.116699219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 654\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 241.978591919\n",
      "range:(5000, 10000) loss= 404.205230713\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 455.489501953\n",
      "range:(5000, 10000) loss= 464.23147583\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 464.637298584\n",
      "range:(5000, 10000) loss= 396.425811768\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 232.472076416\n",
      "range:(5000, 10000) loss= 237.94392395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 362.161224365\n",
      "range:(5000, 10000) loss= 349.911773682\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 655\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 367.442840576\n",
      "range:(5000, 10000) loss= 369.085021973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 436.158294678\n",
      "range:(5000, 10000) loss= 392.377105713\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 382.470825195\n",
      "range:(5000, 10000) loss= 405.758514404\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 362.668823242\n",
      "range:(5000, 10000) loss= 313.941986084\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 307.742767334\n",
      "range:(5000, 10000) loss= 278.063812256\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 656\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 315.618225098\n",
      "range:(5000, 10000) loss= 280.100067139\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 167.26789856\n",
      "range:(5000, 10000) loss= 298.540008545\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 320.3543396\n",
      "range:(5000, 10000) loss= 210.506057739\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 238.24256897\n",
      "range:(5000, 10000) loss= 313.697143555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 425.569732666\n",
      "range:(5000, 10000) loss= 258.181549072\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 657\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 269.8956604\n",
      "range:(5000, 10000) loss= 242.04876709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 226.094802856\n",
      "range:(5000, 10000) loss= 285.022979736\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 232.190322876\n",
      "range:(5000, 10000) loss= 196.218185425\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 187.347808838\n",
      "range:(5000, 10000) loss= 257.00994873\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 199.009078979\n",
      "range:(5000, 10000) loss= 194.065155029\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 658\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 275.77444458\n",
      "range:(5000, 10000) loss= 219.793441772\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 214.863525391\n",
      "range:(5000, 10000) loss= 256.063232422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 207.216796875\n",
      "range:(5000, 10000) loss= 304.760223389\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 313.909851074\n",
      "range:(5000, 10000) loss= 215.592910767\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 271.775665283\n",
      "range:(5000, 10000) loss= 378.356536865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 659\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 231.16317749\n",
      "range:(5000, 10000) loss= 264.885314941\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 248.181747437\n",
      "range:(5000, 10000) loss= 273.354888916\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 216.360748291\n",
      "range:(5000, 10000) loss= 211.335769653\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 212.35093689\n",
      "range:(5000, 10000) loss= 236.267028809\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 206.443481445\n",
      "range:(5000, 10000) loss= 230.130493164\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 660\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 175.594619751\n",
      "range:(5000, 10000) loss= 191.518859863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 223.420547485\n",
      "range:(5000, 10000) loss= 312.37979126\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 246.885314941\n",
      "range:(5000, 10000) loss= 242.824890137\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 306.271484375\n",
      "range:(5000, 10000) loss= 311.534667969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 195.669418335\n",
      "range:(5000, 10000) loss= 293.706665039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 661\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 312.584075928\n",
      "range:(5000, 10000) loss= 298.972076416\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 312.778656006\n",
      "range:(5000, 10000) loss= 289.507202148\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 263.204620361\n",
      "range:(5000, 10000) loss= 224.862045288\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 187.939529419\n",
      "range:(5000, 10000) loss= 189.681228638\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 179.12600708\n",
      "range:(5000, 10000) loss= 155.422485352\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 662\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 145.470748901\n",
      "range:(5000, 10000) loss= 105.215484619\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 93.0037384033\n",
      "range:(5000, 10000) loss= 107.242790222\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 99.8959884644\n",
      "range:(5000, 10000) loss= 120.804771423\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 107.225563049\n",
      "range:(5000, 10000) loss= 120.938514709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 95.671257019\n",
      "range:(5000, 10000) loss= 91.7194519043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 663\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 92.3029022217\n",
      "range:(5000, 10000) loss= 63.6249923706\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 60.0290641785\n",
      "range:(5000, 10000) loss= 64.3688049316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 67.7008895874\n",
      "range:(5000, 10000) loss= 57.4708175659\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 43.926902771\n",
      "range:(5000, 10000) loss= 65.1779098511\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 58.6713523865\n",
      "range:(5000, 10000) loss= 65.485496521\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 664\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 43.816028595\n",
      "range:(5000, 10000) loss= 41.646522522\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 59.6129341125\n",
      "range:(5000, 10000) loss= 47.0416946411\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 50.0576477051\n",
      "range:(5000, 10000) loss= 47.2327041626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 39.4472503662\n",
      "range:(5000, 10000) loss= 47.2612037659\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 52.3146362305\n",
      "range:(5000, 10000) loss= 35.853767395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 665\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 37.9130477905\n",
      "range:(5000, 10000) loss= 38.8712806702\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 34.2024116516\n",
      "range:(5000, 10000) loss= 45.1218032837\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 54.495880127\n",
      "range:(5000, 10000) loss= 40.3481369019\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 75.0517501831\n",
      "range:(5000, 10000) loss= 45.6658821106\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 48.6434516907\n",
      "range:(5000, 10000) loss= 56.8331184387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 666\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 57.2559280396\n",
      "range:(5000, 10000) loss= 48.9389228821\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 50.5869941711\n",
      "range:(5000, 10000) loss= 60.6989173889\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 39.6224784851\n",
      "range:(5000, 10000) loss= 42.4926261902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 39.7423057556\n",
      "range:(5000, 10000) loss= 37.7028427124\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 32.2186126709\n",
      "range:(5000, 10000) loss= 35.0389251709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 667\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 46.4920501709\n",
      "range:(5000, 10000) loss= 41.5013656616\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 57.5766525269\n",
      "range:(5000, 10000) loss= 51.4267539978\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 50.7000083923\n",
      "range:(5000, 10000) loss= 63.3271408081\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 67.3961334229\n",
      "range:(5000, 10000) loss= 58.8126907349\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 62.4748764038\n",
      "range:(5000, 10000) loss= 62.618938446\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 668\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 62.9863204956\n",
      "range:(5000, 10000) loss= 60.3625526428\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 64.1118927002\n",
      "range:(5000, 10000) loss= 54.3443260193\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 44.3459739685\n",
      "range:(5000, 10000) loss= 42.4674224854\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 38.3785591125\n",
      "range:(5000, 10000) loss= 39.8137321472\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 61.3814735413\n",
      "range:(5000, 10000) loss= 68.8345184326\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 669\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 64.3365249634\n",
      "range:(5000, 10000) loss= 64.4389648438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 74.5189666748\n",
      "range:(5000, 10000) loss= 66.7665939331\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 71.305480957\n",
      "range:(5000, 10000) loss= 62.982673645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 65.4719696045\n",
      "range:(5000, 10000) loss= 59.1902732849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 56.6395683289\n",
      "range:(5000, 10000) loss= 54.9857521057\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 670\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 50.1296005249\n",
      "range:(5000, 10000) loss= 43.7616920471\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 41.7113952637\n",
      "range:(5000, 10000) loss= 42.7084999084\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 34.9970932007\n",
      "range:(5000, 10000) loss= 36.5865287781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 32.3728904724\n",
      "range:(5000, 10000) loss= 31.9830303192\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 41.557384491\n",
      "range:(5000, 10000) loss= 67.0827026367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 671\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 68.9648971558\n",
      "range:(5000, 10000) loss= 63.5175628662\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 53.1324691772\n",
      "range:(5000, 10000) loss= 56.5552749634\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 53.922203064\n",
      "range:(5000, 10000) loss= 59.4834403992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 55.1103973389\n",
      "range:(5000, 10000) loss= 50.7206573486\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 46.6537971497\n",
      "range:(5000, 10000) loss= 41.193397522\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 672\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 41.5587310791\n",
      "range:(5000, 10000) loss= 38.3002586365\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 35.516872406\n",
      "range:(5000, 10000) loss= 33.4850540161\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 33.4399490356\n",
      "range:(5000, 10000) loss= 32.1336936951\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 31.8678398132\n",
      "range:(5000, 10000) loss= 27.9447059631\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 32.1004981995\n",
      "range:(5000, 10000) loss= 44.9390449524\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 673\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 47.4924316406\n",
      "range:(5000, 10000) loss= 55.161113739\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 53.4641799927\n",
      "range:(5000, 10000) loss= 53.8449325562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 50.4140815735\n",
      "range:(5000, 10000) loss= 47.8515739441\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 44.7158699036\n",
      "range:(5000, 10000) loss= 44.3296279907\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 39.9764709473\n",
      "range:(5000, 10000) loss= 40.1690750122\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 674\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 40.0992279053\n",
      "range:(5000, 10000) loss= 39.6436042786\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 37.7359085083\n",
      "range:(5000, 10000) loss= 33.4662666321\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 34.0806121826\n",
      "range:(5000, 10000) loss= 33.552772522\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 33.1876106262\n",
      "range:(5000, 10000) loss= 31.2673091888\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 35.7420425415\n",
      "range:(5000, 10000) loss= 27.9433479309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 675\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 28.7658805847\n",
      "range:(5000, 10000) loss= 26.2709846497\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 26.5336265564\n",
      "range:(5000, 10000) loss= 25.1141700745\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 25.2794647217\n",
      "range:(5000, 10000) loss= 24.5399665833\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 24.7552604675\n",
      "range:(5000, 10000) loss= 25.0378379822\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 25.1172046661\n",
      "range:(5000, 10000) loss= 24.1228733063\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 676\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 22.872177124\n",
      "range:(5000, 10000) loss= 23.0969238281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 24.9580669403\n",
      "range:(5000, 10000) loss= 23.6728858948\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 29.7482242584\n",
      "range:(5000, 10000) loss= 28.0194587708\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 45.4263381958\n",
      "range:(5000, 10000) loss= 33.2507705688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 44.2722930908\n",
      "range:(5000, 10000) loss= 42.4982414246\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 677\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 39.4450378418\n",
      "range:(5000, 10000) loss= 34.8755760193\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 31.0131759644\n",
      "range:(5000, 10000) loss= 32.4524726868\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 32.5312690735\n",
      "range:(5000, 10000) loss= 28.9739170074\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 30.3916454315\n",
      "range:(5000, 10000) loss= 30.1205825806\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 28.8732700348\n",
      "range:(5000, 10000) loss= 27.334438324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 678\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 28.6340255737\n",
      "range:(5000, 10000) loss= 26.9741764069\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 28.9506435394\n",
      "range:(5000, 10000) loss= 26.9486465454\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 25.7800960541\n",
      "range:(5000, 10000) loss= 28.7699069977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 25.2460861206\n",
      "range:(5000, 10000) loss= 25.4141311646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 22.0659885406\n",
      "range:(5000, 10000) loss= 31.0768013\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 679\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 34.6058769226\n",
      "range:(5000, 10000) loss= 36.8548622131\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 65.3751144409\n",
      "range:(5000, 10000) loss= 70.391998291\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 59.9643821716\n",
      "range:(5000, 10000) loss= 67.1340255737\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 57.9770622253\n",
      "range:(5000, 10000) loss= 64.209815979\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 59.1057472229\n",
      "range:(5000, 10000) loss= 59.7268028259\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 680\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 67.5681304932\n",
      "range:(5000, 10000) loss= 67.3552474976\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 67.9835510254\n",
      "range:(5000, 10000) loss= 69.2738494873\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 74.345993042\n",
      "range:(5000, 10000) loss= 72.7803268433\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 72.4967498779\n",
      "range:(5000, 10000) loss= 65.1713485718\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 54.1091880798\n",
      "range:(5000, 10000) loss= 48.4379692078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 681\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 42.6758117676\n",
      "range:(5000, 10000) loss= 52.7193908691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 52.362033844\n",
      "range:(5000, 10000) loss= 104.403747559\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 111.76309967\n",
      "range:(5000, 10000) loss= 70.8491973877\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 117.867668152\n",
      "range:(5000, 10000) loss= 132.410049438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 131.353744507\n",
      "range:(5000, 10000) loss= 133.811004639\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 682\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 131.526763916\n",
      "range:(5000, 10000) loss= 120.430526733\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 113.007720947\n",
      "range:(5000, 10000) loss= 89.6197357178\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 81.1853256226\n",
      "range:(5000, 10000) loss= 74.0308761597\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 60.7588882446\n",
      "range:(5000, 10000) loss= 48.5069198608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 45.20287323\n",
      "range:(5000, 10000) loss= 40.0004501343\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 683\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 89.5684814453\n",
      "range:(5000, 10000) loss= 96.1360549927\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 117.062568665\n",
      "range:(5000, 10000) loss= 119.328285217\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 124.839813232\n",
      "range:(5000, 10000) loss= 106.341209412\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 98.6789703369\n",
      "range:(5000, 10000) loss= 116.549728394\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 70.0158920288\n",
      "range:(5000, 10000) loss= 103.115112305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 684\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 96.0717468262\n",
      "range:(5000, 10000) loss= 113.616203308\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 91.6772613525\n",
      "range:(5000, 10000) loss= 85.6369857788\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 100.276344299\n",
      "range:(5000, 10000) loss= 98.3670501709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 91.8956451416\n",
      "range:(5000, 10000) loss= 89.0103530884\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 93.3254394531\n",
      "range:(5000, 10000) loss= 66.9046783447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 685\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 105.501899719\n",
      "range:(5000, 10000) loss= 86.8836669922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 99.3254623413\n",
      "range:(5000, 10000) loss= 88.5353240967\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 75.9022369385\n",
      "range:(5000, 10000) loss= 60.0193138123\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 55.5530853271\n",
      "range:(5000, 10000) loss= 48.4281463623\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 64.7601470947\n",
      "range:(5000, 10000) loss= 90.9465408325\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 686\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 82.2810821533\n",
      "range:(5000, 10000) loss= 123.018875122\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 146.421524048\n",
      "range:(5000, 10000) loss= 92.1492538452\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 98.7593612671\n",
      "range:(5000, 10000) loss= 99.3840255737\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 89.1842956543\n",
      "range:(5000, 10000) loss= 123.425735474\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 95.6503067017\n",
      "range:(5000, 10000) loss= 100.542953491\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 687\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 126.354644775\n",
      "range:(5000, 10000) loss= 107.934585571\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 102.16078949\n",
      "range:(5000, 10000) loss= 121.906646729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 97.3193664551\n",
      "range:(5000, 10000) loss= 81.9720611572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 75.3601150513\n",
      "range:(5000, 10000) loss= 63.3455696106\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 61.6538848877\n",
      "range:(5000, 10000) loss= 96.2961502075\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 688\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 117.690299988\n",
      "range:(5000, 10000) loss= 99.4821777344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 94.730178833\n",
      "range:(5000, 10000) loss= 97.8918991089\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 99.4954223633\n",
      "range:(5000, 10000) loss= 102.800193787\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 106.036743164\n",
      "range:(5000, 10000) loss= 76.65965271\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 118.142189026\n",
      "range:(5000, 10000) loss= 71.3448257446\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 689\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 120.19203186\n",
      "range:(5000, 10000) loss= 67.6612625122\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 102.968009949\n",
      "range:(5000, 10000) loss= 140.973434448\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 94.7759017944\n",
      "range:(5000, 10000) loss= 75.3052749634\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 90.099571228\n",
      "range:(5000, 10000) loss= 97.3341598511\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 88.7493515015\n",
      "range:(5000, 10000) loss= 84.2182235718\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 690\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 82.3587646484\n",
      "range:(5000, 10000) loss= 80.2525024414\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 71.7047958374\n",
      "range:(5000, 10000) loss= 71.9400787354\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 68.2816543579\n",
      "range:(5000, 10000) loss= 66.8952026367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 55.8675804138\n",
      "range:(5000, 10000) loss= 57.2046165466\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 50.9512062073\n",
      "range:(5000, 10000) loss= 40.5977058411\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 691\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 28.085773468\n",
      "range:(5000, 10000) loss= 56.6245727539\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 62.0262832642\n",
      "range:(5000, 10000) loss= 106.852546692\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 84.9631118774\n",
      "range:(5000, 10000) loss= 89.8193054199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 90.2886199951\n",
      "range:(5000, 10000) loss= 88.3463897705\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 72.561126709\n",
      "range:(5000, 10000) loss= 71.7537307739\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 692\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 66.92943573\n",
      "range:(5000, 10000) loss= 62.8236312866\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 54.2513542175\n",
      "range:(5000, 10000) loss= 44.1722640991\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 38.4740180969\n",
      "range:(5000, 10000) loss= 36.8919944763\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 25.1164150238\n",
      "range:(5000, 10000) loss= 49.0465660095\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 52.7038650513\n",
      "range:(5000, 10000) loss= 99.4400939941\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 693\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 85.5539245605\n",
      "range:(5000, 10000) loss= 98.1402053833\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 125.820289612\n",
      "range:(5000, 10000) loss= 110.171768188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 98.5027389526\n",
      "range:(5000, 10000) loss= 89.8389663696\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 83.6615142822\n",
      "range:(5000, 10000) loss= 79.7791824341\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 72.7128295898\n",
      "range:(5000, 10000) loss= 71.8388900757\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 694\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 62.7401847839\n",
      "range:(5000, 10000) loss= 53.8820343018\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 48.0424461365\n",
      "range:(5000, 10000) loss= 45.3895301819\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 37.8087005615\n",
      "range:(5000, 10000) loss= 38.7204704285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 60.2749862671\n",
      "range:(5000, 10000) loss= 67.2549819946\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 93.1637573242\n",
      "range:(5000, 10000) loss= 72.6289749146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 695\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 63.9877281189\n",
      "range:(5000, 10000) loss= 61.6646690369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 53.7432365417\n",
      "range:(5000, 10000) loss= 39.9758033752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 38.7034111023\n",
      "range:(5000, 10000) loss= 30.5192680359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 29.1106204987\n",
      "range:(5000, 10000) loss= 36.8270378113\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 39.2842140198\n",
      "range:(5000, 10000) loss= 37.4658889771\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 696\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 43.8226737976\n",
      "range:(5000, 10000) loss= 49.7217712402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 44.8958091736\n",
      "range:(5000, 10000) loss= 68.1920547485\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 41.3248596191\n",
      "range:(5000, 10000) loss= 49.909324646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 44.8795089722\n",
      "range:(5000, 10000) loss= 86.29271698\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 82.5710144043\n",
      "range:(5000, 10000) loss= 67.5113754272\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 697\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 75.1035308838\n",
      "range:(5000, 10000) loss= 79.6383285522\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 70.5397415161\n",
      "range:(5000, 10000) loss= 70.1236343384\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 76.8896026611\n",
      "range:(5000, 10000) loss= 78.2555999756\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 91.3276290894\n",
      "range:(5000, 10000) loss= 81.5424194336\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 77.499206543\n",
      "range:(5000, 10000) loss= 83.1231231689\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 698\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 87.5739135742\n",
      "range:(5000, 10000) loss= 78.7833633423\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 70.66822052\n",
      "range:(5000, 10000) loss= 83.464263916\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 54.147403717\n",
      "range:(5000, 10000) loss= 76.2698516846\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 94.87084198\n",
      "range:(5000, 10000) loss= 131.338500977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 104.591018677\n",
      "range:(5000, 10000) loss= 121.607719421\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 699\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 128.239837646\n",
      "range:(5000, 10000) loss= 122.183410645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 101.165390015\n",
      "range:(5000, 10000) loss= 105.237319946\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 94.4290466309\n",
      "range:(5000, 10000) loss= 84.9474487305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 83.7277679443\n",
      "range:(5000, 10000) loss= 73.4753646851\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 69.405670166\n",
      "range:(5000, 10000) loss= 60.7963867188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 700\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 84.6715393066\n",
      "range:(5000, 10000) loss= 50.6261329651\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 95.7701263428\n",
      "range:(5000, 10000) loss= 91.4053192139\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 81.0225830078\n",
      "range:(5000, 10000) loss= 118.031166077\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 101.577690125\n",
      "range:(5000, 10000) loss= 102.977455139\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 105.533241272\n",
      "range:(5000, 10000) loss= 112.209289551\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 6055\n",
    "    for ep in range(600, 700):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(float(len(batch_images)) / min_batch_size)):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_2\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7055"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
