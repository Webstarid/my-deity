{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script, I Experiment with the Cifar-10 dataset. Moreover, I will transfer the AANN modifications to conv-deconv autoencoder architecture.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature_survey\n",
      "Models\n",
      "README.md\n",
      "Res\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '../..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../../Data/cifar-10\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "idea = \"IDEA_1\"\n",
    "base_model_path = '../../Models'\n",
    "idea_model_path = os.path.join(base_model_path, idea)\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 500 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "num_classes = 10 # There are 10 different classes in the dataset\n",
    "k_size = 5 # all kernels are 5x5\n",
    "n_hidden_neurons_in_fc_layers = 512\n",
    "representation_vector_length = 128 # length of the mid_level representation vector\n",
    "batch_size = 5000 # we look at 5000 images at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfMMIIf8fzxxRWuxhJTgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff70a9bf050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niAJfiBTy/wHWMxyFgWnjrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff70aa0b6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXtwHVed57/f+9DV4+otS7ItP4RjJ3Fi4iROgEmGEAbY\nLDsMDNTM8toKgaWgCkiY7NYGZmsmLDM1Q5iqbAFTmdlJQipkgYFhJ5vAsnlAwiskxLHjV+z4/bYl\nS7aeV9KVdO/ZP27LvjH9PbrR40pL/z5VLrfOr7vP6dP9u919vv37HTrnYBhGtIgtdAMMwyg/5viG\nEUHM8Q0jgpjjG0YEMcc3jAhijm8YEWRROj7JHMmtJHeSfIxk3RztdxXJnXOxr4v2ezfJO+d6vzOB\n5HdJbiN5xzzXcwfJyqK/h+Zw38+SvOb11D9H9d5E8i0lrvt3wfV5z1y2oVwsSscHkHHOXeOc2wCg\nD8Bn5nDfv7MfLpBsB7DJObfROfe1i2zxOa7u8wBqiv4ud79+HkD1HO/zbQB+r8R1Pwngjc65u4oL\n56Gf54XF6vjFPA9gOQCQrCH5E5IvkdxO8o+C8lUkd5P8J5K7SD5BMhXYrg3ugC+j6AeEZIrkN0nu\nILmF5NuC8ltJPkryKZKHSH6G5J8FTyC/Jtnga2xwt7qX5GaSr5DcRPJ/kdxL8q+K1ns0WGcnyf9Y\nVP6JYN0XguP5elDeQvIHJH8T/Au7QJ8EsCxo641BW/47yRcB3B7000+D/niaZEew74dI3kfyeZIH\ngjvfg0GffjPkGD8HYBmAZ0j+9EIx/zrY969JLim13SQrgyeVV0j+K4DiJ4n7SL4Y9NPdF9X/7FT9\nYesF5V8JroltJL8q2vQWkqsAfBrA54P+u8Fzjh8DkAawheSfBP33DyRfAHAPycbg/G4P+mJDUb1P\nBW28n+QRkk2qnnnFObfo/gEYCv6PA/g+gHcFf8cApIPlZgD7g+VVAMYBbAj+/h6ADwfL2wHcECx/\nFcCOYPlOAA8Ey5cCOAqgAsCtAPahcDdpAdAP4JPBevcCuD2kvXcDuDNYfhbA3wbLtwM4CaA12Pdx\nAI2BrSH4vxLATgCNAJYCOAygPjj2XwD4erDetwH8XrC8AsDukHasmjq+orb8fdHfjwP4aLB8G4BH\ng+WHAHwnWP4jAAMA1gd/v4TCne3iug5NHUvwdx7Au4PlewD8+eto958VnYsNACYAXHNRP8WC47lS\n1P9b6wFoAvBq0Tp1vjYVn8fg7/cA+JK4RgeLlh8C8HjR318H8BfB8s0AXg6WvwHgrmD53wDIAWha\nCB9LYHFSRXIrgA4AuwE8HZTHAPwtybeicKEtI9ka2A4756be37cAWE2yHkC9c+65oPwRALcEyzei\ncILgnNtL8giAdYHtWefcCIARkv0AfhSU70ThwpyOx4vW3+WcOwMAJA+hcKH1oXBneV+wXgeAtSg4\n/s+ccwPB+v8SlAPAOwBcTpLB32mS1UE7fXyvaPktAP44WH4EBQed4odFbe5yzu0O/n4FwGoAOy7a\nL4N/U2Sdcz8OlrcE7S213W8F8DUAcM7tJLm9yPZBkp8EkADQDmA9gF0h9YettwfAKMkHAPwfXDiP\noW266PjgnPthUb9Mx78ULd8I4P3BPp4l2USyNih/X1D+JMm+Evc95yxWxx9xzl3DwuDNkyg8ov89\ngI+gcBe+2jmXJ3kYFx4Ls0Xb54rKiy8OH6+5iIuWXdHfeZTWZ8XrF+8rDyBB8iYAbwfwJudcluSz\nJbSXwfoTJdRfTKZo2fce7m1zCfUUtytXtM1M2k0AILkawH8CcK1zbpDkQyh6DTi/sljPOZcjeT2A\nPwDwJwA+GyyHtunC78CMmK6fw8pmVeFsWKzv+AQA59wYgDsA/GeSMRQegc8ETn8zCo+2r9mmmODO\n2Vf0XvnRIvMvUfghAcl1KNyJ9871gQjqAfQFTn8ZgDcH5ZsBvJVkPckEgA8UbfMUCn2BoM1XiX37\nLqZfA/hQsPxRFPrg9e5jikEAxWqL2qaUdv8CF87FlQDeGJTXARgGMESyDcC/FfWHrhfcxRucc0+g\n8Go3tV/VpqGLjsmHr49+ieBaY2HsqNc5NwzgOQD/Pih/FwDveNF8slgd//yvo3NuGwrv6R9C4d3s\nuuBR8KMoPMr91jYX8XEA9wWvDsXr3AcgTnIHgO8CuFXclV7vaLVv/SnbEwCSJF8B8DcoDGDCOXcq\n+PtFFC6ewyi8bwOFC3VTMGC0C8CnSqj/4rbcDuA2kttQcLQ7xHq+fUxxP4Anigb31HqltPsfUHjc\nfgXAl1AYV4BzbgeAbSic5/8J4Fdh9XvWqwPwo+B6+QUKYwm+Nv0QwB9PDe6RfA/JL4nj8vXRfwNw\nbVDv36AwbjRV/s7gmvsAgC4UfmzKDoOBBmORQLLGOZdhQRZ6FMCDzrnHFrpdxuwhWQEgF7yCvBnA\nfc457/cK88VifcePMl8i+Q4AKQBPmdP/TrESwPeD19YsCt8CLAh2xzeMCLJY3/ENw5hHzPENI4KY\n4xtGBDHHN4wIYo5vGBHEHN8wIog5vmFEEHN8w4gg5viGEUHM8Q0jgpjjG0YEmZXjk7yF5Ksk95G8\na/otDMNYDMw4SCeIMNqHQkaTUygkkfigc+7Vi9azKCDDWCCcc6EJQ2YTlns9CskujwIAyX8G8F4A\nr1684vhzXwEAfPnBp/GXn3jn+fJkrSfBaCotTY76QcXlctKWGxs8v/zlf/wR/vLTf3j+78n+I3K7\nkXM90tZ98qy0vbjtkLQ99sv955f3nJ3A5c3J838fn9CnpaK+UdrSFdIEuDFpihclhN5/IoO1HRey\nZrfW14RsUWBFS21oeWut3iYW0+eub/i15+6Z7Sfw9qs6AADdp3S+ilhcn/OGdp2Be2BcZwObwGuP\nbfPO47huwwoAQIWnvs42vc+Na/UJqmVG2mIX7fIff9yNT7+7DQAwnpeb4U137NL71JtNy3IUssZO\ncSIoMwxjkVOWRBxffrCQJPfnWw/h51cfxE3XrClHtYYRKbbsH8bWA/rJoZjZOP5JFDKKTNERlP0W\nU4/3i8npb9q0bvqVykRL1eIRV5rqktOvVCY62+Zk5rQ5YVnr4mnLprXhr1LXrk3j2rUXXpEfeEK/\nos7mitsM4BIWZmepAPBBXMgnH8picXpgcTn+kurFM+tSc51voKC8dLYvHmdb3la/0E04z6a1evyr\nVGZ8xw8SBn4WhVTFMRSSQu6ZZjPDMBYBs3rHD/KVXzrtetVLw8uTVXqbnFYB8+N68pjJzKC05cbO\nSVt2QNtOndS2F7Ydk7annz8sbUcHs9JWvTR8tBwA6up0Ovf6nB65X7tymbStX7ta2lIxPWw80NsV\nWp4d0efgbN+ktJ0+qUfuhwf0Ptdcrp8k85P61eXwydPSVtGmH4aXNevzk6vV/dXvhqXNeWzVHiVk\nIq8VBh+L5+XSMIyyYY5vGBHEHN8wIog5vmFEEHN8w4gg5viGEUHK8slurC58NuC8J3BvYqhP2nKZ\nXmljRgfN5DN6n4NntG3v3m5pe36zlvOOd2nJrrJWS5mtaS3ZLa3XctGaujZtW9YubZlu3Z8HToVL\ndgBwblh8HppKyW3yk9qGCc+xrdAf0Fy9tkPafrZlv7S5Ud2UGMelrSqpba1pfV6TOS1JxqiPPe+5\nP0+6md277Y5vGBHEHN8wIog5vmFEEHN8w4gg5viGEUHM8Q0jgpRFzssjPCIr74nimhzREpMb1ZJd\nbkTbBs/qfe7epyW7Hbu0ZFdVqeWbm294g7SlG3Tce3WNlgGr4lpKSk1oefTwfi1rHT5yRtoGR3T0\nV5bhl08upfPOjee1hpbwRJrVJ3TuvGRa5yGcyOlLvKdHRzPW1eq2jNfpSLpaj0f58q3E8j45T9sc\nZ5Y4xe74hhFBzPENI4KY4xtGBDHHN4wIYo5vGBGkLKP6GA7PWecLqIl5bDlPXr2RgX5pO3jklLTt\nO6pTEdc26aymG67Ruewqq/SI68CgPr6RUT1qnB3RI/6Do3okundAj6YPewI9sgmdAXhsMry+mNPt\nSMR1XfGkrisH3Zf7D+sZi5JJPXLfUuupb0hvl8/oIKpcXruUo64vH9P7zEOrJHnMbIY6u+MbRgQx\nxzeMCGKObxgRxBzfMCKIOb5hRBBzfMOIILOS80geATAAIA9gwjl3fdh6ri9cRnNZz3RXnpx7A31a\nCjt+InTC3sJ2/QPStnpls7TlnZZhBgZ0gEuXZ0qoiQkt0TQ1NUlbfaMOSBn0BMeMZnV/ZrK6nZVx\nnSMvlgi31Xna39jcKm1nPbn/hsQ1BADdvXoqrEsu03LrjbeEXq4AgIm4PufxvJZb4zk93drgoJaa\na2r0BKGJhJZHfQE8Pmar4+cBvM05p68qwzAWHbN91Occ7MMwjDIzW6d1AJ4muZnkJ+eiQYZhzD+z\nfdS/wTl3muQSFH4A9jjnfnXxSn/10LPnl9+6cTVuurpzltUahnEx2/eNYPt+z2QBRczK8Z1zp4P/\ne0g+CuB6AL/l+H9x282zqcYwjBK4al01rlp3IVPRIz/WA40zftQnWU0yHSzXAHgXgF0z3Z9hGOVj\nNnf8NgCPknTBfr7tnHsqbMWJgXCZZnJMyyJD/VooONOrbX1DWtKajFVK22BGSy1j2fCcgQAQ80SN\n1dR7ZLn6FmmLx3Q+vmPHtFx5tk/Lo/RExbW367ZU1YZPfwYArUuXh2/jyYHXd05Lh31dOkKyukZH\nSDLuOT8xHb3W0qjzJdY16Bx/ldTnNTNYK239GW1DUh+Di2kbPHkKAZ1nccaO75w7DGDjTLc3DGPh\nMCnOMCKIOb5hRBBzfMOIIOb4hhFBzPENI4KUJdnm6PBIaPnIsJbzzvZlpK27XyecPH5O23oGtNzV\n3FYvbR1rV0lbdkzLRTu26SSQYwd1pGBdWkdqne31fZShI7XS9VrKTNfWSBsSep/Hjx8JLR8ePSi3\nOderj3tkSF8PqzvDpUMAWN6xRNqy4/qc796+R9e3sk3aOtq0nFdT6YlmrFoqbWMeWW7SM72WEwlP\np8Pu+IYRQczxDSOCmOMbRgQxxzeMCGKObxgRxBzfMCJIWeS8cyPhyQGGPXPgHffIPrsP6iiufad0\nIs5Vl2pZ7spNG6StqUEnXhwe1tFmR47p39XDB45JW6y1Xdoa67RclKjU7UxVazlvZETPEzcypqMd\nx0XQ2MCgTgYxNqZl2mRSS6Ojozois6JCRxdesvZSaevt6Za27m4tm9bU6Ki+hmYdgedNixnzzClI\nz/x4zubOMwyjRMzxDSOCmOMbRgQxxzeMCGKObxgRxBzfMCJIWeS8ESHb9QxoiWbPcT0n3c7jWupr\nWLVe2q5621ulbWmnlrvcmJ63LTam5byl7TqKa/JynaSTef17nNCbIVntkfoq9PFhZFya8k5XmKwI\nlw9jnmShg4OUtmw2PIoTAE6e1NfDyKiWD9N1OmnmmnV6fgfGdNRbwpO4NK8PD3nofcbgkeWod0qt\n4HqxO75hRBBzfMOIIOb4hhFBzPENI4KY4xtGBJnW8Uk+SLKb5I6iskaST5HcS/JJkjphnWEYi45S\n5LyHAHwDwLeKyr4A4CfOua+SvAvAF4OyUAYHwyWvAyd0JN3eLi2TVXdoye7f/YdPS9sVV62WNjew\nU9qGTut59U6c0hJU1yktM1WmtNQ3Mam3S2ilDKSW3jLDOgKPMa0JxRP63pBKhsuHFZU+CVBLjtkJ\nHb82Pq6TqI56JNVnf7FZ2noHVkvbNddeJm3pKp2c1JMXE3mP1uc8MmAioc+Pm5in6LxgvvuLBff3\nAng4WH4YwPtmVLthGAvCTN/xW51z3QDgnOsC0Dp3TTIMY76Zq8G9mT1vGIaxIMz0k91ukm3OuW6S\n7QD095QAHnrmyPnljZ0NuLpTz7luGMbM2LzrJF7adbKkdUt1fAb/pngcwMcA3APgVgCP+Ta+7e2r\nS6zGMIyZct2Vy3HdlRdmHPof39cDm6XIed8B8GsA60geI3kbgK8AeCfJvQD+IPjbMIz/T5j2ju+c\n+7AwvaPUSg4IOeyV41rOyySape33b3iXtF113e9LW7JaS1qHTmjbmS4dDdjVo7eriGvJLg4dNcak\n/j2uqNTDKRVxnQQyn9dJLlNVervqdFraKkXSyfEJnaBzSCReBYCeszr5KjNaIkzX62tlOKOTZu7c\ndUTXl9D62ls8iVlTFVpv9Q2E5T1JM335NGMxjw7owb7cM4wIYo5vGBHEHN8wIog5vmFEEHN8w4gg\n5viGEUHKkmxzv4jC6x3RvzsrNmjJZOObNklbIqUTGh7Y/7K0HTq6V9qGe7QkdG5Q15dyOsFl0jOT\nmjfgatI3v5yOFBwf0wk129vbpG3p8qV6n7lw2e50t56TrrpSy13L25ZI29iYmKgPwPi47pNUhb7E\n+3VQH44e0vMzrlmlozVXrNB9GXO++fG0LBeDtuUndb/4sDu+YUQQc3zDiCDm+IYRQczxDSOCmOMb\nRgQxxzeMCFIWOe9YT3h0W/1SPXfZFVddIW3pei2h9XbvkLbMWS3Z1Tgt0YxM6PrykzrKbswTOBXz\nyI6VcS0D5nJ6p8dOH5O2Fct0drTly5dLWyymZcchkcBzcEjLin09w9KW8Bxbc6vuZ89Udph0up9r\nU7XSlqzUEYaOukKPYoekV87zbOjJ4Dmen6dkm4Zh/O5hjm8YEcQc3zAiiDm+YUQQc3zDiCDm+IYR\nQcoi57VtWBdavubKN8ptlnbUSdvEgJatclmdVHLyxBFpS2S0BFWd0b+P1Xk9F1yVJ4llskJHy3mm\nq8Ppbp2gdDirJahUjZ7XdHRcS14jIzqErbc/XKYdGdfyk6+uEwcPSVvNaS1bXbb+UmlLJPT5qavQ\nCTwbW/T8eI312hb35r70SG85HWXncrrPYs4zWZ8Hu+MbRgQxxzeMCGKObxgRxBzfMCKIOb5hRJBS\n5s57kGQ3yR1FZXeTPEFya/DvlvltpmEYc0kpct5DAL4B4FsXld/rnLu3lEpu/sPw+exSlToKrYpa\nlpvo1YkQz+zaJ225YT0HXlWTlg/z43FpS0LbUjGdWHJiTB9fDlqWG+zTiT9rq7R0FXNaLhoe9EQm\neuS83Hh4dF5jnZa76qp1P5/t6ZK2WFLPUdi5ZoW0nevV0YBDA33S1tKoI/dSCc88d5P63CU8EXh5\njyxHj20ko4/Px7R3fOfcrwCE9dDMZuszDGPBmc07/mdJbiP5AEn9dYhhGIuOmTr+fQDe4JzbCKAL\nQEmP/IZhLA5m9Mmuc674Jft+AD/0rf/d7z5/fvnKKzuwYYN+JzMMY2a89Go3tryqJzMpplTHJ4re\n6Um2O+emRmLeD2CXb+MPfegtJVZjGMZM2XRZGzZddmEmn/sf1245reOT/A6AtwFoJnkMwN0Abia5\nEUAewBEAn5pViw3DKCvTOr5z7sMhxQ+9nkpSsfDIt2ROR6i5MS1h9Ow/JW3du/dLW3urTjjpklpa\nHJzUUtJg1pMI0TOvWdzpaMBUXO+z0zOXXW29lsqaG7TE1lyvE1mePKkl0Prm8PqWLOuQ2xw8ekba\n8tQyWV2jPrZ0rY6CHB7UcmSqQvdzfVpH7qU8XhOj3qfP2fKeyL1YTAto6Wp93fqwL/cMI4KY4xtG\nBDHHN4wIYo5vGBHEHN8wIog5vmFEkLIk2zx+LjwSLZXQkok7pSPGUp6Iq+bWRmnLNerotf4xLdmN\neGS5VI2Wwjih95nPeuob1TLnkqYl0paY1NtNDOpowESl3mdlXp+jvv5wCWoUWgI8cFgnSh0b1X2y\nsmOttNEzt1xFXEfLtXiuhxS1hJb02PI5XV9+Ukt2CaddMUYdATrBmbmw3fENI4KY4xtGBDHHN4wI\nYo5vGBHEHN8wIog5vmFEkLLIeRQRbI3VOqFhNqejqvKjo9K27rKV0jZSpWWYl/fpJJaxCZ00M+GZ\nusz3q+o8Es3hozrp5LluLWW+8cpOaWtoTktb3CNPdZ3WsmrfeHi/pJzur+OndGRlY4OWRi+9ZLVn\nOy05NtW2SVuFPgWoSupjqHK6v3Ie27gnym7Sk8GSnsi90f55SrZpGMbvHub4hhFBzPENI4KY4xtG\nBDHHN4wIYo5vGBGkLHLenpdPhJaPLfHMF5bRUU6klgE7PJFmQ0M6amzsrK7PjWq5aMITgVejc0CC\nMc8+9S4xOq4j8BKe3/G6Wh21ODSs5dEDx7S0mKoLT17adeiQ3GZ0SM9Xt/G6S6WtoV5fqrU1+rjj\nTkfgxZ2WyVJxXV+VT3uL6e2GK/V2w54knR7VESMDZz1Wjd3xDSOCmOMbRgQxxzeMCGKObxgRxBzf\nMCLItI5PsoPkMyRfIbmT5O1BeSPJp0juJfkkyfr5b65hGHNBKXLeJIA7nXPbSKYBbCH5FIDbAPzE\nOfdVkncB+CKAL4Tt4NUd4ZJQxXKtd53t17LPWEJLb0MpPUdcOpeTtti4/g2s8iRzdND7HB3UEX+J\nuE7g2blKTyPe2qijzZZ5Eo1OTOj6tm5/VdpOdunovHxPNrR8MKuPe3mHjsBrb22StqTnFsWcluXi\nnijIuN4MbkJvl4/pxtAjESbH9bXSUqWjAcfH9DyLSRd+DqZj2ju+c67LObctWB4GsAdAB4D3Ang4\nWO1hAO+bUQsMwyg7r+sdn+RqABsBvACgzTnXDRR+HADoqWgNw1hUlOz4wWP+DwDcEdz5L36m8Tw4\nGYaxmCjpk12SCRSc/hHn3GNBcTfJNudcN8l2AHLi833HLnxW2Fxf5Z2P3TCMmbHlQB+2HNRjMsWU\n+q3+NwHsds59rajscQAfA3APgFsBPBayHQBg3crmEqsxDGOmXHtJI6695MIA7/1PH5XrTuv4JG8A\n8BEAO0m+jMIj/Z+j4PDfJ/lxAEcB/Onsmm0YRrmY1vGdc89BBwi9o5RKYv3hjx+jFb1ymwodvIaa\nGt3stkotvVV55mYb9MyxFquolLbMiJZT3KRuSyql5ZulTXqcdHmLlvOqKnT0167dWrJ7fuseaevz\n5HJM14UPEa1auVRu09KsT+zwgO7LPjH/IgBUtupozZgnkahH3UXMM2I1mfRF9XmGzXK6LdkBHXU5\n5pm7cTSmryMf9uWeYUQQc3zDiCDm+IYRQczxDSOCmOMbRgQxxzeMCFKeufMqwquZrNXV11Vr23Xr\n9fx4ly/VXwUmxnVSyUOV2vbKoR5p6+7TMhOof1erK3VkYrpCyzeJnE6uuG+P/HgSm7fvk7YsdZ+1\nrNIfX11x+drQ8lVLG+Q2PadPSttZEe0HAJ2dOmmm8yQuHZvU54d5T9JMp6+/fFaf1/4BfR3tOaUT\nl54d1+e8bolOINt9emZfytsd3zAiiDm+YUQQc3zDiCDm+IYRQczxDSOCmOMbRgQpi5yHxnDpql2r\nFLh+jY5Qu2KVjlBLp7WUVFHlsdVrKWn3nh9KW+8xHfNcVeWZ061OH0N9rT72RHxIb1c/KG1XXJ6W\ntmXjWiqrbdbSaceKjtDyVFwnqky0avkpk9FJOvt6taQapz7uypQ+BzWexKx5T2RlT78OWXzhwEFp\n23pWR6PGPBGZlyR1EtU2N7Pk1nbHN4wIYo5vGBHEHN8wIog5vmFEEHN8w4gg5viGEUHKIuctbQhP\nCHhVp9bzrrlkubQ11tVJ26TTEk0ur2Wr/p7j0jbar6Oq6hI6+qu9SSfpXLdC/+Yua9OJP2uqtczU\n4YlgWzmoI9FO9er6RsZ1NGDShe8zl9UJIPM5T+QhdfbLyaxORjnUpyPiXI0+B9VxLXFOjum2nDmu\nJdxaT+7LNSuXSdupjJaT6z2ZP1elPBGGHuyObxgRxBzfMCKIOb5hRBBzfMOIIOb4hhFBpnV8kh0k\nnyH5CsmdJD8XlN9N8gTJrcG/W+a/uYZhzAWlyHmTAO50zm0jmQawheTTge1e59y90+3gPRsvDy2/\n7tLwZI0AkPTMQZbVU+BhdEhHQA0NaRnmxOFd0lZdoSWtq9fr6Kh1a3SiyuXLtcyU9EiEzmmZKTeu\n+6zSo/osb9TJNofHtD51bjg8Si2e0H2SrtKJMU/36X5ubNCRlWtWa+k3Bi2FxfL62CbzWnasqm+S\nNjeqz11OK3ZIUV8POaf7LJOaWbLNUibN7ALQFSwPk9wDYKqnZyYiGoaxoLyud3ySqwFsBPCboOiz\nJLeRfIDkzAKDDcMoOyV/uRc85v8AwB3Bnf8+AF92zjmSfw3gXgCfCNv2289cmIZ5Q2cL3uj5Ys8w\njJmx+3A3dh/RcysUU5Ljk0yg4PSPOOceAwDnXHFKlPsByDQ1H3l7+Du+YRhzx/rONqzvvJDZ6V9/\nrsetSn3U/yaA3c65r00VkGwvsr8fgK7FMIxFxbR3fJI3APgIgJ0kXwbgAPw5gA+T3AggD+AIgE/N\nYzsNw5hDShnVfw5AWPbEJ0qt5PrV60PLk6Namhoe1IkXhwb0e8xkTidCjFFHca1s6pO2dTfpqKra\nOh0NmKrUMkzOI8tlRrXu09uro9ROHNc6Z94zN1tzq5bDzk5qyeunv9kTWl7fqOWum998tbRdula3\noyejk22OQkcXNiT0+eGIFqVS1JGOYzGd/HJvRif+7BrV9aWTOkHpiPPM1edJCurDvtwzjAhijm8Y\nEcQc3zAiiDm+YUQQc3zDiCDm+IYRQcqSbDOO8PneJke0ZBebHJC2hmodAZWo9CRzTGo5Je6RaPJO\nSy1jE3qf/RmPbVhLUEOZ8LkGAWA4o6OxEpVa6mtaokMpWpbpT6hPHh2RtiPnwiXQdo80Nea03NrS\nrKME061Lpa0uoWXTZFbLpkOj+lo544l07PKc81iLliSzp7UkmcrqfknFtWQ30a/lQx92xzeMCGKO\nbxgRxBzfMCKIOb5hRBBzfMOIIOb4hhFByiLnZUYPh5YvqdOyT6JOS1PwRLZN5rRMlnNavslO6Ii4\nrCcCKjuupaThUf27Ojyq95n3RIZVpvV8b7G4Ttg4GdMS1K7jWmZ6YVe3tE2IdtbXt8htUjVaqoRn\nHrjquI4STDl9GbucPgdaMAa6KvQ+R6v1MVQ4HQGajulrpcGTM7M5rv0klZiZC9sd3zAiiDm+YUQQ\nc3zDiCC5X6ABAAAG+klEQVTm+IYRQczxDSOCmOMbRgQpi5y399DB0PLMEi0/tbfoSC1PkB2y41oX\nyWS1rT+rpb6MxzY6ruXDwYyW7PqH9D5HPFLfmNN9NuKxnTvXL22nenRk2MCkPg917eGRaC3NOhKw\nvsojVVbo+9BkUst5XTqAEFnPvHNnYvrYjgxqOTmb0UlNO6q0LZ3U57XFc92uTepjSNTp+Rl92B3f\nMCKIOb5hRBBzfMOIIOb4hhFBzPENI4JM6/gkUyR/Q/JlkjtJ3h2UN5J8iuRekk+S1EO5hmEsKkqZ\nOy9L8mbn3AjJOIDnSP5fAB8A8BPn3FdJ3gXgiwC+ELaPTFX43HNbDobLfADQ1BWeoBMAmpu0hDE4\nomWyfs88cL1OJ5zM5rVMNjGhZZieQR3/dWZAJ0nMjGiJcDyvtczhES3L1VfpiLJYg55frrZSb1fl\nwnW0ykp9bGndlUjEa6Xt1ITuk0MZnXzVlyj1zFmtA57t19Ga62v0QVzWoKMn9/XoOR/rPBGny6jv\nz1o89FPSo75z589wCoUfCwfgvQAeDsofBvC+GbbBMIwyU5Ljk4wFU2R3AXjaObcZQJtzrhsAnHNd\nAFrnr5mGYcwlJX2555zLA7iaZB2AR0legcJd/zWrqe2/++yFR/orVzdiQ6eeRtkwjJnx0uFTeOnI\n6ZLWfV2f7DrnBkn+DMAtALpJtjnnukm2A5AvMB+6ec3rqcYwjBmwqXMZNnVeGE/7p5+9LNctZVS/\nZWrEnmQVgHcC2APgcQAfC1a7FcBjM26xYRhlpZQ7/lIAD5OMofBD8T3n3I9JvgDg+yQ/DuAogD+d\nx3YahjGHlCLn7QRwTUj5OQDvKKWSrZPhEkci2Sm3+fUeLfWdGzgkbblqLQnVrVglbfBEjcUntc15\noqoS9Xq+t/Qq3ZY0tbw2NqyjxuoHe6UtNqiTQA73hM+BBwC5ES1rqcA3Up+DDBqk7YRWI7HzrJZG\nuya1hDs8qOdnRK+OWFxXr8ehNnW0S1tTSn/O8mqlPobBUU+2TY9psE9LhD7syz3DiCDm+IYRQcrq\n+EeO6Pzt5eb0af14W25OHC5NgikHZ87NbNrl+WD73hML3YTz7D+uX6PKzYuHTs16H+V1/KOLp/MW\nk+OfPNK10E04T0+fOX4YB06cXegmnOfFQ7O/UdijvmFEkLLk3Fu67A0AgNrarvPLABCv08EVSejg\nkPphPdKcq9R51GraLoyyHzw4gJUr114wVuqRdO+o/qQeco2n9e9qvPbCkPir6cNYsbR4lF8fe3ZU\nB6tM1NdJWyyj+2yk/sJd/lTfKDovubR4r3K7ykS4wrCkXY9sVzd3SNvFY/MVVXWoDdZvTXjOT04H\nuIzUesJYanQgWGv6tcdQU3cGrR2FD9GqloTnGgSAZFIH6TR36KCgWF2jtFW2vVYdStQcQGXbSgBA\nOqkVFB90zqMVzAEk57cCwzAkzrnQcM55d3zDMBYf9o5vGBHEHN8wIkjZHJ/kLSRfJbkvyNizYJA8\nQnJ7kE7sxTLX/SDJbpI7isoWJI2ZaMvdJE+Q3Br8u6UM7egg+QzJV4L0brcH5WXvl5C2fC4oX4h+\nmb+0d865ef+Hwg/MAQCrACQBbANwWTnqFu05BKBxgeq+EcBGADuKyu4B8F+C5bsAfGUB23I3gDvL\n3CftADYGy2kAewFcthD94mlL2fslaEN18H8cwAsArp+LfinXHf96APudc0edcxMA/hmF1F0LBbFA\nrznOuV8BuPjroQVJYybaAhT6p2w457qcc9uC5WEUwr47sAD9Itoypd+VtV+CNsxL2rtyXfzLARwv\n+vsELnTmQuAAPE1yM8lPLmA7pmh1iyuN2WdJbiP5QLmzJ5NcjcJTyAtY4PRuRW35TVBU9n6Zr7R3\nUR3cu8E5dw2AdwP4DMkbF7pBF7GQGut9AN7gnNuIwsV2b7kqJpkG8AMAdwR325LTu5WhLQvSL865\nvHPuahSegK5/vWnvFOVy/JMAVhb93RGULQjOudPB/z0AHkXhVWQh6SbZBgDTpTGbb5xzPS54eQRw\nP4DrylEvyQQKjvaIc24qm9OC9EtYWxaqX6Zwzg0C+BmK0t4FbZ1Rv5TL8TcDuITkKpIVAD6IQuqu\nskOyOvg1B8kaAO8CsKvczcBr3xcXMo3Za9oSXEhTvB/l65tvAtjtnPtaUdlC9ctvtWUh+mVe096V\ncXTyFhRGSPcD+EK5R0eL2tGJgqrwMoCd5W4LgO8AOAUgC+AYgNsANAL4SdA/TwFoWMC2fAvAjqCP\n/jcK75Pz3Y4bAOSKzsvW4HppKne/eNqyEP2yIah/W1D3fw3KZ90v9smuYUSQqA7uGUakMcc3jAhi\njm8YEcQc3zAiiDm+YUQQc3zDiCDm+IYRQczxDSOC/D+fj9r5fLpYVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff70a983a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAESCAYAAADdZ2gcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neOA7ToB44DtOgHjgO06A/D/4UzfSZsiTSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff708fe2550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / highest_pixel_value\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1)))\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image shape: (32, 32, 3)\n",
      "Random image dataTypefloat32\n",
      "\n",
      "\n",
      "check if the data has been properly normalized\n",
      "[[ 0.68235296  0.627451    0.60000002]\n",
      " [ 0.67450982  0.60392159  0.71372551]\n",
      " [ 0.67058825  0.627451    0.84313726]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD9CAYAAACcAsr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff708f0c450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract one image from the data and display it\n",
    "randomIndex = np.random.randint(batch_data.shape[0])\n",
    "randomImage = batch_data[randomIndex]\n",
    "print \"Random image shape: \" + str(randomImage.shape)\n",
    "\n",
    "print \"Random image dataType\" + str(randomImage.dtype)\n",
    "\n",
    "print \"\\n\\ncheck if the data has been properly normalized\"\n",
    "print randomImage[:3, :3, 0]\n",
    "\n",
    "# Visualize the random image from the dataset\n",
    "plt.figure()\n",
    "plt.imshow(randomImage, interpolation='none'); # suppress the unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the computation graph. This uses a conv-deconv network for the ANN concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the placeholders for the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point to reset the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, size, size, channels), name=\"inputs\")\n",
    "    \n",
    "    # add an image summary for the tf_input\n",
    "    tf_input_summary = tf.summary.image(\"Input_images\", tf_input)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"labels\")\n",
    "    # this is to send in the representation vector tweaked by us to generate images that we want\n",
    "    tf_representation_vector = tf.placeholder(tf.float32, shape=(None, num_classes), name=\"representation\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholders/inputs:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/labels:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Placeholders/representation:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all these tensors to check if they have been correctly defined\n",
    "tf_input, tf_labels, tf_representation_vector\n",
    "# all look good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the kernel and bias variables used for the computation. I am defining them separately instead of using the layers api from the latest tensorflow because I am going to use the same weights while deconvolving the representations (Use of tied weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Weights_and_biases\"):\n",
    "    # special b0 for the input images to be added when performing the backward computations\n",
    "    b0 = tf.get_variable(\"b0\", shape=(1, size, size, channels), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # normal kernel weights and biases\n",
    "    w1 = tf.get_variable(\"W1\", shape=(k_size, k_size, channels, 4), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b1 = tf.get_variable(\"b1\", shape=(1, 16, 16, 4), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape=(k_size, k_size, 4, 8), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b2 = tf.get_variable(\"b2\", shape=(1, 8, 8, 8), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\", shape=(k_size, k_size, 8, 16), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b3 = tf.get_variable(\"b3\", shape=(1, 4, 4, 16), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\", shape=(k_size, k_size, 16, 32), dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    b4 = tf.get_variable(\"b4\", shape=(1, 2, 2, 32), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # two more weights and biases for the final fully connected layers\n",
    "    \n",
    "    w_fc1 = tf.get_variable(\"W_fc1\", shape=(representation_vector_length, n_hidden_neurons_in_fc_layers), \n",
    "                            dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc1 = tf.get_variable(\"b_fc1\", shape=(1, n_hidden_neurons_in_fc_layers), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    \n",
    "    w_fc2 = tf.get_variable(\"W_fc2\", shape=(n_hidden_neurons_in_fc_layers, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b_fc2 = tf.get_variable(\"b_fc2\", shape=(1, num_classes), dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the forward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for the forward_computations (named as encode)\n",
    "def encode(inp):\n",
    "    '''\n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to encode the given input images into the final num_classes-dimensional representation vector\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of input images\n",
    "        @return => tensor of shape [batch_size x num_classes] \n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the convolution layers:\n",
    "    z1 = tf.nn.conv2d(inp, w1, stride_pattern, padding_pattern) + b1\n",
    "    a1 = tf.abs(z1)\n",
    "    \n",
    "    z2 = tf.nn.conv2d(a1, w2, stride_pattern, padding_pattern) + b2\n",
    "    a2 = tf.abs(z2)\n",
    "    \n",
    "    z3 = tf.nn.conv2d(a2, w3, stride_pattern, padding_pattern) + b3\n",
    "    a3 = tf.abs(z3)\n",
    "    \n",
    "    z4 = tf.nn.conv2d(a3, w4, stride_pattern, padding_pattern) + b4\n",
    "    a4 = tf.abs(z4)\n",
    "    \n",
    "    # reshape the a4 activation map:\n",
    "    fc_inp = tf.reshape(a4, shape=(-1, representation_vector_length))\n",
    "    \n",
    "    assert fc_inp.shape[-1] == representation_vector_length, \"mid_level_representation_vector isn't 128 dimensional\"\n",
    "    \n",
    "    # define the fully connected layers:\n",
    "    \n",
    "    z_fc1 = tf.matmul(fc_inp, w_fc1) + b_fc1\n",
    "    a_fc1 = tf.abs(z_fc1)\n",
    "    \n",
    "    z_fc2 = tf.matmul(a_fc1, w_fc2) + b_fc2\n",
    "    a_fc2 = tf.abs(z_fc2)\n",
    "    \n",
    "    assert a_fc2.shape[-1] == num_classes, \"final_representation_vector isn't 10 dimensional\"\n",
    "    \n",
    "    # if everything is fine, return the final activation vectors:\n",
    "    return a_fc2, tf.shape(a1), tf.shape(a2), tf.shape(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    y_, sha1, sha2, sha3 = encode(tf_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder/Abs_5:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check the type of y_ \n",
    "print y_\n",
    "# looks good alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the backward computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode(inp, sha1, sha2, sha3):\n",
    "    ''' \n",
    "        ** Note this function uses globally defined filter and bias weights\n",
    "        ** activation function used is tf.abs! (AANN idea)\n",
    "        Function to decode the given input representation vector into \n",
    "        the size - dimensional images that should be as close as possible\n",
    "        @param\n",
    "        inp => tensor corresponding to batch of representation vectors\n",
    "        @return => tensor of shape [batch_size x size x size x channels]\n",
    "    '''\n",
    "    stride_pattern = [1, 2, 2, 1] # define the stride pattern to halve the image everytime\n",
    "    padding_pattern = \"SAME\" # padding pattern for the conv layers\n",
    "    \n",
    "    # define the backward pass through the fully connected layers:\n",
    "    z_b_1 = tf.matmul(inp, tf.transpose(w_fc2)) + b_fc1\n",
    "    a_b_1 = tf.abs(z_b_1)\n",
    "    \n",
    "    z_b_2 = tf.matmul(a_b_1, tf.transpose(w_fc1)) + tf.reshape(b4, shape=(1, -1))\n",
    "    a_b_2 = tf.abs(z_b_2)\n",
    "    \n",
    "    assert a_b_2.shape[-1] == representation_vector_length, \"reverse_pass: vector not 128 dimensional\"\n",
    "    \n",
    "    # reshape the vector into a feature map:\n",
    "    dconv_in = tf.reshape(a_b_2, shape=(-1, 2, 2, 32)) # reshape into 2x2 maps\n",
    "    \n",
    "    # define the deconvolution operations\n",
    "    z_b_dconv_1 = tf.nn.conv2d_transpose(dconv_in, w4, sha3, \n",
    "                                         stride_pattern, padding_pattern) + b3\n",
    "    a_b_dconv_1 = tf.abs(z_b_dconv_1)\n",
    "\n",
    "    \n",
    "    z_b_dconv_2 = tf.nn.conv2d_transpose(a_b_dconv_1, w3, sha2,\n",
    "                                        stride_pattern, padding_pattern) + b2\n",
    "    a_b_dconv_2 = tf.abs(z_b_dconv_2)    \n",
    "    \n",
    "    \n",
    "    z_b_dconv_3 = tf.nn.conv2d_transpose(a_b_dconv_2, w2, sha1,\n",
    "                                        stride_pattern, padding_pattern) + b1\n",
    "    a_b_dconv_3 = tf.abs(z_b_dconv_3)\n",
    "    \n",
    "    \n",
    "    z_b_dconv_4 = tf.nn.conv2d_transpose(a_b_dconv_3, w1, tf.shape(tf_input),\n",
    "                                        stride_pattern, padding_pattern) + b0\n",
    "    a_b_dconv_4 = tf.abs(z_b_dconv_4)\n",
    "    \n",
    "    # return the final computed image:\n",
    "    return a_b_dconv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Decoder\"):\n",
    "    x_ = decode(y_, sha1, sha2, sha3)\n",
    "    \n",
    "    # add the image summary for the x_ tensor\n",
    "    x__summary = tf.summary.image(\"Network_generated_image\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check if the x_ is a good tensor\n",
    "print x_\n",
    "# looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the decoder predictions:\n",
    "with tf.variable_scope(\"Decoder_predictions\"):\n",
    "    generated_image = decode(tf_representation_vector, sha1, sha2, sha3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Decoder_predictions/Abs_5:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check sanity of the generated_image\n",
    "print generated_image\n",
    "# looks good! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the predictions generated by the network in the forward direction:\n",
    "def direction_cosines(vector):\n",
    "    '''\n",
    "        function to calculate the direction cosines of the given batch of input vectors\n",
    "        @param\n",
    "        vector => activations tensor \n",
    "        @return => the direction cosines of x\n",
    "    '''\n",
    "    sqr = tf.square(vector)\n",
    "    div_val = tf.sqrt(tf.reduce_sum(sqr, axis=1, keep_dims=True))\n",
    "    \n",
    "    # return the direction cosines of the vector:\n",
    "    return vector / div_val\n",
    "\n",
    "# use this function to define the predictions:\n",
    "with tf.variable_scope(\"Predictions\"):\n",
    "    predictions = direction_cosines(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Predictions/div:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to define the costs:\n",
    "### Forward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Forward_cost\"):\n",
    "    fwd_cost = tf.reduce_mean(tf.abs(predictions - tf_labels))\n",
    "    \n",
    "    # add scalar summary for the fwd_cost\n",
    "    fwd_cost_summary = tf.summary.scalar(\"Forward_cost\", fwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Backward_cost\"):\n",
    "    bwd_cost = tf.reduce_mean(tf.abs(x_ - tf_input))\n",
    "    \n",
    "    # add a scalar summary for the bwd_cost\n",
    "    bwd_cost_summary = tf.summary.scalar(\"Backward_cost\", bwd_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the final cost and the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Final_cost\"):\n",
    "    cost = fwd_cost + bwd_cost\n",
    "    \n",
    "    # add a scalar summary\n",
    "    cost_summary = tf.summary.scalar(\"Final_cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_step = optimizer.minimize(cost) # minimize the final cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the init and summary errands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensorboard writer and visualize this graph before starting the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(idea_model_path, \"Model_cifar_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../Models/IDEA_1/Model_cifar_2/model_cifar_2-80\n",
      "epoch: 81\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2253.7097168\n",
      "range:(5000, 10000) loss= 2860.94873047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 3750.93554688\n",
      "range:(5000, 10000) loss= 2922.10864258\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 3284.98364258\n",
      "range:(5000, 10000) loss= 3110.24462891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 3007.140625\n",
      "range:(5000, 10000) loss= 3022.59155273\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2727.46386719\n",
      "range:(5000, 10000) loss= 2667.34838867\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 82\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2285.10913086\n",
      "range:(5000, 10000) loss= 2196.16430664\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2052.37402344\n",
      "range:(5000, 10000) loss= 1713.23303223\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1515.79101562\n",
      "range:(5000, 10000) loss= 1305.35778809\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 855.3828125\n",
      "range:(5000, 10000) loss= 1576.07702637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2436.69091797\n",
      "range:(5000, 10000) loss= 2362.08544922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 83\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2872.68359375\n",
      "range:(5000, 10000) loss= 2493.09521484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2477.06469727\n",
      "range:(5000, 10000) loss= 2375.40307617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2142.97631836\n",
      "range:(5000, 10000) loss= 1888.52001953\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1769.75378418\n",
      "range:(5000, 10000) loss= 1694.61853027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1483.40246582\n",
      "range:(5000, 10000) loss= 2627.70654297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 84\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2646.15136719\n",
      "range:(5000, 10000) loss= 2337.53173828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 3398.36889648\n",
      "range:(5000, 10000) loss= 2928.84423828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2957.19360352\n",
      "range:(5000, 10000) loss= 3008.05517578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 3209.66455078\n",
      "range:(5000, 10000) loss= 2561.86523438\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2221.86083984\n",
      "range:(5000, 10000) loss= 2826.94384766\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 85\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2297.94262695\n",
      "range:(5000, 10000) loss= 3628.07104492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1969.0793457\n",
      "range:(5000, 10000) loss= 1878.79492188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2810.08959961\n",
      "range:(5000, 10000) loss= 3222.58300781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 3131.44213867\n",
      "range:(5000, 10000) loss= 2868.00073242\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 3170.4284668\n",
      "range:(5000, 10000) loss= 2503.68579102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 86\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 4220.17773438\n",
      "range:(5000, 10000) loss= 3376.59301758\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 3549.23535156\n",
      "range:(5000, 10000) loss= 3404.76586914\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2770.58740234\n",
      "range:(5000, 10000) loss= 4209.96777344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 4045.3125\n",
      "range:(5000, 10000) loss= 4111.39160156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 3179.4050293\n",
      "range:(5000, 10000) loss= 2954.65454102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 87\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 3181.7512207\n",
      "range:(5000, 10000) loss= 2627.73388672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2952.73168945\n",
      "range:(5000, 10000) loss= 2248.98974609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2019.17797852\n",
      "range:(5000, 10000) loss= 1314.61706543\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1088.92614746\n",
      "range:(5000, 10000) loss= 2026.88537598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1506.31018066\n",
      "range:(5000, 10000) loss= 2014.19482422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 88\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1794.48181152\n",
      "range:(5000, 10000) loss= 1655.52941895\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1527.85217285\n",
      "range:(5000, 10000) loss= 1485.2520752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2164.28417969\n",
      "range:(5000, 10000) loss= 2686.13378906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 2209.28759766\n",
      "range:(5000, 10000) loss= 2484.82446289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2110.98242188\n",
      "range:(5000, 10000) loss= 1796.54919434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 89\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1746.43994141\n",
      "range:(5000, 10000) loss= 1287.01818848\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1372.95361328\n",
      "range:(5000, 10000) loss= 2034.20910645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1951.74719238\n",
      "range:(5000, 10000) loss= 1806.91662598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1881.68395996\n",
      "range:(5000, 10000) loss= 1877.93395996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1899.29077148\n",
      "range:(5000, 10000) loss= 1639.36474609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 90\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1541.0333252\n",
      "range:(5000, 10000) loss= 1467.94519043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1356.5814209\n",
      "range:(5000, 10000) loss= 1336.97753906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1536.01855469\n",
      "range:(5000, 10000) loss= 1389.40441895\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1419.72741699\n",
      "range:(5000, 10000) loss= 3382.04003906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2586.99291992\n",
      "range:(5000, 10000) loss= 2184.49121094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 91\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2406.00292969\n",
      "range:(5000, 10000) loss= 2346.77490234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2226.26123047\n",
      "range:(5000, 10000) loss= 2040.71606445\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2057.10180664\n",
      "range:(5000, 10000) loss= 2152.85693359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1988.62402344\n",
      "range:(5000, 10000) loss= 1901.5657959\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1908.66564941\n",
      "range:(5000, 10000) loss= 1618.43103027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 92\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1349.73535156\n",
      "range:(5000, 10000) loss= 1605.15783691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2310.08959961\n",
      "range:(5000, 10000) loss= 2791.12988281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2340.72143555\n",
      "range:(5000, 10000) loss= 3073.47875977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 3093.80541992\n",
      "range:(5000, 10000) loss= 3040.53979492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2643.3972168\n",
      "range:(5000, 10000) loss= 2885.84741211\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 93\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2894.63867188\n",
      "range:(5000, 10000) loss= 2767.51318359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2647.078125\n",
      "range:(5000, 10000) loss= 2326.30029297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2095.4375\n",
      "range:(5000, 10000) loss= 1735.9473877\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1396.62426758\n",
      "range:(5000, 10000) loss= 1173.66479492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 826.673156738\n",
      "range:(5000, 10000) loss= 1366.01049805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 94\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1343.02563477\n",
      "range:(5000, 10000) loss= 1458.52026367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1457.56091309\n",
      "range:(5000, 10000) loss= 1221.03210449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1083.39355469\n",
      "range:(5000, 10000) loss= 969.239990234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1564.86694336\n",
      "range:(5000, 10000) loss= 1547.53271484\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1807.06945801\n",
      "range:(5000, 10000) loss= 2053.42529297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 95\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1438.97167969\n",
      "range:(5000, 10000) loss= 1914.22595215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1435.05554199\n",
      "range:(5000, 10000) loss= 1204.13562012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1544.42785645\n",
      "range:(5000, 10000) loss= 1060.30065918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1073.67834473\n",
      "range:(5000, 10000) loss= 1303.95117188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1025.20581055\n",
      "range:(5000, 10000) loss= 1318.26086426\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 96\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1503.82043457\n",
      "range:(5000, 10000) loss= 969.379943848\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1163.55737305\n",
      "range:(5000, 10000) loss= 1243.51062012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1162.77001953\n",
      "range:(5000, 10000) loss= 1045.82348633\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1279.44897461\n",
      "range:(5000, 10000) loss= 944.198791504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1290.56213379\n",
      "range:(5000, 10000) loss= 1281.93676758\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 97\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1226.89941406\n",
      "range:(5000, 10000) loss= 1039.54541016\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1715.37597656\n",
      "range:(5000, 10000) loss= 1634.4934082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1640.33752441\n",
      "range:(5000, 10000) loss= 2525.93408203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1648.03210449\n",
      "range:(5000, 10000) loss= 1909.23754883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2198.61401367\n",
      "range:(5000, 10000) loss= 1748.14916992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 98\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2414.80981445\n",
      "range:(5000, 10000) loss= 2190.97387695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2053.01342773\n",
      "range:(5000, 10000) loss= 2115.60009766\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1908.11706543\n",
      "range:(5000, 10000) loss= 1775.05493164\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1587.69213867\n",
      "range:(5000, 10000) loss= 1335.19042969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1352.82434082\n",
      "range:(5000, 10000) loss= 1123.73596191\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 99\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 961.597473145\n",
      "range:(5000, 10000) loss= 1344.4050293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1676.05419922\n",
      "range:(5000, 10000) loss= 2124.20043945\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2428.18994141\n",
      "range:(5000, 10000) loss= 2466.93652344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 2118.33349609\n",
      "range:(5000, 10000) loss= 1942.70593262\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1772.73999023\n",
      "range:(5000, 10000) loss= 1989.25292969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 100\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1796.49951172\n",
      "range:(5000, 10000) loss= 1697.34436035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1309.40771484\n",
      "range:(5000, 10000) loss= 1263.90124512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1036.29211426\n",
      "range:(5000, 10000) loss= 965.685302734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1195.60009766\n",
      "range:(5000, 10000) loss= 1165.16503906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1447.47497559\n",
      "range:(5000, 10000) loss= 1496.77307129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 101\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1418.18688965\n",
      "range:(5000, 10000) loss= 1473.15405273\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1152.67529297\n",
      "range:(5000, 10000) loss= 1556.41931152\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1366.11181641\n",
      "range:(5000, 10000) loss= 2159.86499023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 2111.46972656\n",
      "range:(5000, 10000) loss= 1374.21008301\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1952.32788086\n",
      "range:(5000, 10000) loss= 1614.09350586\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 102\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2408.76220703\n",
      "range:(5000, 10000) loss= 1768.92724609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2007.22937012\n",
      "range:(5000, 10000) loss= 1807.1932373\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1561.88476562\n",
      "range:(5000, 10000) loss= 2050.70874023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1764.22570801\n",
      "range:(5000, 10000) loss= 1738.55383301\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1610.79406738\n",
      "range:(5000, 10000) loss= 1708.41931152\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 103\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1581.38928223\n",
      "range:(5000, 10000) loss= 1870.23632812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2934.97583008\n",
      "range:(5000, 10000) loss= 2745.36254883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 3250.97949219\n",
      "range:(5000, 10000) loss= 3053.54443359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 2748.19848633\n",
      "range:(5000, 10000) loss= 2881.44628906\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2496.88256836\n",
      "range:(5000, 10000) loss= 2324.75292969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 104\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2296.42089844\n",
      "range:(5000, 10000) loss= 2342.27490234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2090.77563477\n",
      "range:(5000, 10000) loss= 1730.84082031\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1403.52612305\n",
      "range:(5000, 10000) loss= 1123.81286621\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1699.54785156\n",
      "range:(5000, 10000) loss= 2613.8425293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2526.39672852\n",
      "range:(5000, 10000) loss= 3453.07324219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 105\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2857.26879883\n",
      "range:(5000, 10000) loss= 2594.7565918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2759.2019043\n",
      "range:(5000, 10000) loss= 2000.75463867\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2237.11547852\n",
      "range:(5000, 10000) loss= 1795.29394531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1785.06591797\n",
      "range:(5000, 10000) loss= 1612.53442383\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 3077.73828125\n",
      "range:(5000, 10000) loss= 1678.02172852\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 106\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2937.49316406\n",
      "range:(5000, 10000) loss= 2737.55102539\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2011.35192871\n",
      "range:(5000, 10000) loss= 2398.73510742\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2075.90673828\n",
      "range:(5000, 10000) loss= 2513.04858398\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1752.25292969\n",
      "range:(5000, 10000) loss= 1629.31188965\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 3299.92236328\n",
      "range:(5000, 10000) loss= 4035.65234375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 107\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 4465.7421875\n",
      "range:(5000, 10000) loss= 3570.89208984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 4409.58886719\n",
      "range:(5000, 10000) loss= 4182.69189453\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 4033.75805664\n",
      "range:(5000, 10000) loss= 4001.41625977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 3725.61157227\n",
      "range:(5000, 10000) loss= 3761.54931641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 3272.86791992\n",
      "range:(5000, 10000) loss= 2729.0234375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 108\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2725.47753906\n",
      "range:(5000, 10000) loss= 2307.3190918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2074.59570312\n",
      "range:(5000, 10000) loss= 1528.89978027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1206.80725098\n",
      "range:(5000, 10000) loss= 1047.05664062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1240.66931152\n",
      "range:(5000, 10000) loss= 1324.89025879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1863.78747559\n",
      "range:(5000, 10000) loss= 1908.56787109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 109\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1574.30749512\n",
      "range:(5000, 10000) loss= 1661.03564453\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1674.18261719\n",
      "range:(5000, 10000) loss= 1568.05712891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1251.36608887\n",
      "range:(5000, 10000) loss= 1641.18554688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1734.73291016\n",
      "range:(5000, 10000) loss= 2404.70458984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1925.22009277\n",
      "range:(5000, 10000) loss= 2086.4753418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 110\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2379.47509766\n",
      "range:(5000, 10000) loss= 1653.92333984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1378.06884766\n",
      "range:(5000, 10000) loss= 2147.41625977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1481.45043945\n",
      "range:(5000, 10000) loss= 1866.50817871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 2193.42993164\n",
      "range:(5000, 10000) loss= 1852.65942383\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1884.32043457\n",
      "range:(5000, 10000) loss= 2038.29309082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 111\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 2318.66845703\n",
      "range:(5000, 10000) loss= 1792.34118652\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1776.9342041\n",
      "range:(5000, 10000) loss= 2297.51245117\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1920.05334473\n",
      "range:(5000, 10000) loss= 2220.34570312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1833.65283203\n",
      "range:(5000, 10000) loss= 1854.81933594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1707.65368652\n",
      "range:(5000, 10000) loss= 1387.18798828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 112\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1886.92663574\n",
      "range:(5000, 10000) loss= 1773.00048828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1282.04345703\n",
      "range:(5000, 10000) loss= 2207.71362305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1582.35192871\n",
      "range:(5000, 10000) loss= 1466.36450195\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1560.17773438\n",
      "range:(5000, 10000) loss= 1431.73083496\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2145.05151367\n",
      "range:(5000, 10000) loss= 1663.31213379\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 113\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1747.42419434\n",
      "range:(5000, 10000) loss= 1542.03076172\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1276.13793945\n",
      "range:(5000, 10000) loss= 1124.51171875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1115.01513672\n",
      "range:(5000, 10000) loss= 918.579650879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 891.775756836\n",
      "range:(5000, 10000) loss= 699.773986816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1024.94250488\n",
      "range:(5000, 10000) loss= 1261.69396973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 114\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1277.56774902\n",
      "range:(5000, 10000) loss= 1293.70532227\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1095.19299316\n",
      "range:(5000, 10000) loss= 1070.51379395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1002.9710083\n",
      "range:(5000, 10000) loss= 789.147216797\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1621.84997559\n",
      "range:(5000, 10000) loss= 1271.26025391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1445.91101074\n",
      "range:(5000, 10000) loss= 1489.29663086\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 115\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1301.08569336\n",
      "range:(5000, 10000) loss= 1649.62426758\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1503.23913574\n",
      "range:(5000, 10000) loss= 1335.37658691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1517.95715332\n",
      "range:(5000, 10000) loss= 1398.76379395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1273.03295898\n",
      "range:(5000, 10000) loss= 1285.50280762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1163.14562988\n",
      "range:(5000, 10000) loss= 976.186401367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 116\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1229.51660156\n",
      "range:(5000, 10000) loss= 1000.9197998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1336.98840332\n",
      "range:(5000, 10000) loss= 1454.08874512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1593.97351074\n",
      "range:(5000, 10000) loss= 1001.68432617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1498.16870117\n",
      "range:(5000, 10000) loss= 1508.11889648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 2397.34667969\n",
      "range:(5000, 10000) loss= 1685.39123535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 117\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1894.26037598\n",
      "range:(5000, 10000) loss= 1800.66796875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1839.37341309\n",
      "range:(5000, 10000) loss= 1632.32995605\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1567.2097168\n",
      "range:(5000, 10000) loss= 1478.14538574\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1422.23840332\n",
      "range:(5000, 10000) loss= 1351.35498047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1143.20410156\n",
      "range:(5000, 10000) loss= 956.810913086\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 118\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 897.79864502\n",
      "range:(5000, 10000) loss= 723.891296387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 578.239135742\n",
      "range:(5000, 10000) loss= 968.113098145\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1039.96777344\n",
      "range:(5000, 10000) loss= 1278.34106445\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1214.61035156\n",
      "range:(5000, 10000) loss= 1175.45825195\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1085.08251953\n",
      "range:(5000, 10000) loss= 916.456542969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 119\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 980.899291992\n",
      "range:(5000, 10000) loss= 769.394287109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 893.893005371\n",
      "range:(5000, 10000) loss= 781.395202637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 744.057189941\n",
      "range:(5000, 10000) loss= 933.52545166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1010.07244873\n",
      "range:(5000, 10000) loss= 1447.08776855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1020.56054688\n",
      "range:(5000, 10000) loss= 1183.93591309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 120\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1299.17138672\n",
      "range:(5000, 10000) loss= 1521.2277832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1390.07373047\n",
      "range:(5000, 10000) loss= 1459.80871582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1527.75439453\n",
      "range:(5000, 10000) loss= 1546.30236816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1324.81945801\n",
      "range:(5000, 10000) loss= 1491.46704102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1297.84057617\n",
      "range:(5000, 10000) loss= 1307.57434082\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 121\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1228.67871094\n",
      "range:(5000, 10000) loss= 1050.80004883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 923.712280273\n",
      "range:(5000, 10000) loss= 808.515075684\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 817.928405762\n",
      "range:(5000, 10000) loss= 614.657104492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 903.494262695\n",
      "range:(5000, 10000) loss= 1397.95959473\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1616.64318848\n",
      "range:(5000, 10000) loss= 1767.26806641\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 122\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1646.9732666\n",
      "range:(5000, 10000) loss= 1681.08544922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1486.98718262\n",
      "range:(5000, 10000) loss= 1722.49804688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1414.38513184\n",
      "range:(5000, 10000) loss= 1287.59130859\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1195.00378418\n",
      "range:(5000, 10000) loss= 1219.41650391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1021.41137695\n",
      "range:(5000, 10000) loss= 1085.63720703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 123\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 905.440917969\n",
      "range:(5000, 10000) loss= 1137.59057617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 880.934570312\n",
      "range:(5000, 10000) loss= 1262.77404785\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1431.3026123\n",
      "range:(5000, 10000) loss= 1437.1171875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1724.49951172\n",
      "range:(5000, 10000) loss= 1776.85632324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1608.6451416\n",
      "range:(5000, 10000) loss= 1290.79052734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 124\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1649.18225098\n",
      "range:(5000, 10000) loss= 1376.00512695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1315.41101074\n",
      "range:(5000, 10000) loss= 1000.84356689\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1109.14990234\n",
      "range:(5000, 10000) loss= 918.992126465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1010.22796631\n",
      "range:(5000, 10000) loss= 1019.9954834\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 999.013305664\n",
      "range:(5000, 10000) loss= 1857.29638672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 125\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1629.73083496\n",
      "range:(5000, 10000) loss= 1768.0526123\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1925.52416992\n",
      "range:(5000, 10000) loss= 1835.79064941\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1661.57250977\n",
      "range:(5000, 10000) loss= 1417.57910156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1228.44104004\n",
      "range:(5000, 10000) loss= 1237.25463867\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1154.70825195\n",
      "range:(5000, 10000) loss= 1028.0880127\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 126\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 941.060302734\n",
      "range:(5000, 10000) loss= 866.590270996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 663.115112305\n",
      "range:(5000, 10000) loss= 582.300842285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1169.53369141\n",
      "range:(5000, 10000) loss= 1372.60168457\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1283.13684082\n",
      "range:(5000, 10000) loss= 1282.89160156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1406.98620605\n",
      "range:(5000, 10000) loss= 1295.73132324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 127\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1138.75366211\n",
      "range:(5000, 10000) loss= 1046.02746582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 956.905822754\n",
      "range:(5000, 10000) loss= 842.1953125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 772.184875488\n",
      "range:(5000, 10000) loss= 786.263061523\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 761.677368164\n",
      "range:(5000, 10000) loss= 662.873413086\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1994.84436035\n",
      "range:(5000, 10000) loss= 1546.52587891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 128\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1711.97644043\n",
      "range:(5000, 10000) loss= 1575.54455566\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1890.13085938\n",
      "range:(5000, 10000) loss= 1658.78808594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1596.51318359\n",
      "range:(5000, 10000) loss= 1532.54858398\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1620.63928223\n",
      "range:(5000, 10000) loss= 1438.46472168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1360.1920166\n",
      "range:(5000, 10000) loss= 1218.88891602\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 129\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1131.90319824\n",
      "range:(5000, 10000) loss= 1012.76605225\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 838.976257324\n",
      "range:(5000, 10000) loss= 783.232177734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 666.969543457\n",
      "range:(5000, 10000) loss= 562.934082031\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1129.06933594\n",
      "range:(5000, 10000) loss= 1682.58312988\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1480.84667969\n",
      "range:(5000, 10000) loss= 2603.19042969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 130\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1687.05334473\n",
      "range:(5000, 10000) loss= 2213.3894043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 2407.87890625\n",
      "range:(5000, 10000) loss= 2116.17480469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 2080.2097168\n",
      "range:(5000, 10000) loss= 2031.15405273\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1880.94433594\n",
      "range:(5000, 10000) loss= 1742.28259277\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1664.14672852\n",
      "range:(5000, 10000) loss= 1350.5723877\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 131\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1206.02941895\n",
      "range:(5000, 10000) loss= 825.834716797\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1015.57519531\n",
      "range:(5000, 10000) loss= 778.624267578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1837.47412109\n",
      "range:(5000, 10000) loss= 1355.85571289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1642.71252441\n",
      "range:(5000, 10000) loss= 2162.36328125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1887.81408691\n",
      "range:(5000, 10000) loss= 1648.7890625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 132\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1201.12438965\n",
      "range:(5000, 10000) loss= 1249.45410156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1181.17492676\n",
      "range:(5000, 10000) loss= 937.939697266\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 822.803527832\n",
      "range:(5000, 10000) loss= 785.66003418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 772.56036377\n",
      "range:(5000, 10000) loss= 1340.96374512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1002.89154053\n",
      "range:(5000, 10000) loss= 1453.46325684\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 133\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1167.83691406\n",
      "range:(5000, 10000) loss= 1187.31518555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1093.27282715\n",
      "range:(5000, 10000) loss= 910.146362305\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 792.504394531\n",
      "range:(5000, 10000) loss= 818.373779297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 748.399963379\n",
      "range:(5000, 10000) loss= 752.530395508\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 660.018981934\n",
      "range:(5000, 10000) loss= 1204.39904785\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 134\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 731.31060791\n",
      "range:(5000, 10000) loss= 1055.57629395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1043.84912109\n",
      "range:(5000, 10000) loss= 1042.70214844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1105.31738281\n",
      "range:(5000, 10000) loss= 830.888427734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 836.723144531\n",
      "range:(5000, 10000) loss= 908.793579102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 782.569824219\n",
      "range:(5000, 10000) loss= 724.62310791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 135\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 790.808837891\n",
      "range:(5000, 10000) loss= 681.876342773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 654.613952637\n",
      "range:(5000, 10000) loss= 574.473449707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 625.775756836\n",
      "range:(5000, 10000) loss= 609.007446289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 783.547424316\n",
      "range:(5000, 10000) loss= 771.495849609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 830.538085938\n",
      "range:(5000, 10000) loss= 856.944396973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 136\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 863.27935791\n",
      "range:(5000, 10000) loss= 828.734924316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 870.860046387\n",
      "range:(5000, 10000) loss= 802.723693848\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 756.68560791\n",
      "range:(5000, 10000) loss= 812.829467773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 817.194641113\n",
      "range:(5000, 10000) loss= 705.110534668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 738.181274414\n",
      "range:(5000, 10000) loss= 716.575927734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 137\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 655.814025879\n",
      "range:(5000, 10000) loss= 646.329345703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 679.980895996\n",
      "range:(5000, 10000) loss= 646.321594238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 589.757141113\n",
      "range:(5000, 10000) loss= 775.616271973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 671.324890137\n",
      "range:(5000, 10000) loss= 784.448791504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 797.351074219\n",
      "range:(5000, 10000) loss= 1004.67327881\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 138\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 910.927368164\n",
      "range:(5000, 10000) loss= 809.933776855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1151.92492676\n",
      "range:(5000, 10000) loss= 860.384216309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 927.238769531\n",
      "range:(5000, 10000) loss= 958.132446289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 856.247436523\n",
      "range:(5000, 10000) loss= 715.979187012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 646.709411621\n",
      "range:(5000, 10000) loss= 579.071105957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 139\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 540.711975098\n",
      "range:(5000, 10000) loss= 506.314178467\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 477.919189453\n",
      "range:(5000, 10000) loss= 444.504882812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 656.385009766\n",
      "range:(5000, 10000) loss= 986.550292969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1176.44165039\n",
      "range:(5000, 10000) loss= 1112.65637207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1127.24230957\n",
      "range:(5000, 10000) loss= 1251.09106445\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 140\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1038.60168457\n",
      "range:(5000, 10000) loss= 1017.38934326\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1246.02429199\n",
      "range:(5000, 10000) loss= 1186.32421875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1142.92333984\n",
      "range:(5000, 10000) loss= 1158.66906738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1062.41955566\n",
      "range:(5000, 10000) loss= 964.057983398\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 852.455932617\n",
      "range:(5000, 10000) loss= 791.37878418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 141\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 685.422729492\n",
      "range:(5000, 10000) loss= 530.57019043\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 583.775024414\n",
      "range:(5000, 10000) loss= 931.229125977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1392.42675781\n",
      "range:(5000, 10000) loss= 1196.63647461\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1271.40612793\n",
      "range:(5000, 10000) loss= 1363.28857422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1595.79101562\n",
      "range:(5000, 10000) loss= 1428.83215332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 142\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1490.92980957\n",
      "range:(5000, 10000) loss= 1311.04846191\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1304.42248535\n",
      "range:(5000, 10000) loss= 1121.97937012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1142.00390625\n",
      "range:(5000, 10000) loss= 964.80267334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 819.380126953\n",
      "range:(5000, 10000) loss= 729.746948242\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 619.691955566\n",
      "range:(5000, 10000) loss= 554.42175293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 143\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 487.97833252\n",
      "range:(5000, 10000) loss= 842.636230469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 906.601379395\n",
      "range:(5000, 10000) loss= 864.317749023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 927.452575684\n",
      "range:(5000, 10000) loss= 906.893859863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 869.202575684\n",
      "range:(5000, 10000) loss= 1017.32446289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 728.744995117\n",
      "range:(5000, 10000) loss= 836.043945312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 144\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 699.943969727\n",
      "range:(5000, 10000) loss= 865.535095215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 844.041320801\n",
      "range:(5000, 10000) loss= 942.607910156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1058.01953125\n",
      "range:(5000, 10000) loss= 1436.47631836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 951.894348145\n",
      "range:(5000, 10000) loss= 1708.85498047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1475.58544922\n",
      "range:(5000, 10000) loss= 1384.9342041\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 145\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1460.515625\n",
      "range:(5000, 10000) loss= 1384.93444824\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1245.32714844\n",
      "range:(5000, 10000) loss= 1258.06152344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1145.87084961\n",
      "range:(5000, 10000) loss= 1110.32104492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1005.65606689\n",
      "range:(5000, 10000) loss= 819.639282227\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 663.998291016\n",
      "range:(5000, 10000) loss= 591.353393555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 146\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 602.813720703\n",
      "range:(5000, 10000) loss= 1273.17749023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1227.9753418\n",
      "range:(5000, 10000) loss= 1492.20227051\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1565.67175293\n",
      "range:(5000, 10000) loss= 1862.88171387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1444.98425293\n",
      "range:(5000, 10000) loss= 1569.22631836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1561.96057129\n",
      "range:(5000, 10000) loss= 1379.05163574\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 147\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1466.25964355\n",
      "range:(5000, 10000) loss= 1369.58813477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1364.31335449\n",
      "range:(5000, 10000) loss= 1216.1361084\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1064.37451172\n",
      "range:(5000, 10000) loss= 845.538085938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 811.681091309\n",
      "range:(5000, 10000) loss= 714.184814453\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1174.87780762\n",
      "range:(5000, 10000) loss= 1508.35412598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 148\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1584.7557373\n",
      "range:(5000, 10000) loss= 1552.55957031\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1304.56860352\n",
      "range:(5000, 10000) loss= 1084.64355469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 869.886413574\n",
      "range:(5000, 10000) loss= 731.022705078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 619.802246094\n",
      "range:(5000, 10000) loss= 641.284912109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 860.188415527\n",
      "range:(5000, 10000) loss= 1131.74768066\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 149\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1116.27587891\n",
      "range:(5000, 10000) loss= 1531.23339844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1611.38269043\n",
      "range:(5000, 10000) loss= 1550.0970459\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1395.53381348\n",
      "range:(5000, 10000) loss= 1369.76867676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1217.25524902\n",
      "range:(5000, 10000) loss= 1250.19018555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1166.64685059\n",
      "range:(5000, 10000) loss= 1025.24365234\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 150\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 874.094787598\n",
      "range:(5000, 10000) loss= 755.463928223\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 611.863891602\n",
      "range:(5000, 10000) loss= 588.328979492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 827.45501709\n",
      "range:(5000, 10000) loss= 1338.56140137\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1283.20776367\n",
      "range:(5000, 10000) loss= 1240.89428711\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1201.34387207\n",
      "range:(5000, 10000) loss= 1320.43212891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 151\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 996.643005371\n",
      "range:(5000, 10000) loss= 913.323669434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 878.808776855\n",
      "range:(5000, 10000) loss= 645.919799805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 898.127624512\n",
      "range:(5000, 10000) loss= 900.76953125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 833.648254395\n",
      "range:(5000, 10000) loss= 1532.22192383\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 901.892578125\n",
      "range:(5000, 10000) loss= 1392.5411377\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 152\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1098.31933594\n",
      "range:(5000, 10000) loss= 1260.77685547\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1151.87805176\n",
      "range:(5000, 10000) loss= 1057.78845215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 968.680847168\n",
      "range:(5000, 10000) loss= 874.291748047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 751.218078613\n",
      "range:(5000, 10000) loss= 678.968933105\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 803.038024902\n",
      "range:(5000, 10000) loss= 681.312316895\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 153\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 867.933044434\n",
      "range:(5000, 10000) loss= 1924.2980957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1591.33435059\n",
      "range:(5000, 10000) loss= 1583.44213867\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1865.88916016\n",
      "range:(5000, 10000) loss= 1998.90563965\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1782.7578125\n",
      "range:(5000, 10000) loss= 1771.0690918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1735.88354492\n",
      "range:(5000, 10000) loss= 1671.84741211\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 154\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1823.9387207\n",
      "range:(5000, 10000) loss= 1634.33337402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1402.41418457\n",
      "range:(5000, 10000) loss= 1360.8626709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1148.67749023\n",
      "range:(5000, 10000) loss= 927.93572998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 832.140258789\n",
      "range:(5000, 10000) loss= 656.206359863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 665.760559082\n",
      "range:(5000, 10000) loss= 1113.45446777\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 155\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1200.0880127\n",
      "range:(5000, 10000) loss= 1256.76074219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1209.16711426\n",
      "range:(5000, 10000) loss= 1105.38867188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 930.907470703\n",
      "range:(5000, 10000) loss= 810.860229492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 694.702575684\n",
      "range:(5000, 10000) loss= 528.416442871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 482.825622559\n",
      "range:(5000, 10000) loss= 410.629211426\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 156\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 613.846618652\n",
      "range:(5000, 10000) loss= 740.548950195\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 966.245117188\n",
      "range:(5000, 10000) loss= 1036.6628418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 928.424743652\n",
      "range:(5000, 10000) loss= 964.339233398\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 922.019287109\n",
      "range:(5000, 10000) loss= 767.29107666\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 672.756713867\n",
      "range:(5000, 10000) loss= 605.636779785\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 157\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 510.07019043\n",
      "range:(5000, 10000) loss= 473.707092285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 475.762054443\n",
      "range:(5000, 10000) loss= 526.696533203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1045.19726562\n",
      "range:(5000, 10000) loss= 1116.92004395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 854.264831543\n",
      "range:(5000, 10000) loss= 937.700622559\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1470.7401123\n",
      "range:(5000, 10000) loss= 1145.40490723\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 158\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 981.678833008\n",
      "range:(5000, 10000) loss= 1151.1829834\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1012.97528076\n",
      "range:(5000, 10000) loss= 995.450256348\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 926.265869141\n",
      "range:(5000, 10000) loss= 764.373962402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 761.24822998\n",
      "range:(5000, 10000) loss= 644.390136719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 523.034851074\n",
      "range:(5000, 10000) loss= 493.780731201\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 159\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 452.672119141\n",
      "range:(5000, 10000) loss= 389.597229004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 633.526611328\n",
      "range:(5000, 10000) loss= 627.975280762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 681.640075684\n",
      "range:(5000, 10000) loss= 892.782775879\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 682.634460449\n",
      "range:(5000, 10000) loss= 720.441040039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 700.808044434\n",
      "range:(5000, 10000) loss= 545.445373535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 160\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 524.814880371\n",
      "range:(5000, 10000) loss= 493.241851807\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 458.744842529\n",
      "range:(5000, 10000) loss= 408.790893555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 349.122528076\n",
      "range:(5000, 10000) loss= 642.00592041\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 582.492919922\n",
      "range:(5000, 10000) loss= 624.217224121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 803.471923828\n",
      "range:(5000, 10000) loss= 763.222717285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 161\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 705.615356445\n",
      "range:(5000, 10000) loss= 634.885864258\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 631.499755859\n",
      "range:(5000, 10000) loss= 640.215148926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 670.581115723\n",
      "range:(5000, 10000) loss= 622.249511719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 635.088562012\n",
      "range:(5000, 10000) loss= 610.378112793\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 572.571166992\n",
      "range:(5000, 10000) loss= 498.334472656\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 162\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 463.821136475\n",
      "range:(5000, 10000) loss= 444.52911377\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 413.287200928\n",
      "range:(5000, 10000) loss= 376.841278076\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 331.440185547\n",
      "range:(5000, 10000) loss= 295.964202881\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 518.211486816\n",
      "range:(5000, 10000) loss= 520.694946289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 578.87713623\n",
      "range:(5000, 10000) loss= 471.131835938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 163\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 525.006958008\n",
      "range:(5000, 10000) loss= 580.617736816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 474.298980713\n",
      "range:(5000, 10000) loss= 482.67199707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 462.484741211\n",
      "range:(5000, 10000) loss= 478.737121582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 431.029876709\n",
      "range:(5000, 10000) loss= 410.178009033\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 385.295623779\n",
      "range:(5000, 10000) loss= 385.095489502\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 164\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 393.400970459\n",
      "range:(5000, 10000) loss= 350.410247803\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 347.235137939\n",
      "range:(5000, 10000) loss= 346.228851318\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 321.371551514\n",
      "range:(5000, 10000) loss= 332.150421143\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 304.592041016\n",
      "range:(5000, 10000) loss= 607.18347168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 518.715820312\n",
      "range:(5000, 10000) loss= 489.823425293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 165\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 562.751342773\n",
      "range:(5000, 10000) loss= 615.320617676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 568.626525879\n",
      "range:(5000, 10000) loss= 717.60925293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 785.532714844\n",
      "range:(5000, 10000) loss= 922.36920166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 935.504333496\n",
      "range:(5000, 10000) loss= 1010.57403564\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1060.82592773\n",
      "range:(5000, 10000) loss= 1078.17126465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 166\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1114.78881836\n",
      "range:(5000, 10000) loss= 1115.05871582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1002.80285645\n",
      "range:(5000, 10000) loss= 955.887451172\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 864.862976074\n",
      "range:(5000, 10000) loss= 763.327819824\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 561.928283691\n",
      "range:(5000, 10000) loss= 606.512329102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 447.025726318\n",
      "range:(5000, 10000) loss= 1488.44873047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 167\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1266.31481934\n",
      "range:(5000, 10000) loss= 1326.31359863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1553.0793457\n",
      "range:(5000, 10000) loss= 1429.12548828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1429.27075195\n",
      "range:(5000, 10000) loss= 1488.38098145\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1391.71923828\n",
      "range:(5000, 10000) loss= 1569.01257324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1433.53527832\n",
      "range:(5000, 10000) loss= 1373.75390625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 168\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1247.24963379\n",
      "range:(5000, 10000) loss= 1208.97998047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1120.94616699\n",
      "range:(5000, 10000) loss= 943.476867676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 751.504760742\n",
      "range:(5000, 10000) loss= 597.658203125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 800.341186523\n",
      "range:(5000, 10000) loss= 401.23526001\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1046.51538086\n",
      "range:(5000, 10000) loss= 1015.37731934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 169\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1161.33215332\n",
      "range:(5000, 10000) loss= 1133.84375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1261.49047852\n",
      "range:(5000, 10000) loss= 970.755859375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 875.334106445\n",
      "range:(5000, 10000) loss= 920.013183594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 813.976318359\n",
      "range:(5000, 10000) loss= 650.07232666\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 560.158569336\n",
      "range:(5000, 10000) loss= 462.791107178\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 170\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 446.901428223\n",
      "range:(5000, 10000) loss= 327.180297852\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 756.426574707\n",
      "range:(5000, 10000) loss= 703.692687988\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 613.224304199\n",
      "range:(5000, 10000) loss= 731.305236816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 803.794067383\n",
      "range:(5000, 10000) loss= 640.253845215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 706.184570312\n",
      "range:(5000, 10000) loss= 588.322509766\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 171\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 483.270996094\n",
      "range:(5000, 10000) loss= 523.936401367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 664.144897461\n",
      "range:(5000, 10000) loss= 955.531311035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 732.525146484\n",
      "range:(5000, 10000) loss= 724.338256836\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 797.190551758\n",
      "range:(5000, 10000) loss= 780.443603516\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 663.936950684\n",
      "range:(5000, 10000) loss= 633.715087891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 172\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 638.604736328\n",
      "range:(5000, 10000) loss= 624.296813965\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 558.738708496\n",
      "range:(5000, 10000) loss= 501.243408203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 457.903411865\n",
      "range:(5000, 10000) loss= 397.222320557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 356.263702393\n",
      "range:(5000, 10000) loss= 359.462341309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1011.96716309\n",
      "range:(5000, 10000) loss= 741.742858887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 173\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 765.557495117\n",
      "range:(5000, 10000) loss= 1040.82226562\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 937.299438477\n",
      "range:(5000, 10000) loss= 929.070983887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1061.26171875\n",
      "range:(5000, 10000) loss= 1158.13024902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1436.88122559\n",
      "range:(5000, 10000) loss= 1329.64160156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1516.8404541\n",
      "range:(5000, 10000) loss= 1358.52734375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 174\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1316.59069824\n",
      "range:(5000, 10000) loss= 1181.0090332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1044.16882324\n",
      "range:(5000, 10000) loss= 857.214172363\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 789.81451416\n",
      "range:(5000, 10000) loss= 655.337768555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 579.926025391\n",
      "range:(5000, 10000) loss= 796.472900391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1490.8684082\n",
      "range:(5000, 10000) loss= 1228.61657715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 175\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1287.06677246\n",
      "range:(5000, 10000) loss= 1759.35205078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1556.5970459\n",
      "range:(5000, 10000) loss= 1555.87854004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1380.68933105\n",
      "range:(5000, 10000) loss= 1073.41040039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1034.83239746\n",
      "range:(5000, 10000) loss= 896.688720703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 772.05291748\n",
      "range:(5000, 10000) loss= 705.21685791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 176\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 636.700195312\n",
      "range:(5000, 10000) loss= 621.622314453\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 363.59475708\n",
      "range:(5000, 10000) loss= 580.495788574\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 832.399963379\n",
      "range:(5000, 10000) loss= 1049.88085938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1013.9887085\n",
      "range:(5000, 10000) loss= 1013.79827881\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 955.08770752\n",
      "range:(5000, 10000) loss= 816.516113281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 177\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 682.968383789\n",
      "range:(5000, 10000) loss= 604.484008789\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 441.673553467\n",
      "range:(5000, 10000) loss= 830.685546875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 652.039916992\n",
      "range:(5000, 10000) loss= 873.07635498\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 810.408447266\n",
      "range:(5000, 10000) loss= 1119.32080078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 795.529174805\n",
      "range:(5000, 10000) loss= 811.368041992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 178\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 834.539428711\n",
      "range:(5000, 10000) loss= 758.910400391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 737.271362305\n",
      "range:(5000, 10000) loss= 707.079589844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 627.312683105\n",
      "range:(5000, 10000) loss= 561.204406738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 470.25881958\n",
      "range:(5000, 10000) loss= 391.269989014\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 335.967895508\n",
      "range:(5000, 10000) loss= 266.72088623\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 179\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 410.189697266\n",
      "range:(5000, 10000) loss= 544.744018555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 569.286437988\n",
      "range:(5000, 10000) loss= 871.284179688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 796.999328613\n",
      "range:(5000, 10000) loss= 896.973815918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 688.595397949\n",
      "range:(5000, 10000) loss= 705.529724121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 601.541137695\n",
      "range:(5000, 10000) loss= 567.659057617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 180\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 522.238586426\n",
      "range:(5000, 10000) loss= 477.760772705\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 422.998718262\n",
      "range:(5000, 10000) loss= 392.92880249\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 346.583587646\n",
      "range:(5000, 10000) loss= 803.162109375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 851.79119873\n",
      "range:(5000, 10000) loss= 889.276550293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 883.783447266\n",
      "range:(5000, 10000) loss= 956.845153809\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 181\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 914.871032715\n",
      "range:(5000, 10000) loss= 918.594604492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 924.675292969\n",
      "range:(5000, 10000) loss= 950.948852539\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 963.840087891\n",
      "range:(5000, 10000) loss= 1026.4876709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 979.673339844\n",
      "range:(5000, 10000) loss= 965.128356934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 876.497375488\n",
      "range:(5000, 10000) loss= 744.957763672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 182\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 626.864990234\n",
      "range:(5000, 10000) loss= 500.649414062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 430.180480957\n",
      "range:(5000, 10000) loss= 348.172424316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 650.541503906\n",
      "range:(5000, 10000) loss= 970.480041504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 772.985778809\n",
      "range:(5000, 10000) loss= 1089.50341797\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1196.12597656\n",
      "range:(5000, 10000) loss= 913.508361816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 183\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 862.571899414\n",
      "range:(5000, 10000) loss= 1025.62634277\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 861.695068359\n",
      "range:(5000, 10000) loss= 713.104919434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 779.385253906\n",
      "range:(5000, 10000) loss= 941.155761719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 639.449279785\n",
      "range:(5000, 10000) loss= 832.284790039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 676.64453125\n",
      "range:(5000, 10000) loss= 938.573913574\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 184\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 665.774902344\n",
      "range:(5000, 10000) loss= 855.25402832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 679.873168945\n",
      "range:(5000, 10000) loss= 841.403442383\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 653.890014648\n",
      "range:(5000, 10000) loss= 573.192382812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 609.269775391\n",
      "range:(5000, 10000) loss= 595.556640625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 594.977111816\n",
      "range:(5000, 10000) loss= 607.889221191\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 185\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 742.00970459\n",
      "range:(5000, 10000) loss= 809.672546387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 655.243896484\n",
      "range:(5000, 10000) loss= 671.104370117\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 703.355834961\n",
      "range:(5000, 10000) loss= 643.167907715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 580.035522461\n",
      "range:(5000, 10000) loss= 517.511779785\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 501.018829346\n",
      "range:(5000, 10000) loss= 395.780670166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 186\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 363.957885742\n",
      "range:(5000, 10000) loss= 365.044525146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 336.885314941\n",
      "range:(5000, 10000) loss= 348.143554688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 494.721008301\n",
      "range:(5000, 10000) loss= 697.995239258\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 734.498535156\n",
      "range:(5000, 10000) loss= 767.023254395\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 945.188964844\n",
      "range:(5000, 10000) loss= 891.807678223\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 187\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 833.558044434\n",
      "range:(5000, 10000) loss= 857.586425781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 906.787353516\n",
      "range:(5000, 10000) loss= 839.96472168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 832.766845703\n",
      "range:(5000, 10000) loss= 751.570373535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 767.119384766\n",
      "range:(5000, 10000) loss= 591.043457031\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 514.425109863\n",
      "range:(5000, 10000) loss= 501.367767334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 188\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 416.565643311\n",
      "range:(5000, 10000) loss= 383.72958374\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 527.630737305\n",
      "range:(5000, 10000) loss= 776.424438477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 815.106384277\n",
      "range:(5000, 10000) loss= 961.256164551\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 907.237304688\n",
      "range:(5000, 10000) loss= 875.310058594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 786.07421875\n",
      "range:(5000, 10000) loss= 690.110595703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 189\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 581.729125977\n",
      "range:(5000, 10000) loss= 554.820983887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 493.88494873\n",
      "range:(5000, 10000) loss= 420.209350586\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 306.141723633\n",
      "range:(5000, 10000) loss= 293.779266357\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 528.719543457\n",
      "range:(5000, 10000) loss= 811.622680664\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 784.642272949\n",
      "range:(5000, 10000) loss= 869.277770996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 190\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1007.66357422\n",
      "range:(5000, 10000) loss= 1032.79736328\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 902.039916992\n",
      "range:(5000, 10000) loss= 799.28338623\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 753.898742676\n",
      "range:(5000, 10000) loss= 787.763549805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 672.634643555\n",
      "range:(5000, 10000) loss= 586.591491699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 566.333679199\n",
      "range:(5000, 10000) loss= 505.756652832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 191\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 543.645629883\n",
      "range:(5000, 10000) loss= 413.046051025\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 513.608154297\n",
      "range:(5000, 10000) loss= 926.356079102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1312.73779297\n",
      "range:(5000, 10000) loss= 1238.18981934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1057.51977539\n",
      "range:(5000, 10000) loss= 1135.18212891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1077.86645508\n",
      "range:(5000, 10000) loss= 931.325561523\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 192\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 868.122924805\n",
      "range:(5000, 10000) loss= 770.704956055\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 815.152954102\n",
      "range:(5000, 10000) loss= 745.228942871\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 549.553955078\n",
      "range:(5000, 10000) loss= 511.731689453\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 452.379150391\n",
      "range:(5000, 10000) loss= 328.518096924\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 296.353057861\n",
      "range:(5000, 10000) loss= 811.304260254\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 193\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 842.820007324\n",
      "range:(5000, 10000) loss= 935.029174805\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 925.127685547\n",
      "range:(5000, 10000) loss= 902.489685059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 832.324890137\n",
      "range:(5000, 10000) loss= 728.655456543\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 665.094970703\n",
      "range:(5000, 10000) loss= 641.088562012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 517.708557129\n",
      "range:(5000, 10000) loss= 450.988220215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 194\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 410.793334961\n",
      "range:(5000, 10000) loss= 416.02835083\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 329.507659912\n",
      "range:(5000, 10000) loss= 663.983276367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 539.290710449\n",
      "range:(5000, 10000) loss= 929.466247559\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 747.195251465\n",
      "range:(5000, 10000) loss= 880.573791504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 762.899108887\n",
      "range:(5000, 10000) loss= 876.08996582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 195\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 703.570739746\n",
      "range:(5000, 10000) loss= 675.738586426\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 645.579467773\n",
      "range:(5000, 10000) loss= 565.10925293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 479.729492188\n",
      "range:(5000, 10000) loss= 458.531860352\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 456.542419434\n",
      "range:(5000, 10000) loss= 374.825531006\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 350.247894287\n",
      "range:(5000, 10000) loss= 324.327484131\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 196\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 246.970230103\n",
      "range:(5000, 10000) loss= 318.831329346\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 354.837341309\n",
      "range:(5000, 10000) loss= 393.403991699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 502.014831543\n",
      "range:(5000, 10000) loss= 538.663635254\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 503.124450684\n",
      "range:(5000, 10000) loss= 477.464447021\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 436.901245117\n",
      "range:(5000, 10000) loss= 389.223114014\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 197\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 352.318511963\n",
      "range:(5000, 10000) loss= 372.670196533\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 357.200561523\n",
      "range:(5000, 10000) loss= 306.225036621\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 310.488647461\n",
      "range:(5000, 10000) loss= 341.608551025\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 249.023162842\n",
      "range:(5000, 10000) loss= 687.918151855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 733.058288574\n",
      "range:(5000, 10000) loss= 1289.55639648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 198\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 843.84552002\n",
      "range:(5000, 10000) loss= 765.165344238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 877.483764648\n",
      "range:(5000, 10000) loss= 939.511901855\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 893.664428711\n",
      "range:(5000, 10000) loss= 1018.32043457\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1020.68121338\n",
      "range:(5000, 10000) loss= 1178.0501709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1157.18359375\n",
      "range:(5000, 10000) loss= 1190.46520996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 199\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1082.84924316\n",
      "range:(5000, 10000) loss= 1040.26074219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 889.448425293\n",
      "range:(5000, 10000) loss= 716.59777832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 547.85736084\n",
      "range:(5000, 10000) loss= 457.278930664\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 353.313110352\n",
      "range:(5000, 10000) loss= 722.789794922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 748.837402344\n",
      "range:(5000, 10000) loss= 996.12512207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 200\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1006.12719727\n",
      "range:(5000, 10000) loss= 967.408081055\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 842.635742188\n",
      "range:(5000, 10000) loss= 783.583984375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 660.641357422\n",
      "range:(5000, 10000) loss= 538.265441895\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 426.789123535\n",
      "range:(5000, 10000) loss= 356.746032715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 360.710388184\n",
      "range:(5000, 10000) loss= 403.509002686\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 201\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 666.608825684\n",
      "range:(5000, 10000) loss= 576.092102051\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 766.603210449\n",
      "range:(5000, 10000) loss= 750.026672363\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 677.272705078\n",
      "range:(5000, 10000) loss= 648.352844238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 569.830627441\n",
      "range:(5000, 10000) loss= 511.867950439\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 470.167663574\n",
      "range:(5000, 10000) loss= 401.877807617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 202\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 375.4425354\n",
      "range:(5000, 10000) loss= 317.207519531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 310.846618652\n",
      "range:(5000, 10000) loss= 255.81098938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 584.508422852\n",
      "range:(5000, 10000) loss= 598.845214844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 790.06829834\n",
      "range:(5000, 10000) loss= 657.715759277\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 679.176269531\n",
      "range:(5000, 10000) loss= 662.948730469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 203\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 576.79296875\n",
      "range:(5000, 10000) loss= 597.821044922\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 595.748718262\n",
      "range:(5000, 10000) loss= 657.922119141\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 607.286987305\n",
      "range:(5000, 10000) loss= 554.748535156\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 481.353210449\n",
      "range:(5000, 10000) loss= 424.012298584\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 367.1378479\n",
      "range:(5000, 10000) loss= 317.339660645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 204\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 278.444946289\n",
      "range:(5000, 10000) loss= 235.456176758\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 461.383087158\n",
      "range:(5000, 10000) loss= 482.704284668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 489.40322876\n",
      "range:(5000, 10000) loss= 578.949462891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 540.242858887\n",
      "range:(5000, 10000) loss= 518.177856445\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 480.238189697\n",
      "range:(5000, 10000) loss= 423.086334229\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 205\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 441.192687988\n",
      "range:(5000, 10000) loss= 426.734649658\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 403.667022705\n",
      "range:(5000, 10000) loss= 372.401824951\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 332.170440674\n",
      "range:(5000, 10000) loss= 315.141235352\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 278.623931885\n",
      "range:(5000, 10000) loss= 242.155883789\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 248.575592041\n",
      "range:(5000, 10000) loss= 305.149749756\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 206\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 326.137298584\n",
      "range:(5000, 10000) loss= 362.641784668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 403.22644043\n",
      "range:(5000, 10000) loss= 458.64666748\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 449.704345703\n",
      "range:(5000, 10000) loss= 452.473999023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 408.179443359\n",
      "range:(5000, 10000) loss= 437.549987793\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 390.446044922\n",
      "range:(5000, 10000) loss= 363.846893311\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 207\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 358.966796875\n",
      "range:(5000, 10000) loss= 350.384185791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 331.772399902\n",
      "range:(5000, 10000) loss= 306.727081299\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 290.477813721\n",
      "range:(5000, 10000) loss= 275.276672363\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 232.93196106\n",
      "range:(5000, 10000) loss= 229.558990479\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 200.616348267\n",
      "range:(5000, 10000) loss= 212.298660278\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 208\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 187.635986328\n",
      "range:(5000, 10000) loss= 448.773651123\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 271.345214844\n",
      "range:(5000, 10000) loss= 340.388671875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 513.20324707\n",
      "range:(5000, 10000) loss= 415.199523926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 480.293914795\n",
      "range:(5000, 10000) loss= 451.252960205\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 418.444763184\n",
      "range:(5000, 10000) loss= 419.70791626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 209\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 372.617340088\n",
      "range:(5000, 10000) loss= 442.248626709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 413.646728516\n",
      "range:(5000, 10000) loss= 404.825378418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 391.734924316\n",
      "range:(5000, 10000) loss= 407.954467773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 411.55456543\n",
      "range:(5000, 10000) loss= 372.870880127\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 305.041625977\n",
      "range:(5000, 10000) loss= 298.379333496\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 210\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 267.770446777\n",
      "range:(5000, 10000) loss= 253.603134155\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 330.244171143\n",
      "range:(5000, 10000) loss= 511.064880371\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 829.617614746\n",
      "range:(5000, 10000) loss= 730.988525391\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 690.369689941\n",
      "range:(5000, 10000) loss= 705.592529297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 685.242675781\n",
      "range:(5000, 10000) loss= 821.043273926\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 211\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 900.832763672\n",
      "range:(5000, 10000) loss= 954.630432129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 887.862792969\n",
      "range:(5000, 10000) loss= 958.469177246\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1107.10705566\n",
      "range:(5000, 10000) loss= 1024.79516602\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 924.112243652\n",
      "range:(5000, 10000) loss= 795.219787598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 611.10345459\n",
      "range:(5000, 10000) loss= 588.421875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 212\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 356.911010742\n",
      "range:(5000, 10000) loss= 1318.63720703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1068.79943848\n",
      "range:(5000, 10000) loss= 1065.9588623\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1347.88671875\n",
      "range:(5000, 10000) loss= 1552.57958984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1953.90563965\n",
      "range:(5000, 10000) loss= 1835.64953613\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1569.98669434\n",
      "range:(5000, 10000) loss= 1613.64770508\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 213\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1821.07348633\n",
      "range:(5000, 10000) loss= 1522.04699707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1338.49206543\n",
      "range:(5000, 10000) loss= 1362.08752441\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1228.22277832\n",
      "range:(5000, 10000) loss= 1156.56616211\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 947.069702148\n",
      "range:(5000, 10000) loss= 812.959533691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 688.974304199\n",
      "range:(5000, 10000) loss= 571.774841309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 214\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 438.616943359\n",
      "range:(5000, 10000) loss= 1037.54614258\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1171.2512207\n",
      "range:(5000, 10000) loss= 952.950866699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1279.60400391\n",
      "range:(5000, 10000) loss= 1065.74633789\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1009.21118164\n",
      "range:(5000, 10000) loss= 903.96875\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 746.321105957\n",
      "range:(5000, 10000) loss= 656.195373535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 215\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 550.22088623\n",
      "range:(5000, 10000) loss= 451.65725708\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 668.064941406\n",
      "range:(5000, 10000) loss= 877.452697754\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 952.999816895\n",
      "range:(5000, 10000) loss= 1045.77307129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1316.35498047\n",
      "range:(5000, 10000) loss= 1012.87652588\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 963.760131836\n",
      "range:(5000, 10000) loss= 909.336914062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 216\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 831.367553711\n",
      "range:(5000, 10000) loss= 760.486633301\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 646.37310791\n",
      "range:(5000, 10000) loss= 584.625061035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 479.264160156\n",
      "range:(5000, 10000) loss= 398.897399902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 321.195098877\n",
      "range:(5000, 10000) loss= 319.981262207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 755.900085449\n",
      "range:(5000, 10000) loss= 1026.33166504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 217\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 895.995300293\n",
      "range:(5000, 10000) loss= 914.26385498\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1255.9576416\n",
      "range:(5000, 10000) loss= 1088.56933594\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1180.64367676\n",
      "range:(5000, 10000) loss= 935.582824707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 829.382385254\n",
      "range:(5000, 10000) loss= 792.416320801\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 793.59588623\n",
      "range:(5000, 10000) loss= 758.042358398\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 218\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 612.145141602\n",
      "range:(5000, 10000) loss= 506.823913574\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 424.387145996\n",
      "range:(5000, 10000) loss= 325.996887207\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 293.644958496\n",
      "range:(5000, 10000) loss= 509.577636719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 559.59173584\n",
      "range:(5000, 10000) loss= 747.960021973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 642.172241211\n",
      "range:(5000, 10000) loss= 679.828125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 219\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 574.216247559\n",
      "range:(5000, 10000) loss= 525.992492676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 444.142089844\n",
      "range:(5000, 10000) loss= 377.22543335\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 386.882232666\n",
      "range:(5000, 10000) loss= 381.293579102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 438.752716064\n",
      "range:(5000, 10000) loss= 829.188964844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 619.558349609\n",
      "range:(5000, 10000) loss= 691.699707031\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 220\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 669.022949219\n",
      "range:(5000, 10000) loss= 645.680053711\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 569.188476562\n",
      "range:(5000, 10000) loss= 547.259033203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 430.344299316\n",
      "range:(5000, 10000) loss= 412.848114014\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 410.442718506\n",
      "range:(5000, 10000) loss= 307.740692139\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 452.93081665\n",
      "range:(5000, 10000) loss= 351.756896973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 221\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 485.07611084\n",
      "range:(5000, 10000) loss= 511.819854736\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 551.568359375\n",
      "range:(5000, 10000) loss= 572.280578613\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 499.577606201\n",
      "range:(5000, 10000) loss= 665.89440918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 536.164306641\n",
      "range:(5000, 10000) loss= 502.094390869\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 493.517456055\n",
      "range:(5000, 10000) loss= 466.341766357\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 222\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 422.79989624\n",
      "range:(5000, 10000) loss= 398.329864502\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 367.9559021\n",
      "range:(5000, 10000) loss= 338.723876953\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 318.688964844\n",
      "range:(5000, 10000) loss= 268.14440918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 283.478057861\n",
      "range:(5000, 10000) loss= 246.380004883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 345.759887695\n",
      "range:(5000, 10000) loss= 497.828216553\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 223\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 398.556427002\n",
      "range:(5000, 10000) loss= 471.435028076\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 474.974914551\n",
      "range:(5000, 10000) loss= 474.284454346\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 433.8309021\n",
      "range:(5000, 10000) loss= 426.777954102\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 376.704986572\n",
      "range:(5000, 10000) loss= 353.368927002\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 323.085540771\n",
      "range:(5000, 10000) loss= 288.060211182\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 224\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 280.358337402\n",
      "range:(5000, 10000) loss= 246.537506104\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 236.661819458\n",
      "range:(5000, 10000) loss= 208.647369385\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 214.862365723\n",
      "range:(5000, 10000) loss= 208.841247559\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 282.985473633\n",
      "range:(5000, 10000) loss= 413.030151367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 365.327056885\n",
      "range:(5000, 10000) loss= 419.833251953\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 225\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 424.352203369\n",
      "range:(5000, 10000) loss= 369.18182373\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 388.718933105\n",
      "range:(5000, 10000) loss= 316.913085938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 325.276275635\n",
      "range:(5000, 10000) loss= 314.153381348\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 283.46774292\n",
      "range:(5000, 10000) loss= 326.24407959\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 278.469390869\n",
      "range:(5000, 10000) loss= 281.442901611\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 226\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 261.395507812\n",
      "range:(5000, 10000) loss= 274.317565918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 269.733459473\n",
      "range:(5000, 10000) loss= 447.338104248\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 382.341949463\n",
      "range:(5000, 10000) loss= 333.820068359\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 384.992004395\n",
      "range:(5000, 10000) loss= 391.8074646\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 381.025634766\n",
      "range:(5000, 10000) loss= 377.19229126\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 227\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 368.285430908\n",
      "range:(5000, 10000) loss= 394.643859863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 363.885101318\n",
      "range:(5000, 10000) loss= 356.473266602\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 314.358551025\n",
      "range:(5000, 10000) loss= 309.534790039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 269.418792725\n",
      "range:(5000, 10000) loss= 270.295715332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 245.162536621\n",
      "range:(5000, 10000) loss= 263.54574585\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 228\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 340.502838135\n",
      "range:(5000, 10000) loss= 695.825317383\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 530.505187988\n",
      "range:(5000, 10000) loss= 428.152587891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 453.534484863\n",
      "range:(5000, 10000) loss= 454.439147949\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 471.215515137\n",
      "range:(5000, 10000) loss= 482.295196533\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 427.612182617\n",
      "range:(5000, 10000) loss= 455.098419189\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 229\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 496.004455566\n",
      "range:(5000, 10000) loss= 563.697998047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 537.663696289\n",
      "range:(5000, 10000) loss= 488.027587891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 425.990661621\n",
      "range:(5000, 10000) loss= 437.967651367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 353.298675537\n",
      "range:(5000, 10000) loss= 361.757904053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 357.498504639\n",
      "range:(5000, 10000) loss= 270.323150635\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 230\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 252.581069946\n",
      "range:(5000, 10000) loss= 604.576904297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 506.545410156\n",
      "range:(5000, 10000) loss= 492.028320312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 631.632873535\n",
      "range:(5000, 10000) loss= 558.506530762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 529.772766113\n",
      "range:(5000, 10000) loss= 493.324920654\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 439.772766113\n",
      "range:(5000, 10000) loss= 412.783874512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 231\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 372.902099609\n",
      "range:(5000, 10000) loss= 320.863220215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 310.804656982\n",
      "range:(5000, 10000) loss= 273.077453613\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 232.004165649\n",
      "range:(5000, 10000) loss= 237.151428223\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 485.733703613\n",
      "range:(5000, 10000) loss= 317.121368408\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 570.889221191\n",
      "range:(5000, 10000) loss= 492.133605957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 232\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 597.124206543\n",
      "range:(5000, 10000) loss= 544.62298584\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 521.737792969\n",
      "range:(5000, 10000) loss= 540.722595215\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 541.604370117\n",
      "range:(5000, 10000) loss= 448.012786865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 445.504150391\n",
      "range:(5000, 10000) loss= 412.214660645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 395.408081055\n",
      "range:(5000, 10000) loss= 374.280426025\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 233\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 334.847961426\n",
      "range:(5000, 10000) loss= 298.858886719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 303.140258789\n",
      "range:(5000, 10000) loss= 269.854827881\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 284.078277588\n",
      "range:(5000, 10000) loss= 484.461242676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 557.842224121\n",
      "range:(5000, 10000) loss= 516.743835449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 507.652832031\n",
      "range:(5000, 10000) loss= 459.178283691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 234\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 486.651153564\n",
      "range:(5000, 10000) loss= 419.833770752\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 372.728637695\n",
      "range:(5000, 10000) loss= 363.274536133\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 317.054412842\n",
      "range:(5000, 10000) loss= 260.256530762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 213.686950684\n",
      "range:(5000, 10000) loss= 191.6300354\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 178.147293091\n",
      "range:(5000, 10000) loss= 171.352249146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 235\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 248.528701782\n",
      "range:(5000, 10000) loss= 424.672607422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 427.148712158\n",
      "range:(5000, 10000) loss= 509.121551514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 457.01260376\n",
      "range:(5000, 10000) loss= 568.000793457\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 513.218566895\n",
      "range:(5000, 10000) loss= 498.190246582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 467.489257812\n",
      "range:(5000, 10000) loss= 438.244720459\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 236\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 459.261962891\n",
      "range:(5000, 10000) loss= 430.277770996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 381.460876465\n",
      "range:(5000, 10000) loss= 339.764587402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 307.879394531\n",
      "range:(5000, 10000) loss= 263.762878418\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 232.943023682\n",
      "range:(5000, 10000) loss= 203.183883667\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 183.836685181\n",
      "range:(5000, 10000) loss= 372.151885986\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 237\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 452.93536377\n",
      "range:(5000, 10000) loss= 482.036804199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 457.241638184\n",
      "range:(5000, 10000) loss= 440.282592773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 433.726654053\n",
      "range:(5000, 10000) loss= 411.113769531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 357.436737061\n",
      "range:(5000, 10000) loss= 332.466156006\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 307.51260376\n",
      "range:(5000, 10000) loss= 251.636169434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 238\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 224.276565552\n",
      "range:(5000, 10000) loss= 290.680389404\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 476.1824646\n",
      "range:(5000, 10000) loss= 1141.44824219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1065.796875\n",
      "range:(5000, 10000) loss= 710.500366211\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 861.506408691\n",
      "range:(5000, 10000) loss= 1024.07824707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1004.23248291\n",
      "range:(5000, 10000) loss= 1088.58361816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 239\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1261.76086426\n",
      "range:(5000, 10000) loss= 1353.62585449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1553.36999512\n",
      "range:(5000, 10000) loss= 1674.58007812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1587.87915039\n",
      "range:(5000, 10000) loss= 1360.65197754\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1532.91845703\n",
      "range:(5000, 10000) loss= 1273.87890625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 976.416503906\n",
      "range:(5000, 10000) loss= 747.800842285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 240\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 608.946166992\n",
      "range:(5000, 10000) loss= 588.581604004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 1416.70800781\n",
      "range:(5000, 10000) loss= 1168.30358887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1282.86791992\n",
      "range:(5000, 10000) loss= 1467.20922852\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1275.84985352\n",
      "range:(5000, 10000) loss= 1303.18908691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 949.854797363\n",
      "range:(5000, 10000) loss= 1010.76464844\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 241\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 694.090515137\n",
      "range:(5000, 10000) loss= 847.207336426\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 491.809265137\n",
      "range:(5000, 10000) loss= 783.250671387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 1224.66455078\n",
      "range:(5000, 10000) loss= 1294.8651123\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 1325.87097168\n",
      "range:(5000, 10000) loss= 1238.49316406\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1075.64648438\n",
      "range:(5000, 10000) loss= 896.520935059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 242\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 778.15838623\n",
      "range:(5000, 10000) loss= 662.871459961\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 550.493225098\n",
      "range:(5000, 10000) loss= 476.474914551\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 414.594512939\n",
      "range:(5000, 10000) loss= 762.01348877\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 799.125793457\n",
      "range:(5000, 10000) loss= 850.932495117\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 1378.4354248\n",
      "range:(5000, 10000) loss= 1141.93432617\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 243\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 1085.33703613\n",
      "range:(5000, 10000) loss= 1028.21325684\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 908.142578125\n",
      "range:(5000, 10000) loss= 906.739807129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 780.749084473\n",
      "range:(5000, 10000) loss= 791.723754883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 724.630859375\n",
      "range:(5000, 10000) loss= 591.648925781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 487.098937988\n",
      "range:(5000, 10000) loss= 382.658081055\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 244\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 281.922454834\n",
      "range:(5000, 10000) loss= 220.164108276\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 336.44430542\n",
      "range:(5000, 10000) loss= 484.643707275\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 577.526977539\n",
      "range:(5000, 10000) loss= 517.90411377\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 535.608337402\n",
      "range:(5000, 10000) loss= 427.721374512\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 392.25177002\n",
      "range:(5000, 10000) loss= 344.800109863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 245\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 293.514648438\n",
      "range:(5000, 10000) loss= 230.798080444\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 244.556915283\n",
      "range:(5000, 10000) loss= 361.529632568\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 355.468475342\n",
      "range:(5000, 10000) loss= 420.786468506\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 399.022918701\n",
      "range:(5000, 10000) loss= 330.672149658\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 357.134246826\n",
      "range:(5000, 10000) loss= 336.68661499\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 246\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 313.014526367\n",
      "range:(5000, 10000) loss= 385.773162842\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 352.608612061\n",
      "range:(5000, 10000) loss= 571.289367676\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 455.704986572\n",
      "range:(5000, 10000) loss= 401.112487793\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 575.914733887\n",
      "range:(5000, 10000) loss= 472.235015869\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 592.933837891\n",
      "range:(5000, 10000) loss= 510.772155762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 247\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 471.941955566\n",
      "range:(5000, 10000) loss= 520.271057129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 498.628601074\n",
      "range:(5000, 10000) loss= 518.860046387\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 451.239074707\n",
      "range:(5000, 10000) loss= 436.724639893\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 416.172424316\n",
      "range:(5000, 10000) loss= 362.46496582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 297.177642822\n",
      "range:(5000, 10000) loss= 262.553619385\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 248\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 237.066757202\n",
      "range:(5000, 10000) loss= 202.024398804\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 164.877502441\n",
      "range:(5000, 10000) loss= 250.602279663\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 327.590026855\n",
      "range:(5000, 10000) loss= 416.789093018\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 549.989746094\n",
      "range:(5000, 10000) loss= 499.724334717\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 536.375366211\n",
      "range:(5000, 10000) loss= 478.588134766\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 249\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 424.730316162\n",
      "range:(5000, 10000) loss= 382.389007568\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 363.023376465\n",
      "range:(5000, 10000) loss= 325.319793701\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 294.869659424\n",
      "range:(5000, 10000) loss= 311.199554443\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 255.660186768\n",
      "range:(5000, 10000) loss= 228.429626465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 254.000396729\n",
      "range:(5000, 10000) loss= 221.285980225\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 250\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 196.027328491\n",
      "range:(5000, 10000) loss= 270.571594238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 358.224914551\n",
      "range:(5000, 10000) loss= 302.14239502\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 322.274353027\n",
      "range:(5000, 10000) loss= 368.664550781\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 351.89175415\n",
      "range:(5000, 10000) loss= 324.175323486\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 287.45413208\n",
      "range:(5000, 10000) loss= 251.264343262\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 251\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 250.088638306\n",
      "range:(5000, 10000) loss= 218.851425171\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 206.024765015\n",
      "range:(5000, 10000) loss= 197.02684021\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 181.873474121\n",
      "range:(5000, 10000) loss= 178.0887146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 158.604187012\n",
      "range:(5000, 10000) loss= 149.675369263\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 256.712799072\n",
      "range:(5000, 10000) loss= 328.10269165\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 252\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 251.144393921\n",
      "range:(5000, 10000) loss= 296.477355957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 264.002441406\n",
      "range:(5000, 10000) loss= 272.152709961\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 232.109085083\n",
      "range:(5000, 10000) loss= 206.566070557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 196.521759033\n",
      "range:(5000, 10000) loss= 170.247573853\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 172.019592285\n",
      "range:(5000, 10000) loss= 168.320877075\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 253\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 161.055099487\n",
      "range:(5000, 10000) loss= 157.302322388\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 155.018753052\n",
      "range:(5000, 10000) loss= 157.029953003\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 148.435272217\n",
      "range:(5000, 10000) loss= 148.064208984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 139.339324951\n",
      "range:(5000, 10000) loss= 263.441619873\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 379.422912598\n",
      "range:(5000, 10000) loss= 361.811065674\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 254\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 397.716064453\n",
      "range:(5000, 10000) loss= 408.788787842\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 397.909240723\n",
      "range:(5000, 10000) loss= 386.654449463\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 468.796508789\n",
      "range:(5000, 10000) loss= 501.538238525\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 553.329040527\n",
      "range:(5000, 10000) loss= 602.770629883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 745.740356445\n",
      "range:(5000, 10000) loss= 651.951477051\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 255\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 737.171386719\n",
      "range:(5000, 10000) loss= 768.733459473\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 635.77520752\n",
      "range:(5000, 10000) loss= 540.420715332\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 398.001190186\n",
      "range:(5000, 10000) loss= 335.844146729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 285.115722656\n",
      "range:(5000, 10000) loss= 456.602111816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 580.805175781\n",
      "range:(5000, 10000) loss= 562.60925293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 256\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 523.178161621\n",
      "range:(5000, 10000) loss= 484.409423828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 404.919189453\n",
      "range:(5000, 10000) loss= 436.645690918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 422.094177246\n",
      "range:(5000, 10000) loss= 371.973907471\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 487.602325439\n",
      "range:(5000, 10000) loss= 388.19909668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 392.480407715\n",
      "range:(5000, 10000) loss= 611.939758301\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 257\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 423.588500977\n",
      "range:(5000, 10000) loss= 526.176757812\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 487.800292969\n",
      "range:(5000, 10000) loss= 539.218688965\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 518.084289551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1aabe525c452>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mminX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mminY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mminX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mminY\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'range:{} loss= {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/botman/Programming/Platforms/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=model_path, graph=sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 840\n",
    "    for ep in range(80, no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = batch_size \n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(int(float(len(batch_images)) / min_batch_size)):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                minX = batch_images[start: end]; minY = batch_labels[start: end]\n",
    "                \n",
    "                _, loss = sess.run([train_step, cost], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "                print('range:{} loss= {}'.format((start, end), loss))\n",
    "            \n",
    "                g_step += 1\n",
    "                \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model_cifar_2\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2605"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
