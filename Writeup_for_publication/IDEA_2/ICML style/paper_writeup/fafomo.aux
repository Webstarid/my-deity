\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{goodfellow-et-al:deep-learning}
\citation{cauchy:gradient-descent}
\citation{sutton:gradient-descent-problems}
\citation{qian:momentum}
\citation{nesterov:momentum}
\citation{kingma-ba:adam}
\citation{rudder:overview}
\citation{darken:schedule}
\citation{robbins:annealing}
\citation{koolen:learning-learning-rate}
\citation{schaul:pesky}
\newlabel{alg:optimization}{{1}{2}{}{algorithm.1}{}}
\citation{einstein:relativity}
\citation{akram:newton-raphson}
\newlabel{fig:figure_1}{{1}{3}{The 3D surface illustration of the update function $u(x, y)$}{figure.1}{}}
\newlabel{fig:figure_2}{{2}{4}{2D $y$ v/s $x$ plot of the square objective function}{figure.2}{}}
\newlabel{fig:figure_3}{{3}{4}{2D Cost v/s Iteration plot of the simulation on square function}{figure.3}{}}
\citation{goodfellow-et-al:deep-learning}
\citation{nair:relu}
\citation{ioffe:batch-normalization}
\citation{abadi:tensorflow}
\citation{glorot:xavier}
\citation{anonymous:adam-problem}
\newlabel{fig:figure_4}{{4}{5}{2D $y$ v/s $x$ plot of the exponentiation objective function}{figure.4}{}}
\newlabel{fig:figure_5}{{5}{5}{2D Cost v/s Iteration plot of the simulation on exponentiation function}{figure.5}{}}
\newlabel{fig:figure_6}{{6}{5}{2D Cost v/s Iteration plot of the described experiment}{figure.6}{}}
\bibdata{fafomo}
\bibcite{abadi:tensorflow}{{1}{2016}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Mane, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Viegas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng}}}
\bibcite{akram:newton-raphson}{{2}{2015}{{Akram \& ul~Ann}}{{Akram and ul~Ann}}}
\bibcite{anonymous:adam-problem}{{3}{2018}{{Anonymous}}{{}}}
\bibcite{cauchy:gradient-descent}{{4}{1847}{{Cauchy}}{{}}}
\bibcite{darken:schedule}{{5}{2002}{{Darken et~al.}}{{Darken, Chang, and Moody}}}
\bibcite{einstein:relativity}{{6}{1914 - 1917}{{Einstein}}{{}}}
\bibcite{glorot:xavier}{{7}{2010}{{Glorot \& Bengio}}{{Glorot and Bengio}}}
\newlabel{fig:figure_7}{{7}{6}{Comparison of Cost plots of the proposed algorithm and the Adam Optimizer}{figure.7}{}}
\newlabel{fig:figure_8}{{8}{6}{Comparison of Cost plots of the proposed algorithm and other prevailing optimizers}{figure.8}{}}
\newlabel{fig:figure_9}{{9}{6}{Histograms of the distributions of the trainable parameters in the Network}{figure.9}{}}
\bibcite{goodfellow-et-al:deep-learning}{{8}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, and Courville}}}
\bibcite{ioffe:batch-normalization}{{9}{2015}{{Ioffe \& Szegedy}}{{Ioffe and Szegedy}}}
\bibcite{kingma-ba:adam}{{10}{2015}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{koolen:learning-learning-rate}{{11}{2014}{{Koolen et~al.}}{{Koolen, van Erven, and Gr\"{u}nwald}}}
\bibcite{nair:relu}{{12}{2010}{{Nair \& Hinton}}{{Nair and Hinton}}}
\bibcite{nesterov:momentum}{{13}{1983}{{Nesterov}}{{}}}
\bibcite{qian:momentum}{{14}{1999}{{Qian}}{{}}}
\bibcite{robbins:annealing}{{15}{1951}{{Robbins \& Monro}}{{Robbins and Monro}}}
\bibcite{rudder:overview}{{16}{2017}{{Rudder}}{{}}}
\bibcite{schaul:pesky}{{17}{2013}{{Schaul et~al.}}{{Schaul, Zhang, and LeCun}}}
\bibcite{sutton:gradient-descent-problems}{{18}{1986}{{Sutton}}{{}}}
\bibstyle{icml2018}
